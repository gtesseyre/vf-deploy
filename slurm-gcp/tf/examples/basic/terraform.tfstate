{
  "version": 4,
  "terraform_version": "0.12.24",
  "serial": 77,
  "lineage": "07bdb4cd-bba8-d581-edc8-4f2b27e9cb65",
  "outputs": {
    "controller_network_ips": {
      "value": [
        [
          "192.168.0.28"
        ]
      ],
      "type": [
        "tuple",
        [
          [
            "tuple",
            [
              "string"
            ]
          ]
        ]
      ]
    },
    "login_network_ips": {
      "value": [
        [
          "192.168.0.27"
        ]
      ],
      "type": [
        "tuple",
        [
          [
            "tuple",
            [
              "string"
            ]
          ]
        ]
      ]
    }
  },
  "resources": [
    {
      "module": "module.slurm_cluster_controller",
      "mode": "managed",
      "type": "google_compute_disk",
      "name": "secondary",
      "each": "list",
      "provider": "provider.google",
      "instances": []
    },
    {
      "module": "module.slurm_cluster_network",
      "mode": "managed",
      "type": "google_compute_firewall",
      "name": "cluster_iap_ssh_firewall",
      "each": "list",
      "provider": "provider.google",
      "instances": []
    },
    {
      "module": "module.slurm_cluster_network",
      "mode": "managed",
      "type": "google_compute_firewall",
      "name": "cluster_internal_firewall",
      "each": "list",
      "provider": "provider.google",
      "instances": []
    },
    {
      "module": "module.slurm_cluster_network",
      "mode": "managed",
      "type": "google_compute_firewall",
      "name": "cluster_ssh_firewall",
      "each": "list",
      "provider": "provider.google",
      "instances": []
    },
    {
      "module": "module.slurm_cluster_compute",
      "mode": "managed",
      "type": "google_compute_instance",
      "name": "compute_node",
      "each": "map",
      "provider": "provider.google",
      "instances": [
        {
          "index_key": "gcluster-compute-0-image",
          "schema_version": 6,
          "attributes": {
            "allow_stopping_for_update": null,
            "attached_disk": [],
            "boot_disk": [
              {
                "auto_delete": true,
                "device_name": "persistent-disk-0",
                "disk_encryption_key_raw": "",
                "disk_encryption_key_sha256": "",
                "initialize_params": [
                  {
                    "image": "https://www.googleapis.com/compute/v1/projects/centos-cloud/global/images/centos-7-v20200403",
                    "labels": {},
                    "size": 20,
                    "type": "pd-standard"
                  }
                ],
                "kms_key_self_link": "",
                "mode": "READ_WRITE",
                "source": "https://www.googleapis.com/compute/v1/projects/playground-190420/zones/us-west1-a/disks/gcluster-compute-0-image"
              }
            ],
            "can_ip_forward": false,
            "cpu_platform": "Intel Broadwell",
            "current_status": "RUNNING",
            "deletion_protection": false,
            "description": "",
            "desired_status": null,
            "enable_display": false,
            "guest_accelerator": [],
            "hostname": "",
            "id": "projects/playground-190420/zones/us-west1-a/instances/gcluster-compute-0-image",
            "instance_id": "6029922099034364119",
            "label_fingerprint": "42WmSpB8rSM=",
            "labels": null,
            "machine_type": "n1-standard-2",
            "metadata": {
              "VmDnsSetting": "GlobalOnly",
              "config": "{\"cluster_name\":\"gcluster\",\"compute_node_prefix\":\"gcluster-compute\",\"controller_secondary_disk\":0,\"munge_key\":\"\",\"network_storage\":[{\"fs_type\":\"nfs\",\"local_mount\":\"/mnt/virtualflow\",\"mount_options\":\"defaults,hard,intr\",\"remote_mount\":\"/virtualflow\",\"server_ip\":\"10.102.14.42\"}],\"ompi_version\":null,\"partitions\":[{\"compute_disk_size_gb\":20,\"compute_disk_type\":\"pd-standard\",\"compute_labels\":{},\"cpu_platform\":null,\"gpu_count\":0,\"gpu_type\":null,\"machine_type\":\"n1-standard-2\",\"max_node_count\":10,\"name\":\"debug\",\"network_storage\":[],\"preemptible_bursting\":true,\"static_node_count\":0,\"vpc_subnet\":\"vf-west-subnet\",\"zone\":\"us-west1-a\"}],\"zone\":\"us-west1-a\"}\n",
              "enable-oslogin": "TRUE",
              "fluentd_conf_tpl": "# This filter goes with all of the slurm sources. It maps slurm log levels\n# to logging api severity levels and prepends the tag to the log message.\n\u003cfilter slurmctld slurmdbd slurmd\u003e\n  @type record_transformer\n  enable_ruby true\n  \u003crecord\u003e\n    severity ${ {'debug'=\u003e'DEBUG', 'debug2'=\u003e'DEBUG', 'debug3'=\u003e'DEBUG', 'debug4'=\u003e'DEBUG', 'debug5'=\u003e'DEBUG', 'error'=\u003e'ERROR', 'fatal'=\u003e'CRITICAL'}.tap{|map| map.default='INFO'}[record['severity']] }\n    message ${tag + \" \" + record['message']}\n  \u003c/record\u003e\n\u003c/filter\u003e\n\n\u003csource\u003e\n  @type tail\n  tag slurmd\n  path /var/log/slurm/slurmd*.log\n  pos_file /var/lib/google-fluentd/pos/slurm_slurmd.log.pos\n  read_from_head true\n  \u003cparse\u003e\n    @type regexp\n    expression /^\\[(?\u003ctime\u003e[^\\]]*)\\] (?\u003cmessage\u003e(?\u003cseverity\u003e\\w*).*)$/\n    time_format %Y-%m-%dT%H:%M:%S.%N\n  \u003c/parse\u003e\n\u003c/source\u003e\n\n",
              "setup_script": "#!/usr/bin/env python3\n\n# Copyright 2017 SchedMD LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport datetime\nimport importlib\nimport itertools as it\nimport logging\nimport os\nimport shutil\nimport socket\nimport sys\nimport time\nimport threading\nimport urllib.request\nfrom pathlib import Path\nfrom subprocess import DEVNULL\n\nimport googleapiclient.discovery\nimport requests\nimport yaml\n\n\n# get util.py from metadata\nUTIL_FILE = Path('/tmp/util.py')\ntry:\n    resp = requests.get('http://metadata.google.internal/computeMetadata/v1/instance/attributes/util_script',\n                        headers={'Metadata-Flavor': 'Google'})\n    resp.raise_for_status()\n    UTIL_FILE.write_text(resp.text)\nexcept requests.exceptions.RequestException:\n    print(\"util.py script not found in metadata\")\n    if not UTIL_FILE.exists():\n        print(f\"{UTIL_FILE} also does not exist, aborting\")\n        sys.exit(1)\n\nspec = importlib.util.spec_from_file_location('util', UTIL_FILE)\nutil = importlib.util.module_from_spec(spec)\nsys.modules[spec.name] = util\nspec.loader.exec_module(util)\ncd = util.cd  # import util.cd into local namespace\n\nutil.config_root_logger()\nlog = logging.getLogger(Path(__file__).name)\n\n# get setup config from metadata\nconfig_yaml = yaml.safe_load(util.get_metadata('attributes/config'))\nif not util.get_metadata('attributes/terraform'):\n    config_yaml = yaml.safe_load(config_yaml)\ncfg = util.Config.new_config(config_yaml)\n\nHOME_DIR = Path('/home')\nAPPS_DIR = Path('/apps')\nCURR_SLURM_DIR = APPS_DIR/'slurm/current'\nMUNGE_DIR = Path('/etc/munge')\nSLURM_LOG = Path('/var/log/slurm')\n\nSEC_DISK_DIR = Path('/mnt/disks/sec')\nRESUME_TIMEOUT = 300\nSUSPEND_TIMEOUT = 300\n\nCONTROL_MACHINE = cfg.cluster_name + '-controller'\n\nMOTD_HEADER = \"\"\"\n\n                                 SSSSSSS\n                                SSSSSSSSS\n                                SSSSSSSSS\n                                SSSSSSSSS\n                        SSSS     SSSSSSS     SSSS\n                       SSSSSS               SSSSSS\n                       SSSSSS    SSSSSSS    SSSSSS\n                        SSSS    SSSSSSSSS    SSSS\n                SSS             SSSSSSSSS             SSS\n               SSSSS    SSSS    SSSSSSSSS    SSSS    SSSSS\n                SSS    SSSSSS   SSSSSSSSS   SSSSSS    SSS\n                       SSSSSS    SSSSSSS    SSSSSS\n                SSS    SSSSSS               SSSSSS    SSS\n               SSSSS    SSSS     SSSSSSS     SSSS    SSSSS\n          S     SSS             SSSSSSSSS             SSS     S\n         SSS            SSSS    SSSSSSSSS    SSSS            SSS\n          S     SSS    SSSSSS   SSSSSSSSS   SSSSSS    SSS     S\n               SSSSS   SSSSSS   SSSSSSSSS   SSSSSS   SSSSS\n          S    SSSSS    SSSS     SSSSSSS     SSSS    SSSSS    S\n    S    SSS    SSS                                   SSS    SSS    S\n    S     S                                                   S     S\n                SSS\n                SSS\n                SSS\n                SSS\n SSSSSSSSSSSS   SSS   SSSS       SSSS    SSSSSSSSS   SSSSSSSSSSSSSSSSSSSS\nSSSSSSSSSSSSS   SSS   SSSS       SSSS   SSSSSSSSSS  SSSSSSSSSSSSSSSSSSSSSS\nSSSS            SSS   SSSS       SSSS   SSSS        SSSS     SSSS     SSSS\nSSSS            SSS   SSSS       SSSS   SSSS        SSSS     SSSS     SSSS\nSSSSSSSSSSSS    SSS   SSSS       SSSS   SSSS        SSSS     SSSS     SSSS\n SSSSSSSSSSSS   SSS   SSSS       SSSS   SSSS        SSSS     SSSS     SSSS\n         SSSS   SSS   SSSS       SSSS   SSSS        SSSS     SSSS     SSSS\n         SSSS   SSS   SSSS       SSSS   SSSS        SSSS     SSSS     SSSS\nSSSSSSSSSSSSS   SSS   SSSSSSSSSSSSSSS   SSSS        SSSS     SSSS     SSSS\nSSSSSSSSSSSS    SSS    SSSSSSSSSSSSS    SSSS        SSSS     SSSS     SSSS\n\n\n\"\"\"\n\n\ndef add_slurm_user():\n\n    util.run(\"useradd -m -c SlurmUser -d /var/lib/slurm -U -r slurm\")\n# END add_slurm_user()\n\n\ndef setup_modules():\n\n    appsmfs = Path('/apps/modulefiles')\n\n    with open('/usr/share/Modules/init/.modulespath', 'r+') as dotmp:\n        if str(appsmfs) not in dotmp.read():\n            if cfg.instance_type == 'controller' and not appsmfs.is_dir():\n                appsmfs.mkdir(parents=True)\n            # after read, file cursor is at end of file\n            dotmp.write(f'\\n{appsmfs}\\n')\n# END setup_modules\n\n\ndef start_motd():\n\n    msg = MOTD_HEADER + \"\"\"\n*** Slurm is currently being installed/configured in the background. ***\nA terminal broadcast will announce when installation and configuration is\ncomplete.\n\nPartitions will be marked down until the compute image has been created.\nFor instances with gpus attached, it could take ~10 mins after the controller\nhas finished installing.\n\n\"\"\"\n\n    if cfg.instance_type != \"controller\":\n        msg += \"\"\"/home on the controller will be mounted over the existing /home.\nAny changes in /home will be hidden. Please wait until the installation is\ncomplete before making changes in your home directory.\n\n\"\"\"\n\n    with open('/etc/motd', 'w') as f:\n        f.write(msg)\n# END start_motd()\n\n\ndef end_motd(broadcast=True):\n\n    with open('/etc/motd', 'w') as f:\n        f.write(MOTD_HEADER)\n\n    if not broadcast:\n        return\n\n    util.run(\"wall -n '*** Slurm {} daemon installation complete ***'\"\n             .format(cfg.instance_type))\n\n    if cfg.instance_type != 'controller':\n        util.run(\"\"\"wall -n '\n/home on the controller was mounted over the existing /home.\nEither log out and log back in or cd into ~.\n'\"\"\")\n# END start_motd()\n\n\ndef have_gpus(hostname):\n\n    pid = util.get_pid(hostname)\n    return cfg.partitions[pid].gpu_count \u003e 0\n# END have_gpus()\n\n\ndef install_slurmlog_conf():\n    \"\"\" Install fluentd config for slurm logs \"\"\"\n\n    slurmlog_config = util.get_metadata('attributes/fluentd_conf_tpl')\n    if slurmlog_config:\n        Path('/etc/google-fluentd/config.d/slurmlogs.conf').write_text(\n            slurmlog_config)\n\n\ndef install_packages():\n\n    # install stackdriver monitoring and logging\n    add_mon_script = Path('/tmp/add-monitoring-agent-repo.sh')\n    add_mon_url = f'https://dl.google.com/cloudagents/{add_mon_script.name}'\n    urllib.request.urlretrieve(add_mon_url, add_mon_script)\n    util.run(f\"bash {add_mon_script}\")\n    util.run(\"yum install -y stackdriver-agent\")\n\n    add_log_script = Path('/tmp/install-logging-agent.sh')\n    add_log_url = f'https://dl.google.com/cloudagents/{add_log_script.name}'\n    urllib.request.urlretrieve(add_log_url, add_log_script)\n    util.run(f\"bash {add_log_script}\")\n    install_slurmlog_conf()\n\n    util.run(\"systemctl enable stackdriver-agent google-fluentd\")\n    util.run(\"systemctl start stackdriver-agent google-fluentd\")\n\n    # install cuda if needed\n    if cfg.instance_type == 'compute' and have_gpus(socket.gethostname()):\n        util.run(\"yum -y install kernel-devel-$(uname -r) kernel-headers-$(uname -r)\",\n                 shell=True)\n        repo = 'http://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/cuda-rhel7.repo'\n        util.run(f\"yum-config-manager --add-repo {repo}\")\n        util.run(\"yum clean all\")\n        util.run(\"yum -y install nvidia-driver-latest-dkms cuda\")\n        util.run(\"yum -y install cuda-drivers\")\n        # Creates the device files\n        util.run(\"nvidia-smi\")\n# END install_packages()\n\n\ndef setup_munge():\n\n    munge_service_patch = Path('/usr/lib/systemd/system/munge.service')\n    req_mount = (f\"\\nRequiresMountsFor={MUNGE_DIR}\"\n                 if cfg.instance_type != 'controller' else '')\n    with munge_service_patch.open('w') as f:\n        f.write(f\"\"\"\n[Unit]\nDescription=MUNGE authentication service\nDocumentation=man:munged(8)\nAfter=network.target\nAfter=syslog.target\nAfter=time-sync.target{req_mount}\n\n[Service]\nType=forking\nExecStart=/usr/sbin/munged --num-threads=10\nPIDFile=/var/run/munge/munged.pid\nUser=munge\nGroup=munge\nRestart=on-abort\n\n[Install]\nWantedBy=multi-user.target\n\"\"\")\n\n    util.run(\"systemctl enable munge\")\n\n    if cfg.instance_type != 'controller':\n        return\n\n    if cfg.munge_key:\n        with (MUNGE_DIR/'munge.key').open('w') as f:\n            f.write(cfg.munge_key)\n\n        util.run(f\"chown -R munge: {MUNGE_DIR} /var/log/munge/\")\n\n        (MUNGE_DIR/'munge_key').chmod(0o400)\n        MUNGE_DIR.chmod(0o700)\n        Path('var/log/munge/').chmod(0o700)\n    else:\n        util.run('create-munge-key')\n# END setup_munge ()\n\n\ndef start_munge():\n    util.run(\"systemctl start munge\")\n# END start_munge()\n\n\ndef setup_nfs_exports():\n\n    export_paths = (\n        (HOME_DIR, not EXTERNAL_MOUNT_HOME),\n        (APPS_DIR, not EXTERNAL_MOUNT_APPS),\n        (MUNGE_DIR, not EXTERNAL_MOUNT_MUNGE),\n        (SEC_DISK_DIR, cfg.controller_secondary_disk),\n    )\n\n    # export path if corresponding selector boolean is True\n    for path in it.compress(*zip(*export_paths)):\n        util.run(rf\"sed -i '\\#{path}#d' /etc/exports\")\n        with open('/etc/exports', 'a') as f:\n            f.write(f\"\\n{path}  *(rw,no_subtree_check,no_root_squash)\")\n\n    util.run(\"exportfs -a\")\n# END setup_nfs_exports()\n\n\ndef expand_machine_type():\n\n    machines = []\n    compute = googleapiclient.discovery.build('compute', 'v1',\n                                              cache_discovery=False)\n    for part in cfg.partitions:\n        machine = {'cpus': 1, 'memory': 1}\n        try:\n            type_resp = compute.machineTypes().get(\n                project=cfg.project, zone=part.zone,\n                machineType=part.machine_type).execute()\n            if type_resp:\n                machine['cpus'] = type_resp['guestCpus']\n\n                # Because the actual memory on the host will be different than\n                # what is configured (e.g. kernel will take it). From\n                # experiments, about 16 MB per GB are used (plus about 400 MB\n                # buffer for the first couple of GB's. Using 30 MB to be safe.\n                gb = type_resp['memoryMb'] // 1024\n                machine['memory'] = type_resp['memoryMb'] - (400 + (gb * 30))\n\n        except Exception:\n            log.exception(\"Failed to get MachineType '{}' from google api\"\n                          .format(part.machine_type))\n        finally:\n            machines.append(machine)\n\n    return machines\n# END expand_machine_type()\n\n\ndef install_slurm_conf():\n\n    machines = expand_machine_type()\n\n    if cfg.ompi_version:\n        mpi_default = \"pmi2\"\n    else:\n        mpi_default = \"none\"\n\n    conf_resp = util.get_metadata('attributes/slurm_conf_tpl')\n    conf = conf_resp.format(**globals(), **locals())\n\n    static_nodes = []\n    for i, machine in enumerate(machines):\n        part = cfg.partitions[i]\n        static_range = ''\n        if part.static_node_count:\n            if part.static_node_count \u003e 1:\n                static_range = '{}-{}-[0-{}]'.format(\n                    cfg.compute_node_prefix, i, part.static_node_count - 1)\n            else:\n                static_range = f\"{cfg.compute_node_prefix}-{i}-0\"\n\n        cloud_range = \"\"\n        if (part.max_node_count and\n                (part.max_node_count != part.static_node_count)):\n            cloud_range = \"{}-{}-[{}-{}]\".format(\n                cfg.compute_node_prefix, i, part.static_node_count,\n                part.max_node_count - 1)\n\n        conf += (\"NodeName=DEFAULT \"\n                 f\"CPUs={machine['cpus']} \"\n                 f\"RealMemory={machine['memory']} \"\n                 \"State=UNKNOWN\")\n        conf += '\\n'\n\n        # Nodes\n        gres = \"\"\n        if part.gpu_count:\n            gres = \" Gres=gpu:\" + str(part.gpu_count)\n        if static_range:\n            static_nodes.append(static_range)\n            conf += f\"NodeName={static_range}{gres}\\n\"\n\n        if cloud_range:\n            conf += f\"NodeName={cloud_range} State=CLOUD{gres}\\n\"\n\n        # Partitions\n        part_nodes = f'-{i}-[0-{part.max_node_count - 1}]'\n\n        def_mem_per_cpu = max(100, machine['memory'] // machine['cpus'])\n\n        conf += (\"PartitionName={} Nodes={}-compute{} MaxTime=INFINITE \"\n                 \"State=UP DefMemPerCPU={} LLN=yes\"\n                 .format(part.name, cfg.cluster_name, part_nodes,\n                         def_mem_per_cpu))\n\n        # First partition specified is treated as the default partition\n        if i == 0:\n            conf += \" Default=YES\"\n        conf += \"\\n\\n\"\n\n    if len(static_nodes):\n        conf += \"\\nSuspendExcNodes={}\\n\".format(','.join(static_nodes))\n\n    etc_dir = CURR_SLURM_DIR/'etc'\n    if not etc_dir.exists():\n        etc_dir.mkdir(parents=True)\n    with (etc_dir/'slurm.conf').open('w') as f:\n        f.write(conf)\n# END install_slurm_conf()\n\n\ndef install_slurmdbd_conf():\n    if cfg.cloudsql:\n        db_name = cfg.cloudsql['db_name']\n        db_user = cfg.cloudsql['user']\n        db_pass = cfg.cloudsql['password']\n        db_host_str = cfg.cloudsql['server_ip'].split(':')\n        db_host = db_host_str[0]\n        db_port = db_host_str[1] if len(db_host_str) \u003e= 2 else '3306'\n    else:\n        db_name = \"slurm_acct_db\"\n        db_user = 'slurm'\n        db_pass = '\"\"'\n        db_host = 'localhost'\n        db_port = '3306'\n\n    conf_resp = util.get_metadata('attributes/slurmdbd_conf_tpl')\n    conf = conf_resp.format(**globals(), **locals())\n\n    etc_dir = CURR_SLURM_DIR/'etc'\n    if not etc_dir.exists():\n        etc_dir.mkdir(parents=True)\n    (etc_dir/'slurmdbd.conf').write_text(conf)\n    (etc_dir/'slurmdbd.conf').chmod(0o600)\n\n# END install_slurmdbd_conf()\n\n\ndef install_cgroup_conf():\n\n    conf = util.get_metadata('attributes/cgroup_conf_tpl')\n\n    etc_dir = CURR_SLURM_DIR/'etc'\n    with (etc_dir/'cgroup.conf').open('w') as f:\n        f.write(conf)\n\n    gpu_parts = [(i, x) for i, x in enumerate(cfg.partitions)\n                 if x.gpu_count]\n    gpu_conf = \"\"\n    for i, part in gpu_parts:\n        driver_range = '0'\n        if part.gpu_count \u003e 1:\n            driver_range = '[0-{}]'.format(part.gpu_count-1)\n\n        gpu_conf += (\"NodeName={}-{}-[0-{}] Name=gpu File=/dev/nvidia{}\\n\"\n                     .format(cfg.compute_node_prefix, i,\n                             part.max_node_count - 1, driver_range))\n    if gpu_conf:\n        with (etc_dir/'gres.conf').open('w') as f:\n            f.write(gpu_conf)\n\n# END install_cgroup_conf()\n\n\ndef install_meta_files():\n\n    scripts_path = APPS_DIR/'slurm/scripts'\n    if not scripts_path.exists():\n        scripts_path.mkdir(parents=True)\n\n    cfg.slurm_cmd_path = str(CURR_SLURM_DIR/'bin')\n    cfg.log_dir = str(SLURM_LOG)\n\n    cfg.save_config(scripts_path/'config.yaml')\n\n    meta_files = [\n        ('suspend.py', 'slurm_suspend'),\n        ('resume.py', 'slurm_resume'),\n        ('slurmsync.py', 'slurmsync'),\n        ('util.py', 'util_script'),\n        ('compute-shutdown', 'compute-shutdown'),\n        ('custom-compute-install', 'custom-compute-install'),\n        ('custom-controller-install', 'custom-controller-install'),\n    ]\n\n    for file_name, meta_name in meta_files:\n        text = util.get_metadata('attributes/' + meta_name)\n        if not text:\n            continue\n\n        with (scripts_path/file_name).open('w') as f:\n            f.write(text)\n        (scripts_path/file_name).chmod(0o755)\n\n    util.run(\n        \"gcloud compute instances remove-metadata {} --zone={} --keys={}\"\n        .format(CONTROL_MACHINE, cfg.zone,\n                ','.join([x[1] for x in meta_files])))\n\n# END install_meta_files()\n\n\ndef install_slurm():\n\n    src_path = APPS_DIR/'slurm/src'\n    if not src_path.exists():\n        src_path.mkdir(parents=True)\n\n    with cd(src_path):\n        use_version = ''\n        if (cfg.slurm_version[0:2] == 'b:'):\n            GIT_URL = 'https://github.com/SchedMD/slurm.git'\n            use_version = cfg.slurm_version[2:]\n            util.run(\n                \"git clone -b {0} {1} {0}\".format(use_version, GIT_URL))\n        else:\n            file = 'slurm-{}.tar.bz2'.format(cfg.slurm_version)\n            slurm_url = 'https://download.schedmd.com/slurm/' + file\n            urllib.request.urlretrieve(slurm_url, src_path/file)\n\n            use_version = util.run(f\"tar -xvjf {file}\", check=True,\n                                   get_stdout=True).stdout.splitlines()[0][:-1]\n\n    SLURM_PREFIX = APPS_DIR/'slurm'/use_version\n\n    build_dir = src_path/use_version/'build'\n    if not build_dir.exists():\n        build_dir.mkdir(parents=True)\n\n    with cd(build_dir):\n        util.run(\"../configure --prefix={} --sysconfdir={}/etc\"\n                 .format(SLURM_PREFIX, CURR_SLURM_DIR), stdout=DEVNULL)\n        util.run(\"make -j install\", stdout=DEVNULL)\n    with cd(build_dir/'contribs'):\n        util.run(\"make -j install\", stdout=DEVNULL)\n\n    os.symlink(SLURM_PREFIX, CURR_SLURM_DIR)\n\n    state_dir = APPS_DIR/'slurm/state'\n    if not state_dir.exists():\n        state_dir.mkdir(parents=True)\n        util.run(f\"chown -R slurm: {state_dir}\")\n\n    install_slurm_conf()\n    install_slurmdbd_conf()\n    install_cgroup_conf()\n    install_meta_files()\n\n# END install_slurm()\n\n\ndef install_slurm_tmpfile():\n\n    run_dir = Path('/var/run/slurm')\n\n    with open('/etc/tmpfiles.d/slurm.conf', 'w') as f:\n        f.write(f\"\\nd {run_dir} 0755 slurm slurm -\")\n\n    if not run_dir.exists():\n        run_dir.mkdir(parents=True)\n    run_dir.chmod(0o755)\n\n    util.run(f\"chown slurm: {run_dir}\")\n\n# END install_slurm_tmpfile()\n\n\ndef install_controller_service_scripts():\n\n    install_slurm_tmpfile()\n\n    # slurmctld.service\n    ctld_service = Path('/usr/lib/systemd/system/slurmctld.service')\n    with ctld_service.open('w') as f:\n        f.write(\"\"\"\n[Unit]\nDescription=Slurm controller daemon\nAfter=network.target munge.service\nConditionPathExists={prefix}/etc/slurm.conf\n\n[Service]\nType=forking\nEnvironmentFile=-/etc/sysconfig/slurmctld\nExecStart={prefix}/sbin/slurmctld $SLURMCTLD_OPTIONS\nExecReload=/bin/kill -HUP $MAINPID\nPIDFile=/var/run/slurm/slurmctld.pid\n\n[Install]\nWantedBy=multi-user.target\n\"\"\".format(prefix=CURR_SLURM_DIR))\n\n    ctld_service.chmod(0o644)\n\n    # slurmdbd.service\n    dbd_service = Path('/usr/lib/systemd/system/slurmdbd.service')\n    with dbd_service.open('w') as f:\n        f.write(\"\"\"\n[Unit]\nDescription=Slurm DBD accounting daemon\nAfter=network.target munge.service\nConditionPathExists={prefix}/etc/slurmdbd.conf\n\n[Service]\nType=forking\nEnvironmentFile=-/etc/sysconfig/slurmdbd\nExecStart={prefix}/sbin/slurmdbd $SLURMDBD_OPTIONS\nExecReload=/bin/kill -HUP $MAINPID\nPIDFile=/var/run/slurm/slurmdbd.pid\n\n[Install]\nWantedBy=multi-user.target\n\"\"\".format(prefix=CURR_SLURM_DIR))\n\n    dbd_service.chmod(0o644)\n\n# END install_controller_service_scripts()\n\n\ndef install_compute_service_scripts():\n\n    install_slurm_tmpfile()\n\n    # slurmd.service\n    slurmd_service = Path('/usr/lib/systemd/system/slurmd.service')\n    with slurmd_service.open('w') as f:\n        f.write(\"\"\"\n[Unit]\nDescription=Slurm node daemon\nAfter=network.target munge.service home.mount apps.mount etc-munge.mount\nConditionPathExists={prefix}/etc/slurm.conf\n\n[Service]\nType=forking\nEnvironmentFile=-/etc/sysconfig/slurmd\nExecStart={prefix}/sbin/slurmd $SLURMD_OPTIONS\nExecReload=/bin/kill -HUP $MAINPID\nPIDFile=/var/run/slurm/slurmd.pid\nKillMode=process\nLimitNOFILE=51200\nLimitMEMLOCK=infinity\nLimitSTACK=infinity\n\n[Install]\nWantedBy=multi-user.target\n\"\"\".format(prefix=CURR_SLURM_DIR))\n\n    slurmd_service.chmod(0o644)\n    util.run('systemctl enable slurmd')\n\n# END install_compute_service_scripts()\n\n\ndef setup_bash_profile():\n\n    with open('/etc/profile.d/slurm.sh', 'w') as f:\n        f.write(\"\"\"\nS_PATH={}\nPATH=$PATH:$S_PATH/bin:$S_PATH/sbin\n\"\"\".format(CURR_SLURM_DIR))\n\n    if cfg.instance_type == 'compute' and have_gpus(socket.gethostname()):\n        with open('/etc/profile.d/cuda.sh', 'w') as f:\n            f.write(\"\"\"\nCUDA_PATH=/usr/local/cuda\nPATH=$CUDA_PATH/bin${PATH:+:${PATH}}\nLD_LIBRARY_PATH=$CUDA_PATH/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\n\"\"\")\n\n# END setup_bash_profile()\n\n\ndef setup_ompi_bash_profile():\n    if not cfg.ompi_version:\n        return\n    with open(f'/etc/profile.d/ompi-{cfg.ompi_version}.sh', 'w') as f:\n        f.write(f\"PATH={APPS_DIR}/ompi/{cfg.ompi_version}/bin:$PATH\")\n# END setup_ompi_bash_profile()\n\n\ndef setup_logrotate():\n    with open('/etc/logrotate.d/slurm', 'w') as f:\n        f.write(\"\"\"\n##\n# Slurm Logrotate Configuration\n##\n/var/log/slurm/*.log {\n        compress\n        missingok\n        nocopytruncate\n        nodelaycompress\n        nomail\n        notifempty\n        noolddir\n        rotate 5\n        sharedscripts\n        size=5M\n        create 640 slurm root\n        postrotate\n                pkill -x --signal SIGUSR2 slurmctld\n                pkill -x --signal SIGUSR2 slurmd\n                pkill -x --signal SIGUSR2 slurmdbd\n                exit 0\n        endscript\n}\n\"\"\")\n# END setup_logrotate()\n\n\ndef setup_network_storage():\n    log.info(\"Set up network storage\")\n\n    global EXTERNAL_MOUNT_APPS\n    global EXTERNAL_MOUNT_HOME\n    global EXTERNAL_MOUNT_MUNGE\n\n    EXTERNAL_MOUNT_APPS = False\n    EXTERNAL_MOUNT_HOME = False\n    EXTERNAL_MOUNT_MUNGE = False\n    cifs_installed = False\n\n    # create dict of mounts, local_mount: mount_info\n    if cfg.instance_type == 'controller':\n        ext_mounts = {}\n    else:  # on non-controller instances, low priority mount these\n        CONTROL_NFS = {\n            'server_ip': CONTROL_MACHINE,\n            'remote_mount': 'none',\n            'local_mount': 'none',\n            'fs_type': 'nfs',\n            'mount_options': 'defaults,hard,intr',\n        }\n        ext_mounts = {\n            HOME_DIR: dict(CONTROL_NFS, remote_mount=HOME_DIR,\n                           local_mount=HOME_DIR),\n            APPS_DIR: dict(CONTROL_NFS, remote_mount=APPS_DIR,\n                           local_mount=APPS_DIR),\n            MUNGE_DIR: dict(CONTROL_NFS, remote_mount=MUNGE_DIR,\n                            local_mount=MUNGE_DIR),\n        }\n\n    # convert network_storage list of mounts to dict of mounts,\n    #   local_mount as key\n    def listtodict(mountlist):\n        return {Path(d['local_mount']).resolve(): d for d in mountlist}\n\n    ext_mounts.update(listtodict(cfg.network_storage))\n    if cfg.instance_type == 'compute':\n        pid = util.get_pid(socket.gethostname())\n        ext_mounts.update(listtodict(cfg.partitions[pid].network_storage))\n    else:\n        ext_mounts.update(listtodict(cfg.login_network_storage))\n\n    # Install lustre, cifs, and/or gcsfuse as needed and write mount to fstab\n    fstab_entries = []\n    for local_mount, mount in ext_mounts.items():\n        remote_mount = mount['remote_mount']\n        fs_type = mount['fs_type']\n        server_ip = mount['server_ip']\n        log.info(\"Setting up mount ({}) {}{} to {}\".format(\n            fs_type, server_ip+':' if fs_type != 'gcsfuse' else \"\",\n            remote_mount, local_mount))\n        if not local_mount.exists():\n            local_mount.mkdir(parents=True)\n        # Check if we're going to overlap with what's normally hosted on the\n        # controller (/apps, /home, /etc/munge).\n        # If so delete the entries pointing to the controller, and tell the\n        # nodes.\n        if local_mount == APPS_DIR:\n            EXTERNAL_MOUNT_APPS = True\n        elif local_mount == HOME_DIR:\n            EXTERNAL_MOUNT_HOME = True\n        elif local_mount == MUNGE_DIR:\n            EXTERNAL_MOUNT_MUNGE = True\n\n        lustre_path = Path('/sys/module/lustre')\n        gcsf_path = Path('/etc/yum.repos.d/gcsfuse.repo')\n\n        if fs_type == 'cifs' and not cifs_installed:\n            util.run(\"sudo yum install -y cifs-utils\")\n            cifs_installed = True\n        elif fs_type == 'lustre' and not lustre_path.exists():\n            lustre_url = 'https://downloads.whamcloud.com/public/lustre/latest-release/el7.7.1908/client/RPMS/x86_64/'\n            lustre_tmp = Path('/tmp/lustre')\n            lustre_tmp.mkdir(parents=True)\n            util.run('sudo yum update -y')\n            util.run('sudo yum install -y wget libyaml')\n            for rpm in ('kmod-lustre-client-2*.rpm', 'lustre-client-2*.rpm'):\n                util.run(\n                    f\"wget -r -l1 --no-parent -A '{rpm}' '{lustre_url}' -P {lustre_tmp}\")\n            util.run(\n                f\"find {lustre_tmp} -name '*.rpm' -execdir rpm -ivh {{}} ';'\")\n            util.run(f\"rm -rf {lustre_tmp}\")\n            util.run(\"modprobe lustre\")\n        elif fs_type == 'gcsfuse' and not gcsf_path.exists():\n            with gcsf_path.open('a') as f:\n                f.write(\"\"\"\n[gcsfuse]\nname=gcsfuse (packages.cloud.google.com)\nbaseurl=https://packages.cloud.google.com/yum/repos/gcsfuse-el7-x86_64\nenabled=1\ngpgcheck=1\nrepo_gpgcheck=1\ngpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg\nhttps://packages.cloud.google.com/yum/doc/rpm-package-key.gpg\"\"\")\n            util.run(\"sudo yum update -y\")\n            util.run(\"sudo yum install -y gcsfuse\")\n\n        mount_options = mount['mount_options']\n        if fs_type == 'gcsfuse':\n            if 'nonempty' not in mount['mount_options']:\n                mount_options += \",nonempty\"\n            fstab_entries.append(\n                \"\\n{0}   {1}     {2}     {3}     0 0\"\n                .format(remote_mount, local_mount, fs_type, mount_options))\n        else:\n            remote_mount = Path(remote_mount).resolve()\n            fstab_entries.append(\n                \"\\n{0}:{1}    {2}     {3}      {4}  0 0\"\n                .format(server_ip, remote_mount, local_mount,\n                        fs_type, mount_options))\n\n    with open('/etc/fstab', 'a') as f:\n        for entry in fstab_entries:\n            f.write(entry)\n# END setup_network_storage()\n\n\ndef setup_secondary_disks():\n\n    if not SEC_DISK_DIR.exists():\n        SEC_DISK_DIR.mkdir(parents=True)\n    util.run(\n        \"sudo mkfs.ext4 -m 0 -F -E lazy_itable_init=0,lazy_journal_init=0,discard /dev/sdb\")\n    with open('/etc/fstab', 'a') as f:\n        f.write(\n            \"\\n/dev/sdb     {0}     ext4    discard,defaults,nofail     0 2\"\n            .format(SEC_DISK_DIR))\n\n# END setup_secondary_disks()\n\n\ndef mount_nfs_vols():\n\n    mount_paths = (\n        (HOME_DIR, EXTERNAL_MOUNT_HOME),\n        (APPS_DIR, EXTERNAL_MOUNT_APPS),\n        (MUNGE_DIR, EXTERNAL_MOUNT_MUNGE),\n    )\n    # compress yields values from the first arg that are matched with True\n    # in the second arg. The result is the paths filtered by the booleans.\n    # For non-controller instances, all three are always external nfs\n    for path in it.compress(*zip(*mount_paths)):\n        while not os.path.ismount(path):\n            log.info(f\"Waiting for {path} to be mounted\")\n            util.run(f\"mount {path}\", wait=5)\n    util.run(\"mount -a\", wait=1)\n\n# END mount_nfs_vols()\n\n\n# Tune the NFS server to support many mounts\ndef setup_nfs_threads():\n\n    with open('/etc/sysconfig/nfs', 'a') as f:\n        f.write(\"\"\"\n# Added by Google\nRPCNFSDCOUNT=256\n\"\"\")\n\n# END setup_nfs_threads()\n\n\ndef setup_sync_cronjob():\n\n    util.run(\"crontab -u slurm -\", input=(\n        f\"*/1 * * * * {APPS_DIR}/slurm/scripts/slurmsync.py\\n\"))\n\n# END setup_sync_cronjob()\n\n\ndef setup_slurmd_cronjob():\n    util.run(\n        \"crontab -u root -\", input=(\n            \"*/2 * * * * \"\n            \"if [ `systemctl status slurmd | grep -c inactive` -gt 0 ]; then \"\n            \"mount -a; \"\n            \"systemctl restart munge; \"\n            \"systemctl restart slurmd; \"\n            \"fi\\n\"\n        ))\n# END setup_slurmd_cronjob()\n\n\ndef create_compute_images():\n\n    def create_compute_image(instance, partition):\n        try:\n            compute = googleapiclient.discovery.build('compute', 'v1',\n                                                      cache_discovery=False)\n\n            while True:\n                resp = compute.instances().get(\n                    project=cfg.project, zone=cfg.zone, fields=\"status\",\n                    instance=instance).execute()\n                if resp['status'] == 'TERMINATED':\n                    break\n                log.info(f\"waiting for {instance} to be stopped (status: {resp['status']})\"\n                         .format(instance=instance, status=resp['status']))\n                time.sleep(30)\n\n            log.info(\"Creating image of {}...\".format(instance))\n            ver = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n            util.run(f\"gcloud compute images create \"\n                     f\"{instance}-{ver} --source-disk {instance} \"\n                     f\"--source-disk-zone {cfg.zone} --force \"\n                     f\"--family {instance}-family\")\n\n            util.run(\"{}/bin/scontrol update partitionname={} state=up\"\n                     .format(CURR_SLURM_DIR, partition.name))\n        except Exception as e:\n            log.exception(f\"{instance} not found: {e}\")\n\n    threads = []\n    for i, part in enumerate(cfg.partitions):\n        instance = f\"{cfg.compute_node_prefix}-{i}-image\"\n        thread = threading.Thread(target=create_compute_image,\n                                  args=(instance, part))\n        threads.append(thread)\n        thread.start()\n\n    for thread in threads:\n        thread.join()\n\n# END create_compute_image()\n\n\ndef setup_selinux():\n\n    util.run('setenforce 0')\n    with open('/etc/selinux/config', 'w') as f:\n        f.write(\"\"\"\nSELINUX=permissive\nSELINUXTYPE=targeted\n\"\"\")\n# END setup_selinux()\n\n\ndef install_ompi():\n\n    if not cfg.ompi_version:\n        return\n\n    packages = ['autoconf',\n                'flex',\n                'gcc-c++',\n                'libevent-devel',\n                'libtool']\n    util.run(\"yum install -y {}\".format(' '.join(packages)))\n\n    ompi_git = \"https://github.com/open-mpi/ompi.git\"\n    ompi_path = APPS_DIR/'ompi'/cfg.ompi_version/'src'\n    if not ompi_path.exists():\n        ompi_path.mkdir(parents=True)\n    util.run(f\"git clone -b {cfg.ompi_version} {ompi_git} {ompi_path}\")\n    with cd(ompi_path):\n        util.run(\"./autogen.pl\", stdout=DEVNULL)\n\n    build_path = ompi_path/'build'\n    if not build_path.exists():\n        build_path.mkdir(parents=True)\n    with cd(build_path):\n        util.run(\n            f\"../configure --prefix={APPS_DIR}/ompi/{cfg.ompi_version} \"\n            f\"--with-pmi={APPS_DIR}/slurm/current --with-libevent=/usr \"\n            \"--with-hwloc=/usr\", stdout=DEVNULL)\n        util.run(\"make -j install\", stdout=DEVNULL)\n# END install_ompi()\n\n\ndef remove_startup_scripts(hostname):\n\n    cmd = \"gcloud compute instances remove-metadata\"\n    common_keys = \"startup-script,setup_script,util_script,config\"\n    controller_keys = (f\"{common_keys},\"\n                       \"slurm_conf_tpl,slurmdbd_conf_tpl,cgroup_conf_tpl\")\n    compute_keys = f\"{common_keys},slurm_fluentd_log_tpl\"\n\n    # controller\n    util.run(f\"{cmd} {hostname} --zone={cfg.zone} --keys={controller_keys}\")\n\n    # logins\n    for i in range(0, cfg.login_node_count):\n        util.run(\"{} {}-login{} --zone={} --keys={}\"\n                 .format(cmd, cfg.cluster_name, i, cfg.zone, common_keys))\n    # computes\n    for i, part in enumerate(cfg.partitions):\n        # partition compute image\n        util.run(f\"{cmd} {cfg.compute_node_prefix}-{i}-image \"\n                 f\"--zone={cfg.zone} --keys={compute_keys}\")\n        if not part.static_node_count:\n            continue\n        for j in range(part.static_node_count):\n            util.run(\"{} {}-{}-{} --zone={} --keys={}\"\n                     .format(cmd, cfg.compute_node_prefix, i, j,\n                             part.zone, compute_keys))\n# END remove_startup_scripts()\n\n\ndef setup_nss_slurm():\n\n    # setup nss_slurm\n    util.run(\"ln -s {}/lib/libnss_slurm.so.2 /usr/lib64/libnss_slurm.so.2\"\n             .format(CURR_SLURM_DIR))\n    util.run(\n        r\"sed -i 's/\\(^\\(passwd\\|group\\):\\s\\+\\)/\\1slurm /g' /etc/nsswitch.conf\"\n    )\n# END setup_nss_slurm()\n\n\ndef main():\n    hostname = socket.gethostname()\n\n    setup_selinux()\n\n    start_motd()\n\n    add_slurm_user()\n    install_packages()\n    setup_munge()\n    setup_bash_profile()\n    setup_ompi_bash_profile()\n    setup_modules()\n\n    if cfg.controller_secondary_disk and cfg.instance_type == 'controller':\n        setup_secondary_disks()\n    setup_network_storage()\n\n    if not SLURM_LOG.exists():\n        SLURM_LOG.mkdir(parents=True)\n    shutil.chown(SLURM_LOG, user='slurm', group='slurm')\n\n    if cfg.instance_type == 'controller':\n        mount_nfs_vols()\n        time.sleep(5)\n        start_munge()\n        install_slurm()\n        install_ompi()\n\n        try:\n            util.run(str(APPS_DIR/'slurm/scripts/custom-controller-install'))\n        except Exception:\n            # Ignore blank files with no shell magic.\n            pass\n\n        install_controller_service_scripts()\n\n        if not cfg.cloudsql:\n            util.run('systemctl enable mariadb')\n            util.run('systemctl start mariadb')\n\n            mysql = \"mysql -u root -e\"\n            util.run(\n                f\"\"\"{mysql} \"create user 'slurm'@'localhost'\";\"\"\")\n            util.run(\n                f\"\"\"{mysql} \"grant all on slurm_acct_db.* TO 'slurm'@'localhost'\";\"\"\")\n            util.run(\n                f\"\"\"{mysql} \"grant all on slurm_acct_db.* TO 'slurm'@'{CONTROL_MACHINE}'\";\"\"\")\n\n        util.run(\"systemctl enable slurmdbd\")\n        util.run(\"systemctl start slurmdbd\")\n\n        # Wait for slurmdbd to come up\n        time.sleep(5)\n\n        sacctmgr = f\"{CURR_SLURM_DIR}/bin/sacctmgr -i\"\n        util.run(f\"{sacctmgr} add cluster {cfg.cluster_name}\")\n\n        util.run(\"systemctl enable slurmctld\")\n        util.run(\"systemctl start slurmctld\")\n        setup_nfs_threads()\n        # Export at the end to signal that everything is up\n        util.run(\"systemctl enable nfs-server\")\n        util.run(\"systemctl start nfs-server\")\n        setup_nfs_exports()\n\n        setup_sync_cronjob()\n\n        # DOWN partitions until image is created.\n        for part in cfg.partitions:\n            util.run(\"{}/bin/scontrol update partitionname={} state=down\"\n                     .format(CURR_SLURM_DIR, part.name))\n\n        create_compute_images()\n        remove_startup_scripts(hostname)\n        log.info(\"Done installing controller\")\n    elif cfg.instance_type == 'compute':\n        install_compute_service_scripts()\n        mount_nfs_vols()\n        start_munge()\n        setup_nss_slurm()\n        setup_slurmd_cronjob()\n\n        try:\n            util.run(str(APPS_DIR/'slurm/scripts/custom-compute-install'))\n        except Exception:\n            # Ignore blank files with no shell magic.\n            pass\n\n        if hostname.endswith('-image'):\n            end_motd(False)\n            util.run(\"sync\")\n            util.run(f\"gcloud compute instances stop {hostname} \"\n                     f\"--zone {cfg.zone} --quiet\")\n        else:\n            util.run(\"systemctl start slurmd\")\n\n    else:  # login nodes\n        mount_nfs_vols()\n        start_munge()\n\n        try:\n            util.run(str(APPS_DIR/\"slurm/scripts/custom-compute-install\"))\n        except Exception:\n            # Ignore blank files with no shell magic.\n            pass\n\n    setup_logrotate()\n\n    end_motd()\n\n# END main()\n\n\nif __name__ == '__main__':\n    main()\n\n",
              "startup-script": "#!/bin/bash\n# Copyright 2019 SchedMD LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nPACKAGES=(\n        'bind-utils'\n        'environment-modules'\n        'epel-release'\n        'gcc'\n        'git'\n        'hwloc'\n        'hwloc-devel'\n        'libibmad'\n        'libibumad'\n        'lua'\n        'lua-devel'\n        'man2html'\n        'mariadb'\n        'mariadb-devel'\n        'mariadb-server'\n        'munge'\n        'munge-devel'\n        'munge-libs'\n        'ncurses-devel'\n        'nfs-utils'\n        'numactl'\n        'numactl-devel'\n        'openssl-devel'\n        'pam-devel'\n        'perl-ExtUtils-MakeMaker'\n        'python3'\n        'python3-pip'\n        'readline-devel'\n        'rpm-build'\n        'rrdtool-devel'\n        'vim'\n        'wget'\n        'tmux'\n        'pdsh'\n        'openmpi'\n        'yum-utils'\n    )\n\nPY_PACKAGES=(\n        'pyyaml'\n        'requests'\n        'google-api-python-client'\n    )\n\nPING_HOST=8.8.8.8\nuntil ( ping -q -w1 -c1 $PING_HOST \u003e /dev/null ) ; do\n    echo \"Waiting for internet\"\n    sleep .5\ndone\n\necho \"yum install -y ${PACKAGES[*]}\"\nuntil ( yum install -y ${PACKAGES[*]} \u003e /dev/null ) ; do\n    echo \"yum failed to install packages. Trying again in 5 seconds\"\n    sleep 5\ndone\n\necho   \"pip3 install --upgrade ${PY_PACKAGES[*]}\"\nuntil ( pip3 install --upgrade ${PY_PACKAGES[*]} ) ; do\n    echo \"pip3 failed to install python packages. Trying again in 5 seconds\"\n    sleep 5\ndone\n\nSETUP_SCRIPT=\"setup.py\"\nSETUP_META=\"setup_script\"\nDIR=\"/tmp\"\nURL=\"http://metadata.google.internal/computeMetadata/v1/instance/attributes/$SETUP_META\"\nHEADER=\"Metadata-Flavor:Google\"\necho  \"wget -nv --header $HEADER $URL -O $DIR/$SETUP_SCRIPT\"\nif ! ( wget -nv --header $HEADER $URL -O $DIR/$SETUP_SCRIPT ) ; then\n    echo \"Failed to fetch $SETUP_META:$SETUP_SCRIPT from metadata\"\n    exit 1\nfi\n\necho \"running python cluster setup script\"\nchmod +x $DIR/$SETUP_SCRIPT\n$DIR/$SETUP_SCRIPT\n\n",
              "terraform": "TRUE",
              "util_script": "#!/usr/bin/env python3\n# Copyright 2019 SchedMD LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport logging\nimport logging.config\nimport os\nimport shlex\nimport subprocess\nimport sys\nimport time\nfrom pathlib import Path\nfrom contextlib import contextmanager\nfrom collections import OrderedDict\n\nimport requests\nimport yaml\n\n\nlog = logging.getLogger(__name__)\n\n\ndef config_root_logger(level='DEBUG', util_level=None, file=None):\n    if not util_level:\n        util_level = level\n    handler = 'file_handler' if file else 'stdout_handler'\n    config = {\n        'version': 1,\n        'disable_existing_loggers': True,\n        'formatters': {\n            'standard': {\n                'format': '',\n            },\n            'stamp': {\n                'format': '%(asctime)s %(name)s %(levelname)s: %(message)s',\n            },\n        },\n        'handlers': {\n            'stdout_handler': {\n                'level': 'DEBUG',\n                'formatter': 'standard',\n                'class': 'logging.StreamHandler',\n                'stream': sys.stdout,\n            },\n        },\n        'loggers': {\n            '': {\n                'handlers': [handler],\n                'level': level,\n            },\n            __name__: {  # enable util.py logging\n                'level': util_level,\n            }\n        },\n    }\n    if file:\n        config['handlers']['file_handler'] = {\n            'level': 'DEBUG',\n            'formatter': 'stamp',\n            'class': 'logging.handlers.WatchedFileHandler',\n            'filename': file,\n        }\n    logging.config.dictConfig(config)\n\n\ndef get_metadata(path):\n    \"\"\" Get metadata relative to metadata/computeMetadata/v1/instance/ \"\"\"\n    URL = 'http://metadata.google.internal/computeMetadata/v1/instance/'\n    HEADERS = {'Metadata-Flavor': 'Google'}\n    full_path = URL + path\n    try:\n        resp = requests.get(full_path, headers=HEADERS)\n        resp.raise_for_status()\n    except requests.exceptions.RequestException:\n        log.exception(f\"Error while getting metadata from {full_path}\")\n        return None\n    return resp.text\n\n\ndef run(cmd, wait=0, quiet=False, get_stdout=False,\n        shell=False, universal_newlines=True, **kwargs):\n    \"\"\" run in subprocess. Optional wait after return. \"\"\"\n    if not quiet:\n        log.debug(f\"run: {cmd}\")\n    if get_stdout:\n        kwargs['stdout'] = subprocess.PIPE\n\n    args = cmd if shell else shlex.split(cmd)\n    ret = subprocess.run(args, shell=shell,\n                         universal_newlines=universal_newlines,\n                         **kwargs)\n    if wait:\n        time.sleep(wait)\n    return ret\n\n\ndef spawn(cmd, quiet=False, shell=False, **kwargs):\n    \"\"\" nonblocking spawn of subprocess \"\"\"\n    if not quiet:\n        log.debug(f\"spawn: {cmd}\")\n    args = cmd if shell else shlex.split(cmd)\n    return subprocess.Popen(args, shell=shell, **kwargs)\n\n\ndef get_pid(node_name):\n    \"\"\"Convert \u003cprefix\u003e-\u003cpid\u003e-\u003cnid\u003e\"\"\"\n\n    return int(node_name.split('-')[-2])\n\n\n@contextmanager\ndef cd(path):\n    \"\"\" Change working directory for context \"\"\"\n    prev = Path.cwd()\n    os.chdir(path)\n    try:\n        yield\n    finally:\n        os.chdir(prev)\n\n\ndef static_vars(**kwargs):\n    \"\"\"\n    Add variables to the function namespace.\n    @static_vars(var=init): var must be referenced func.var\n    \"\"\"\n    def decorate(func):\n        for k in kwargs:\n            setattr(func, k, kwargs[k])\n        return func\n    return decorate\n\n\nclass cached_property:\n    \"\"\"\n    Descriptor for creating a property that is computed once and cached\n    \"\"\"\n    def __init__(self, factory):\n        self._attr_name = factory.__name__\n        self._factory = factory\n\n    def __get__(self, instance, owner=None):\n        if instance is None:  # only if invoked from class\n            return self\n        attr = self._factory(instance)\n        setattr(instance, self._attr_name, attr)\n        return attr\n\n\nclass Config(OrderedDict):\n    \"\"\" Loads config from yaml and holds values in nested namespaces \"\"\"\n\n    TYPES = set(('compute', 'login', 'controller'))\n    # PROPERTIES defines which properties in slurm.jinja.schema are included\n    #   in the config file. SAVED_PROPS are saved to file via save_config.\n    SAVED_PROPS = ('project',\n                   'zone',\n                   'cluster_name',\n                   'external_compute_ips',\n                   'shared_vpc_host_project',\n                   'compute_node_prefix',\n                   'compute_node_service_account',\n                   'compute_node_scopes',\n                   'slurm_cmd_path',\n                   'log_dir',\n                   'google_app_cred_path',\n                   'update_node_addrs',\n                   'partitions',\n                   )\n    PROPERTIES = (*SAVED_PROPS,\n                  'munge_key',\n                  'external_compute_ips',\n                  'nfs_home_server',\n                  'nfs_home_dir',\n                  'nfs_apps_server',\n                  'nfs_apps_dir',\n                  'ompi_version',\n                  'controller_secondary_disk',\n                  'slurm_version',\n                  'suspend_time',\n                  'network_storage',\n                  'login_network_storage',\n                  'login_node_count',\n                  'cloudsql',\n                  )\n\n    def __init__(self, *args, **kwargs):\n        def from_nested(value):\n            \"\"\" If value is dict, convert to Config. Also recurse lists. \"\"\"\n            if isinstance(value, dict):\n                return Config({k: from_nested(v) for k, v in value.items()})\n            elif isinstance(value, list):\n                return [from_nested(v) for v in value]\n            else:\n                return value\n\n        super(Config, self).__init__(*args, **kwargs)\n        self.__dict__ = self  # all properties are member attributes\n\n        # Convert nested dicts to Configs\n        for k, v in self.items():\n            self[k] = from_nested(v)\n\n    @classmethod\n    def new_config(cls, properties):\n        # If k is ever not found, None will be inserted as the value\n        return cls({k: properties.setdefault(k, None) for k in cls.PROPERTIES})\n\n    @classmethod\n    def load_config(cls, path):\n        config = yaml.safe_load(Path(path).read_text())\n        return cls(config)\n\n    def save_config(self, path):\n        save_dict = Config([(k, self[k]) for k in self.SAVED_PROPS])\n        Path(path).write_text(yaml.dump(save_dict, Dumper=self.Dumper))\n\n    @cached_property\n    def instance_type(self):\n        # get tags, intersect with possible types, get the first or none\n        tags = yaml.safe_load(get_metadata('tags'))\n        # TODO what to default to if no match found.\n        return next(iter(set(tags) \u0026 self.TYPES), None)\n\n    @property\n    def region(self):\n        return self.zone and '-'.join(self.zone.split('-')[:-1])\n\n    def __getattr__(self, item):\n        \"\"\" only called if item is not found in self \"\"\"\n        return None\n\n    class Dumper(yaml.SafeDumper):\n        def __init__(self, *args, **kwargs):\n            super().__init__(*args, **kwargs)\n            self.add_representer(Config, self.represent_config)\n\n        @staticmethod\n        def represent_config(dumper, data):\n            return dumper.represent_mapping('tag:yaml.org,2002:map',\n                                            data.items())\n\n"
            },
            "metadata_fingerprint": "bhxlKxhaoxc=",
            "metadata_startup_script": null,
            "min_cpu_platform": "",
            "name": "gcluster-compute-0-image",
            "network_interface": [
              {
                "access_config": [
                  {
                    "nat_ip": "34.82.93.133",
                    "network_tier": "PREMIUM",
                    "public_ptr_domain_name": ""
                  }
                ],
                "alias_ip_range": [],
                "name": "nic0",
                "network": "https://www.googleapis.com/compute/v1/projects/playground-190420/global/networks/vf-network",
                "network_ip": "192.168.0.26",
                "subnetwork": "https://www.googleapis.com/compute/v1/projects/playground-190420/regions/us-west1/subnetworks/vf-west-subnet",
                "subnetwork_project": "playground-190420"
              }
            ],
            "project": "playground-190420",
            "scheduling": [
              {
                "automatic_restart": true,
                "node_affinities": [],
                "on_host_maintenance": "MIGRATE",
                "preemptible": false
              }
            ],
            "scratch_disk": [],
            "self_link": "https://www.googleapis.com/compute/v1/projects/playground-190420/zones/us-west1-a/instances/gcluster-compute-0-image",
            "service_account": [
              {
                "email": "861735104433-compute@developer.gserviceaccount.com",
                "scopes": [
                  "https://www.googleapis.com/auth/cloud-platform"
                ]
              }
            ],
            "shielded_instance_config": [
              {
                "enable_integrity_monitoring": true,
                "enable_secure_boot": false,
                "enable_vtpm": true
              }
            ],
            "tags": [
              "compute"
            ],
            "tags_fingerprint": "micHwuEN8fk=",
            "timeouts": null,
            "zone": "us-west1-a"
          },
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjoxMjAwMDAwMDAwMDAwLCJkZWxldGUiOjEyMDAwMDAwMDAwMDAsInVwZGF0ZSI6MTIwMDAwMDAwMDAwMH0sInNjaGVtYV92ZXJzaW9uIjoiNiJ9",
          "dependencies": [
            "module.slurm_cluster_network.google_compute_subnetwork.cluster_subnet"
          ]
        }
      ]
    },
    {
      "module": "module.slurm_cluster_controller",
      "mode": "managed",
      "type": "google_compute_instance",
      "name": "controller_node",
      "provider": "provider.google",
      "instances": [
        {
          "schema_version": 6,
          "attributes": {
            "allow_stopping_for_update": null,
            "attached_disk": [],
            "boot_disk": [
              {
                "auto_delete": true,
                "device_name": "persistent-disk-0",
                "disk_encryption_key_raw": "",
                "disk_encryption_key_sha256": "",
                "initialize_params": [
                  {
                    "image": "https://www.googleapis.com/compute/v1/projects/centos-cloud/global/images/centos-7-v20200403",
                    "labels": {},
                    "size": 50,
                    "type": "pd-standard"
                  }
                ],
                "kms_key_self_link": "",
                "mode": "READ_WRITE",
                "source": "https://www.googleapis.com/compute/v1/projects/playground-190420/zones/us-west1-a/disks/gcluster-controller"
              }
            ],
            "can_ip_forward": false,
            "cpu_platform": "Intel Broadwell",
            "current_status": "RUNNING",
            "deletion_protection": false,
            "description": "",
            "desired_status": null,
            "enable_display": false,
            "guest_accelerator": [],
            "hostname": "",
            "id": "projects/playground-190420/zones/us-west1-a/instances/gcluster-controller",
            "instance_id": "26317272523512023",
            "label_fingerprint": "42WmSpB8rSM=",
            "labels": null,
            "machine_type": "n1-standard-2",
            "metadata": {
              "VmDnsSetting": "GlobalOnly",
              "cgroup_conf_tpl": "CgroupAutomount=no\n#CgroupMountpoint=/sys/fs/cgroup\nConstrainCores=yes\nConstrainRamSpace=yes\nConstrainSwapSpace=yes\nTaskAffinity=no\nConstrainDevices=yes\n\n",
              "compute-shutdown": "#!/bin/sh\n\nkillall slurmd\n\nrm -rf /var/spool/slurmd/*\n\n\n",
              "config": "{\"cloudsql\":null,\"cluster_name\":\"gcluster\",\"compute_node_prefix\":\"gcluster-compute\",\"compute_node_scopes\":[\"https://www.googleapis.com/auth/monitoring.write\",\"https://www.googleapis.com/auth/logging.write\"],\"compute_node_service_account\":\"default\",\"controller_secondary_disk\":false,\"external_compute_ips\":true,\"login_network_storage\":[],\"login_node_count\":1,\"munge_key\":null,\"network_storage\":[{\"fs_type\":\"nfs\",\"local_mount\":\"/mnt/virtualflow\",\"mount_options\":\"defaults,hard,intr\",\"remote_mount\":\"/virtualflow\",\"server_ip\":\"10.102.14.42\"}],\"ompi_version\":null,\"partitions\":[{\"compute_disk_size_gb\":20,\"compute_disk_type\":\"pd-standard\",\"compute_labels\":{},\"cpu_platform\":null,\"gpu_count\":0,\"gpu_type\":null,\"machine_type\":\"n1-standard-2\",\"max_node_count\":10,\"name\":\"debug\",\"network_storage\":[],\"preemptible_bursting\":true,\"static_node_count\":0,\"vpc_subnet\":\"vf-west-subnet\",\"zone\":\"us-west1-a\"}],\"project\":\"playground-190420\",\"region\":\"us-west1\",\"shared_vpc_host_project\":null,\"slurm_version\":\"19.05-latest\",\"suspend_time\":300,\"vpc_subnet\":\"vf-west-subnet\",\"zone\":\"us-west1-a\"}\n",
              "custom-compute-install": "#!/usr/bin/env bash\nsudo yum -y install bc\nsudo yum -y install openbabel\nsudo mount 10.102.14.42:/virtualflow /mnt/virtualflow/\n\n\n",
              "custom-controller-install": "#!/usr/bin/env bash\n#sudo echo \"10.102.14.42:/virtualflow /mnt/virtualflow nfs defaults,hard,intr 0 0\" \u003e\u003e /etc/fstab\nsudo yum -y install bc\nsudo yum -y install openbabel\nsudo mount 10.102.14.42:/virtualflow /mnt/virtualflow/\n\n\n",
              "enable-oslogin": "TRUE",
              "fluentd_conf_tpl": "# This filter goes with all of the slurm sources. It maps slurm log levels\n# to logging api severity levels and prepends the tag to the log message.\n\u003cfilter slurmctld slurmdbd slurmd\u003e\n  @type record_transformer\n  enable_ruby true\n  \u003crecord\u003e\n    severity ${ {'debug'=\u003e'DEBUG', 'debug2'=\u003e'DEBUG', 'debug3'=\u003e'DEBUG', 'debug4'=\u003e'DEBUG', 'debug5'=\u003e'DEBUG', 'error'=\u003e'ERROR', 'fatal'=\u003e'CRITICAL'}.tap{|map| map.default='INFO'}[record['severity']] }\n    message ${tag + \" \" + record['message']}\n  \u003c/record\u003e\n\u003c/filter\u003e\n\n\u003csource\u003e\n  @type tail\n  tag slurmctld\n  path /var/log/slurm/slurmctld.log\n  pos_file /var/lib/google-fluentd/pos/slurm_slurmctld.log.pos\n  read_from_head true\n  \u003cparse\u003e\n    @type regexp\n    expression /^\\[(?\u003ctime\u003e[^\\]]*)\\] (?\u003cmessage\u003e(?\u003cseverity\u003e\\w*).*)$/\n    time_format %Y-%m-%dT%H:%M:%S.%N\n  \u003c/parse\u003e\n\u003c/source\u003e\n\n\u003csource\u003e\n  @type tail\n  tag slurmdbd\n  path /var/log/slurm/slurmdbd.log\n  pos_file /var/lib/google-fluentd/pos/slurm_slurmdbd.log.pos\n  read_from_head true\n  \u003cparse\u003e\n    @type regexp\n    expression /^\\[(?\u003ctime\u003e[^\\]]*)\\] (?\u003cmessage\u003e(?\u003cseverity\u003e\\w*).*)$/\n    time_format %Y-%m-%dT%H:%M:%S.%N\n  \u003c/parse\u003e\n\u003c/source\u003e\n\n\u003csource\u003e\n  @type tail\n  tag resume\n  path /var/log/slurm/resume.log\n  pos_file /var/lib/google-fluentd/pos/slurm_resume.log.pos\n  read_from_head true\n  \u003cparse\u003e\n    @type regexp\n    expression /^(?\u003ctime\u003e\\S+ \\S+) (?\u003cmessage\u003e\\S+ (?\u003cseverity\u003e\\S+):.*)$/\n    time_format %Y-%m-%d %H:%M:%S,%N\n  \u003c/parse\u003e\n\u003c/source\u003e\n\n\u003csource\u003e\n  @type tail\n  tag suspend\n  path /var/log/slurm/suspend.log\n  pos_file /var/lib/google-fluentd/pos/slurm_suspend.log.pos\n  read_from_head true\n  \u003cparse\u003e\n    @type regexp\n    expression /^(?\u003ctime\u003e\\S+ \\S+) (?\u003cmessage\u003e\\S+ (?\u003cseverity\u003e\\S+):.*)$/\n    time_format %Y-%m-%d %H:%M:%S,%N\n  \u003c/parse\u003e\n\u003c/source\u003e\n\n\u003csource\u003e\n  @type tail\n  tag slurmsync\n  path /var/log/slurm/slurmsync.log\n  pos_file /var/lib/google-fluentd/pos/slurm_slurmsync.log.pos\n  read_from_head true\n  \u003cparse\u003e\n    @type regexp\n    expression /^(?\u003ctime\u003e\\S+ \\S+) (?\u003cmessage\u003e\\S+ (?\u003cseverity\u003e\\S+):.*)$/\n    time_format %Y-%m-%d %H:%M:%S,%N\n  \u003c/parse\u003e\n\u003c/source\u003e\n\n",
              "setup_script": "#!/usr/bin/env python3\n\n# Copyright 2017 SchedMD LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport datetime\nimport importlib\nimport itertools as it\nimport logging\nimport os\nimport shutil\nimport socket\nimport sys\nimport time\nimport threading\nimport urllib.request\nfrom pathlib import Path\nfrom subprocess import DEVNULL\n\nimport googleapiclient.discovery\nimport requests\nimport yaml\n\n\n# get util.py from metadata\nUTIL_FILE = Path('/tmp/util.py')\ntry:\n    resp = requests.get('http://metadata.google.internal/computeMetadata/v1/instance/attributes/util_script',\n                        headers={'Metadata-Flavor': 'Google'})\n    resp.raise_for_status()\n    UTIL_FILE.write_text(resp.text)\nexcept requests.exceptions.RequestException:\n    print(\"util.py script not found in metadata\")\n    if not UTIL_FILE.exists():\n        print(f\"{UTIL_FILE} also does not exist, aborting\")\n        sys.exit(1)\n\nspec = importlib.util.spec_from_file_location('util', UTIL_FILE)\nutil = importlib.util.module_from_spec(spec)\nsys.modules[spec.name] = util\nspec.loader.exec_module(util)\ncd = util.cd  # import util.cd into local namespace\n\nutil.config_root_logger()\nlog = logging.getLogger(Path(__file__).name)\n\n# get setup config from metadata\nconfig_yaml = yaml.safe_load(util.get_metadata('attributes/config'))\nif not util.get_metadata('attributes/terraform'):\n    config_yaml = yaml.safe_load(config_yaml)\ncfg = util.Config.new_config(config_yaml)\n\nHOME_DIR = Path('/home')\nAPPS_DIR = Path('/apps')\nCURR_SLURM_DIR = APPS_DIR/'slurm/current'\nMUNGE_DIR = Path('/etc/munge')\nSLURM_LOG = Path('/var/log/slurm')\n\nSEC_DISK_DIR = Path('/mnt/disks/sec')\nRESUME_TIMEOUT = 300\nSUSPEND_TIMEOUT = 300\n\nCONTROL_MACHINE = cfg.cluster_name + '-controller'\n\nMOTD_HEADER = \"\"\"\n\n                                 SSSSSSS\n                                SSSSSSSSS\n                                SSSSSSSSS\n                                SSSSSSSSS\n                        SSSS     SSSSSSS     SSSS\n                       SSSSSS               SSSSSS\n                       SSSSSS    SSSSSSS    SSSSSS\n                        SSSS    SSSSSSSSS    SSSS\n                SSS             SSSSSSSSS             SSS\n               SSSSS    SSSS    SSSSSSSSS    SSSS    SSSSS\n                SSS    SSSSSS   SSSSSSSSS   SSSSSS    SSS\n                       SSSSSS    SSSSSSS    SSSSSS\n                SSS    SSSSSS               SSSSSS    SSS\n               SSSSS    SSSS     SSSSSSS     SSSS    SSSSS\n          S     SSS             SSSSSSSSS             SSS     S\n         SSS            SSSS    SSSSSSSSS    SSSS            SSS\n          S     SSS    SSSSSS   SSSSSSSSS   SSSSSS    SSS     S\n               SSSSS   SSSSSS   SSSSSSSSS   SSSSSS   SSSSS\n          S    SSSSS    SSSS     SSSSSSS     SSSS    SSSSS    S\n    S    SSS    SSS                                   SSS    SSS    S\n    S     S                                                   S     S\n                SSS\n                SSS\n                SSS\n                SSS\n SSSSSSSSSSSS   SSS   SSSS       SSSS    SSSSSSSSS   SSSSSSSSSSSSSSSSSSSS\nSSSSSSSSSSSSS   SSS   SSSS       SSSS   SSSSSSSSSS  SSSSSSSSSSSSSSSSSSSSSS\nSSSS            SSS   SSSS       SSSS   SSSS        SSSS     SSSS     SSSS\nSSSS            SSS   SSSS       SSSS   SSSS        SSSS     SSSS     SSSS\nSSSSSSSSSSSS    SSS   SSSS       SSSS   SSSS        SSSS     SSSS     SSSS\n SSSSSSSSSSSS   SSS   SSSS       SSSS   SSSS        SSSS     SSSS     SSSS\n         SSSS   SSS   SSSS       SSSS   SSSS        SSSS     SSSS     SSSS\n         SSSS   SSS   SSSS       SSSS   SSSS        SSSS     SSSS     SSSS\nSSSSSSSSSSSSS   SSS   SSSSSSSSSSSSSSS   SSSS        SSSS     SSSS     SSSS\nSSSSSSSSSSSS    SSS    SSSSSSSSSSSSS    SSSS        SSSS     SSSS     SSSS\n\n\n\"\"\"\n\n\ndef add_slurm_user():\n\n    util.run(\"useradd -m -c SlurmUser -d /var/lib/slurm -U -r slurm\")\n# END add_slurm_user()\n\n\ndef setup_modules():\n\n    appsmfs = Path('/apps/modulefiles')\n\n    with open('/usr/share/Modules/init/.modulespath', 'r+') as dotmp:\n        if str(appsmfs) not in dotmp.read():\n            if cfg.instance_type == 'controller' and not appsmfs.is_dir():\n                appsmfs.mkdir(parents=True)\n            # after read, file cursor is at end of file\n            dotmp.write(f'\\n{appsmfs}\\n')\n# END setup_modules\n\n\ndef start_motd():\n\n    msg = MOTD_HEADER + \"\"\"\n*** Slurm is currently being installed/configured in the background. ***\nA terminal broadcast will announce when installation and configuration is\ncomplete.\n\nPartitions will be marked down until the compute image has been created.\nFor instances with gpus attached, it could take ~10 mins after the controller\nhas finished installing.\n\n\"\"\"\n\n    if cfg.instance_type != \"controller\":\n        msg += \"\"\"/home on the controller will be mounted over the existing /home.\nAny changes in /home will be hidden. Please wait until the installation is\ncomplete before making changes in your home directory.\n\n\"\"\"\n\n    with open('/etc/motd', 'w') as f:\n        f.write(msg)\n# END start_motd()\n\n\ndef end_motd(broadcast=True):\n\n    with open('/etc/motd', 'w') as f:\n        f.write(MOTD_HEADER)\n\n    if not broadcast:\n        return\n\n    util.run(\"wall -n '*** Slurm {} daemon installation complete ***'\"\n             .format(cfg.instance_type))\n\n    if cfg.instance_type != 'controller':\n        util.run(\"\"\"wall -n '\n/home on the controller was mounted over the existing /home.\nEither log out and log back in or cd into ~.\n'\"\"\")\n# END start_motd()\n\n\ndef have_gpus(hostname):\n\n    pid = util.get_pid(hostname)\n    return cfg.partitions[pid].gpu_count \u003e 0\n# END have_gpus()\n\n\ndef install_slurmlog_conf():\n    \"\"\" Install fluentd config for slurm logs \"\"\"\n\n    slurmlog_config = util.get_metadata('attributes/fluentd_conf_tpl')\n    if slurmlog_config:\n        Path('/etc/google-fluentd/config.d/slurmlogs.conf').write_text(\n            slurmlog_config)\n\n\ndef install_packages():\n\n    # install stackdriver monitoring and logging\n    add_mon_script = Path('/tmp/add-monitoring-agent-repo.sh')\n    add_mon_url = f'https://dl.google.com/cloudagents/{add_mon_script.name}'\n    urllib.request.urlretrieve(add_mon_url, add_mon_script)\n    util.run(f\"bash {add_mon_script}\")\n    util.run(\"yum install -y stackdriver-agent\")\n\n    add_log_script = Path('/tmp/install-logging-agent.sh')\n    add_log_url = f'https://dl.google.com/cloudagents/{add_log_script.name}'\n    urllib.request.urlretrieve(add_log_url, add_log_script)\n    util.run(f\"bash {add_log_script}\")\n    install_slurmlog_conf()\n\n    util.run(\"systemctl enable stackdriver-agent google-fluentd\")\n    util.run(\"systemctl start stackdriver-agent google-fluentd\")\n\n    # install cuda if needed\n    if cfg.instance_type == 'compute' and have_gpus(socket.gethostname()):\n        util.run(\"yum -y install kernel-devel-$(uname -r) kernel-headers-$(uname -r)\",\n                 shell=True)\n        repo = 'http://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/cuda-rhel7.repo'\n        util.run(f\"yum-config-manager --add-repo {repo}\")\n        util.run(\"yum clean all\")\n        util.run(\"yum -y install nvidia-driver-latest-dkms cuda\")\n        util.run(\"yum -y install cuda-drivers\")\n        # Creates the device files\n        util.run(\"nvidia-smi\")\n# END install_packages()\n\n\ndef setup_munge():\n\n    munge_service_patch = Path('/usr/lib/systemd/system/munge.service')\n    req_mount = (f\"\\nRequiresMountsFor={MUNGE_DIR}\"\n                 if cfg.instance_type != 'controller' else '')\n    with munge_service_patch.open('w') as f:\n        f.write(f\"\"\"\n[Unit]\nDescription=MUNGE authentication service\nDocumentation=man:munged(8)\nAfter=network.target\nAfter=syslog.target\nAfter=time-sync.target{req_mount}\n\n[Service]\nType=forking\nExecStart=/usr/sbin/munged --num-threads=10\nPIDFile=/var/run/munge/munged.pid\nUser=munge\nGroup=munge\nRestart=on-abort\n\n[Install]\nWantedBy=multi-user.target\n\"\"\")\n\n    util.run(\"systemctl enable munge\")\n\n    if cfg.instance_type != 'controller':\n        return\n\n    if cfg.munge_key:\n        with (MUNGE_DIR/'munge.key').open('w') as f:\n            f.write(cfg.munge_key)\n\n        util.run(f\"chown -R munge: {MUNGE_DIR} /var/log/munge/\")\n\n        (MUNGE_DIR/'munge_key').chmod(0o400)\n        MUNGE_DIR.chmod(0o700)\n        Path('var/log/munge/').chmod(0o700)\n    else:\n        util.run('create-munge-key')\n# END setup_munge ()\n\n\ndef start_munge():\n    util.run(\"systemctl start munge\")\n# END start_munge()\n\n\ndef setup_nfs_exports():\n\n    export_paths = (\n        (HOME_DIR, not EXTERNAL_MOUNT_HOME),\n        (APPS_DIR, not EXTERNAL_MOUNT_APPS),\n        (MUNGE_DIR, not EXTERNAL_MOUNT_MUNGE),\n        (SEC_DISK_DIR, cfg.controller_secondary_disk),\n    )\n\n    # export path if corresponding selector boolean is True\n    for path in it.compress(*zip(*export_paths)):\n        util.run(rf\"sed -i '\\#{path}#d' /etc/exports\")\n        with open('/etc/exports', 'a') as f:\n            f.write(f\"\\n{path}  *(rw,no_subtree_check,no_root_squash)\")\n\n    util.run(\"exportfs -a\")\n# END setup_nfs_exports()\n\n\ndef expand_machine_type():\n\n    machines = []\n    compute = googleapiclient.discovery.build('compute', 'v1',\n                                              cache_discovery=False)\n    for part in cfg.partitions:\n        machine = {'cpus': 1, 'memory': 1}\n        try:\n            type_resp = compute.machineTypes().get(\n                project=cfg.project, zone=part.zone,\n                machineType=part.machine_type).execute()\n            if type_resp:\n                machine['cpus'] = type_resp['guestCpus']\n\n                # Because the actual memory on the host will be different than\n                # what is configured (e.g. kernel will take it). From\n                # experiments, about 16 MB per GB are used (plus about 400 MB\n                # buffer for the first couple of GB's. Using 30 MB to be safe.\n                gb = type_resp['memoryMb'] // 1024\n                machine['memory'] = type_resp['memoryMb'] - (400 + (gb * 30))\n\n        except Exception:\n            log.exception(\"Failed to get MachineType '{}' from google api\"\n                          .format(part.machine_type))\n        finally:\n            machines.append(machine)\n\n    return machines\n# END expand_machine_type()\n\n\ndef install_slurm_conf():\n\n    machines = expand_machine_type()\n\n    if cfg.ompi_version:\n        mpi_default = \"pmi2\"\n    else:\n        mpi_default = \"none\"\n\n    conf_resp = util.get_metadata('attributes/slurm_conf_tpl')\n    conf = conf_resp.format(**globals(), **locals())\n\n    static_nodes = []\n    for i, machine in enumerate(machines):\n        part = cfg.partitions[i]\n        static_range = ''\n        if part.static_node_count:\n            if part.static_node_count \u003e 1:\n                static_range = '{}-{}-[0-{}]'.format(\n                    cfg.compute_node_prefix, i, part.static_node_count - 1)\n            else:\n                static_range = f\"{cfg.compute_node_prefix}-{i}-0\"\n\n        cloud_range = \"\"\n        if (part.max_node_count and\n                (part.max_node_count != part.static_node_count)):\n            cloud_range = \"{}-{}-[{}-{}]\".format(\n                cfg.compute_node_prefix, i, part.static_node_count,\n                part.max_node_count - 1)\n\n        conf += (\"NodeName=DEFAULT \"\n                 f\"CPUs={machine['cpus']} \"\n                 f\"RealMemory={machine['memory']} \"\n                 \"State=UNKNOWN\")\n        conf += '\\n'\n\n        # Nodes\n        gres = \"\"\n        if part.gpu_count:\n            gres = \" Gres=gpu:\" + str(part.gpu_count)\n        if static_range:\n            static_nodes.append(static_range)\n            conf += f\"NodeName={static_range}{gres}\\n\"\n\n        if cloud_range:\n            conf += f\"NodeName={cloud_range} State=CLOUD{gres}\\n\"\n\n        # Partitions\n        part_nodes = f'-{i}-[0-{part.max_node_count - 1}]'\n\n        def_mem_per_cpu = max(100, machine['memory'] // machine['cpus'])\n\n        conf += (\"PartitionName={} Nodes={}-compute{} MaxTime=INFINITE \"\n                 \"State=UP DefMemPerCPU={} LLN=yes\"\n                 .format(part.name, cfg.cluster_name, part_nodes,\n                         def_mem_per_cpu))\n\n        # First partition specified is treated as the default partition\n        if i == 0:\n            conf += \" Default=YES\"\n        conf += \"\\n\\n\"\n\n    if len(static_nodes):\n        conf += \"\\nSuspendExcNodes={}\\n\".format(','.join(static_nodes))\n\n    etc_dir = CURR_SLURM_DIR/'etc'\n    if not etc_dir.exists():\n        etc_dir.mkdir(parents=True)\n    with (etc_dir/'slurm.conf').open('w') as f:\n        f.write(conf)\n# END install_slurm_conf()\n\n\ndef install_slurmdbd_conf():\n    if cfg.cloudsql:\n        db_name = cfg.cloudsql['db_name']\n        db_user = cfg.cloudsql['user']\n        db_pass = cfg.cloudsql['password']\n        db_host_str = cfg.cloudsql['server_ip'].split(':')\n        db_host = db_host_str[0]\n        db_port = db_host_str[1] if len(db_host_str) \u003e= 2 else '3306'\n    else:\n        db_name = \"slurm_acct_db\"\n        db_user = 'slurm'\n        db_pass = '\"\"'\n        db_host = 'localhost'\n        db_port = '3306'\n\n    conf_resp = util.get_metadata('attributes/slurmdbd_conf_tpl')\n    conf = conf_resp.format(**globals(), **locals())\n\n    etc_dir = CURR_SLURM_DIR/'etc'\n    if not etc_dir.exists():\n        etc_dir.mkdir(parents=True)\n    (etc_dir/'slurmdbd.conf').write_text(conf)\n    (etc_dir/'slurmdbd.conf').chmod(0o600)\n\n# END install_slurmdbd_conf()\n\n\ndef install_cgroup_conf():\n\n    conf = util.get_metadata('attributes/cgroup_conf_tpl')\n\n    etc_dir = CURR_SLURM_DIR/'etc'\n    with (etc_dir/'cgroup.conf').open('w') as f:\n        f.write(conf)\n\n    gpu_parts = [(i, x) for i, x in enumerate(cfg.partitions)\n                 if x.gpu_count]\n    gpu_conf = \"\"\n    for i, part in gpu_parts:\n        driver_range = '0'\n        if part.gpu_count \u003e 1:\n            driver_range = '[0-{}]'.format(part.gpu_count-1)\n\n        gpu_conf += (\"NodeName={}-{}-[0-{}] Name=gpu File=/dev/nvidia{}\\n\"\n                     .format(cfg.compute_node_prefix, i,\n                             part.max_node_count - 1, driver_range))\n    if gpu_conf:\n        with (etc_dir/'gres.conf').open('w') as f:\n            f.write(gpu_conf)\n\n# END install_cgroup_conf()\n\n\ndef install_meta_files():\n\n    scripts_path = APPS_DIR/'slurm/scripts'\n    if not scripts_path.exists():\n        scripts_path.mkdir(parents=True)\n\n    cfg.slurm_cmd_path = str(CURR_SLURM_DIR/'bin')\n    cfg.log_dir = str(SLURM_LOG)\n\n    cfg.save_config(scripts_path/'config.yaml')\n\n    meta_files = [\n        ('suspend.py', 'slurm_suspend'),\n        ('resume.py', 'slurm_resume'),\n        ('slurmsync.py', 'slurmsync'),\n        ('util.py', 'util_script'),\n        ('compute-shutdown', 'compute-shutdown'),\n        ('custom-compute-install', 'custom-compute-install'),\n        ('custom-controller-install', 'custom-controller-install'),\n    ]\n\n    for file_name, meta_name in meta_files:\n        text = util.get_metadata('attributes/' + meta_name)\n        if not text:\n            continue\n\n        with (scripts_path/file_name).open('w') as f:\n            f.write(text)\n        (scripts_path/file_name).chmod(0o755)\n\n    util.run(\n        \"gcloud compute instances remove-metadata {} --zone={} --keys={}\"\n        .format(CONTROL_MACHINE, cfg.zone,\n                ','.join([x[1] for x in meta_files])))\n\n# END install_meta_files()\n\n\ndef install_slurm():\n\n    src_path = APPS_DIR/'slurm/src'\n    if not src_path.exists():\n        src_path.mkdir(parents=True)\n\n    with cd(src_path):\n        use_version = ''\n        if (cfg.slurm_version[0:2] == 'b:'):\n            GIT_URL = 'https://github.com/SchedMD/slurm.git'\n            use_version = cfg.slurm_version[2:]\n            util.run(\n                \"git clone -b {0} {1} {0}\".format(use_version, GIT_URL))\n        else:\n            file = 'slurm-{}.tar.bz2'.format(cfg.slurm_version)\n            slurm_url = 'https://download.schedmd.com/slurm/' + file\n            urllib.request.urlretrieve(slurm_url, src_path/file)\n\n            use_version = util.run(f\"tar -xvjf {file}\", check=True,\n                                   get_stdout=True).stdout.splitlines()[0][:-1]\n\n    SLURM_PREFIX = APPS_DIR/'slurm'/use_version\n\n    build_dir = src_path/use_version/'build'\n    if not build_dir.exists():\n        build_dir.mkdir(parents=True)\n\n    with cd(build_dir):\n        util.run(\"../configure --prefix={} --sysconfdir={}/etc\"\n                 .format(SLURM_PREFIX, CURR_SLURM_DIR), stdout=DEVNULL)\n        util.run(\"make -j install\", stdout=DEVNULL)\n    with cd(build_dir/'contribs'):\n        util.run(\"make -j install\", stdout=DEVNULL)\n\n    os.symlink(SLURM_PREFIX, CURR_SLURM_DIR)\n\n    state_dir = APPS_DIR/'slurm/state'\n    if not state_dir.exists():\n        state_dir.mkdir(parents=True)\n        util.run(f\"chown -R slurm: {state_dir}\")\n\n    install_slurm_conf()\n    install_slurmdbd_conf()\n    install_cgroup_conf()\n    install_meta_files()\n\n# END install_slurm()\n\n\ndef install_slurm_tmpfile():\n\n    run_dir = Path('/var/run/slurm')\n\n    with open('/etc/tmpfiles.d/slurm.conf', 'w') as f:\n        f.write(f\"\\nd {run_dir} 0755 slurm slurm -\")\n\n    if not run_dir.exists():\n        run_dir.mkdir(parents=True)\n    run_dir.chmod(0o755)\n\n    util.run(f\"chown slurm: {run_dir}\")\n\n# END install_slurm_tmpfile()\n\n\ndef install_controller_service_scripts():\n\n    install_slurm_tmpfile()\n\n    # slurmctld.service\n    ctld_service = Path('/usr/lib/systemd/system/slurmctld.service')\n    with ctld_service.open('w') as f:\n        f.write(\"\"\"\n[Unit]\nDescription=Slurm controller daemon\nAfter=network.target munge.service\nConditionPathExists={prefix}/etc/slurm.conf\n\n[Service]\nType=forking\nEnvironmentFile=-/etc/sysconfig/slurmctld\nExecStart={prefix}/sbin/slurmctld $SLURMCTLD_OPTIONS\nExecReload=/bin/kill -HUP $MAINPID\nPIDFile=/var/run/slurm/slurmctld.pid\n\n[Install]\nWantedBy=multi-user.target\n\"\"\".format(prefix=CURR_SLURM_DIR))\n\n    ctld_service.chmod(0o644)\n\n    # slurmdbd.service\n    dbd_service = Path('/usr/lib/systemd/system/slurmdbd.service')\n    with dbd_service.open('w') as f:\n        f.write(\"\"\"\n[Unit]\nDescription=Slurm DBD accounting daemon\nAfter=network.target munge.service\nConditionPathExists={prefix}/etc/slurmdbd.conf\n\n[Service]\nType=forking\nEnvironmentFile=-/etc/sysconfig/slurmdbd\nExecStart={prefix}/sbin/slurmdbd $SLURMDBD_OPTIONS\nExecReload=/bin/kill -HUP $MAINPID\nPIDFile=/var/run/slurm/slurmdbd.pid\n\n[Install]\nWantedBy=multi-user.target\n\"\"\".format(prefix=CURR_SLURM_DIR))\n\n    dbd_service.chmod(0o644)\n\n# END install_controller_service_scripts()\n\n\ndef install_compute_service_scripts():\n\n    install_slurm_tmpfile()\n\n    # slurmd.service\n    slurmd_service = Path('/usr/lib/systemd/system/slurmd.service')\n    with slurmd_service.open('w') as f:\n        f.write(\"\"\"\n[Unit]\nDescription=Slurm node daemon\nAfter=network.target munge.service home.mount apps.mount etc-munge.mount\nConditionPathExists={prefix}/etc/slurm.conf\n\n[Service]\nType=forking\nEnvironmentFile=-/etc/sysconfig/slurmd\nExecStart={prefix}/sbin/slurmd $SLURMD_OPTIONS\nExecReload=/bin/kill -HUP $MAINPID\nPIDFile=/var/run/slurm/slurmd.pid\nKillMode=process\nLimitNOFILE=51200\nLimitMEMLOCK=infinity\nLimitSTACK=infinity\n\n[Install]\nWantedBy=multi-user.target\n\"\"\".format(prefix=CURR_SLURM_DIR))\n\n    slurmd_service.chmod(0o644)\n    util.run('systemctl enable slurmd')\n\n# END install_compute_service_scripts()\n\n\ndef setup_bash_profile():\n\n    with open('/etc/profile.d/slurm.sh', 'w') as f:\n        f.write(\"\"\"\nS_PATH={}\nPATH=$PATH:$S_PATH/bin:$S_PATH/sbin\n\"\"\".format(CURR_SLURM_DIR))\n\n    if cfg.instance_type == 'compute' and have_gpus(socket.gethostname()):\n        with open('/etc/profile.d/cuda.sh', 'w') as f:\n            f.write(\"\"\"\nCUDA_PATH=/usr/local/cuda\nPATH=$CUDA_PATH/bin${PATH:+:${PATH}}\nLD_LIBRARY_PATH=$CUDA_PATH/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\n\"\"\")\n\n# END setup_bash_profile()\n\n\ndef setup_ompi_bash_profile():\n    if not cfg.ompi_version:\n        return\n    with open(f'/etc/profile.d/ompi-{cfg.ompi_version}.sh', 'w') as f:\n        f.write(f\"PATH={APPS_DIR}/ompi/{cfg.ompi_version}/bin:$PATH\")\n# END setup_ompi_bash_profile()\n\n\ndef setup_logrotate():\n    with open('/etc/logrotate.d/slurm', 'w') as f:\n        f.write(\"\"\"\n##\n# Slurm Logrotate Configuration\n##\n/var/log/slurm/*.log {\n        compress\n        missingok\n        nocopytruncate\n        nodelaycompress\n        nomail\n        notifempty\n        noolddir\n        rotate 5\n        sharedscripts\n        size=5M\n        create 640 slurm root\n        postrotate\n                pkill -x --signal SIGUSR2 slurmctld\n                pkill -x --signal SIGUSR2 slurmd\n                pkill -x --signal SIGUSR2 slurmdbd\n                exit 0\n        endscript\n}\n\"\"\")\n# END setup_logrotate()\n\n\ndef setup_network_storage():\n    log.info(\"Set up network storage\")\n\n    global EXTERNAL_MOUNT_APPS\n    global EXTERNAL_MOUNT_HOME\n    global EXTERNAL_MOUNT_MUNGE\n\n    EXTERNAL_MOUNT_APPS = False\n    EXTERNAL_MOUNT_HOME = False\n    EXTERNAL_MOUNT_MUNGE = False\n    cifs_installed = False\n\n    # create dict of mounts, local_mount: mount_info\n    if cfg.instance_type == 'controller':\n        ext_mounts = {}\n    else:  # on non-controller instances, low priority mount these\n        CONTROL_NFS = {\n            'server_ip': CONTROL_MACHINE,\n            'remote_mount': 'none',\n            'local_mount': 'none',\n            'fs_type': 'nfs',\n            'mount_options': 'defaults,hard,intr',\n        }\n        ext_mounts = {\n            HOME_DIR: dict(CONTROL_NFS, remote_mount=HOME_DIR,\n                           local_mount=HOME_DIR),\n            APPS_DIR: dict(CONTROL_NFS, remote_mount=APPS_DIR,\n                           local_mount=APPS_DIR),\n            MUNGE_DIR: dict(CONTROL_NFS, remote_mount=MUNGE_DIR,\n                            local_mount=MUNGE_DIR),\n        }\n\n    # convert network_storage list of mounts to dict of mounts,\n    #   local_mount as key\n    def listtodict(mountlist):\n        return {Path(d['local_mount']).resolve(): d for d in mountlist}\n\n    ext_mounts.update(listtodict(cfg.network_storage))\n    if cfg.instance_type == 'compute':\n        pid = util.get_pid(socket.gethostname())\n        ext_mounts.update(listtodict(cfg.partitions[pid].network_storage))\n    else:\n        ext_mounts.update(listtodict(cfg.login_network_storage))\n\n    # Install lustre, cifs, and/or gcsfuse as needed and write mount to fstab\n    fstab_entries = []\n    for local_mount, mount in ext_mounts.items():\n        remote_mount = mount['remote_mount']\n        fs_type = mount['fs_type']\n        server_ip = mount['server_ip']\n        log.info(\"Setting up mount ({}) {}{} to {}\".format(\n            fs_type, server_ip+':' if fs_type != 'gcsfuse' else \"\",\n            remote_mount, local_mount))\n        if not local_mount.exists():\n            local_mount.mkdir(parents=True)\n        # Check if we're going to overlap with what's normally hosted on the\n        # controller (/apps, /home, /etc/munge).\n        # If so delete the entries pointing to the controller, and tell the\n        # nodes.\n        if local_mount == APPS_DIR:\n            EXTERNAL_MOUNT_APPS = True\n        elif local_mount == HOME_DIR:\n            EXTERNAL_MOUNT_HOME = True\n        elif local_mount == MUNGE_DIR:\n            EXTERNAL_MOUNT_MUNGE = True\n\n        lustre_path = Path('/sys/module/lustre')\n        gcsf_path = Path('/etc/yum.repos.d/gcsfuse.repo')\n\n        if fs_type == 'cifs' and not cifs_installed:\n            util.run(\"sudo yum install -y cifs-utils\")\n            cifs_installed = True\n        elif fs_type == 'lustre' and not lustre_path.exists():\n            lustre_url = 'https://downloads.whamcloud.com/public/lustre/latest-release/el7.7.1908/client/RPMS/x86_64/'\n            lustre_tmp = Path('/tmp/lustre')\n            lustre_tmp.mkdir(parents=True)\n            util.run('sudo yum update -y')\n            util.run('sudo yum install -y wget libyaml')\n            for rpm in ('kmod-lustre-client-2*.rpm', 'lustre-client-2*.rpm'):\n                util.run(\n                    f\"wget -r -l1 --no-parent -A '{rpm}' '{lustre_url}' -P {lustre_tmp}\")\n            util.run(\n                f\"find {lustre_tmp} -name '*.rpm' -execdir rpm -ivh {{}} ';'\")\n            util.run(f\"rm -rf {lustre_tmp}\")\n            util.run(\"modprobe lustre\")\n        elif fs_type == 'gcsfuse' and not gcsf_path.exists():\n            with gcsf_path.open('a') as f:\n                f.write(\"\"\"\n[gcsfuse]\nname=gcsfuse (packages.cloud.google.com)\nbaseurl=https://packages.cloud.google.com/yum/repos/gcsfuse-el7-x86_64\nenabled=1\ngpgcheck=1\nrepo_gpgcheck=1\ngpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg\nhttps://packages.cloud.google.com/yum/doc/rpm-package-key.gpg\"\"\")\n            util.run(\"sudo yum update -y\")\n            util.run(\"sudo yum install -y gcsfuse\")\n\n        mount_options = mount['mount_options']\n        if fs_type == 'gcsfuse':\n            if 'nonempty' not in mount['mount_options']:\n                mount_options += \",nonempty\"\n            fstab_entries.append(\n                \"\\n{0}   {1}     {2}     {3}     0 0\"\n                .format(remote_mount, local_mount, fs_type, mount_options))\n        else:\n            remote_mount = Path(remote_mount).resolve()\n            fstab_entries.append(\n                \"\\n{0}:{1}    {2}     {3}      {4}  0 0\"\n                .format(server_ip, remote_mount, local_mount,\n                        fs_type, mount_options))\n\n    with open('/etc/fstab', 'a') as f:\n        for entry in fstab_entries:\n            f.write(entry)\n# END setup_network_storage()\n\n\ndef setup_secondary_disks():\n\n    if not SEC_DISK_DIR.exists():\n        SEC_DISK_DIR.mkdir(parents=True)\n    util.run(\n        \"sudo mkfs.ext4 -m 0 -F -E lazy_itable_init=0,lazy_journal_init=0,discard /dev/sdb\")\n    with open('/etc/fstab', 'a') as f:\n        f.write(\n            \"\\n/dev/sdb     {0}     ext4    discard,defaults,nofail     0 2\"\n            .format(SEC_DISK_DIR))\n\n# END setup_secondary_disks()\n\n\ndef mount_nfs_vols():\n\n    mount_paths = (\n        (HOME_DIR, EXTERNAL_MOUNT_HOME),\n        (APPS_DIR, EXTERNAL_MOUNT_APPS),\n        (MUNGE_DIR, EXTERNAL_MOUNT_MUNGE),\n    )\n    # compress yields values from the first arg that are matched with True\n    # in the second arg. The result is the paths filtered by the booleans.\n    # For non-controller instances, all three are always external nfs\n    for path in it.compress(*zip(*mount_paths)):\n        while not os.path.ismount(path):\n            log.info(f\"Waiting for {path} to be mounted\")\n            util.run(f\"mount {path}\", wait=5)\n    util.run(\"mount -a\", wait=1)\n\n# END mount_nfs_vols()\n\n\n# Tune the NFS server to support many mounts\ndef setup_nfs_threads():\n\n    with open('/etc/sysconfig/nfs', 'a') as f:\n        f.write(\"\"\"\n# Added by Google\nRPCNFSDCOUNT=256\n\"\"\")\n\n# END setup_nfs_threads()\n\n\ndef setup_sync_cronjob():\n\n    util.run(\"crontab -u slurm -\", input=(\n        f\"*/1 * * * * {APPS_DIR}/slurm/scripts/slurmsync.py\\n\"))\n\n# END setup_sync_cronjob()\n\n\ndef setup_slurmd_cronjob():\n    util.run(\n        \"crontab -u root -\", input=(\n            \"*/2 * * * * \"\n            \"if [ `systemctl status slurmd | grep -c inactive` -gt 0 ]; then \"\n            \"mount -a; \"\n            \"systemctl restart munge; \"\n            \"systemctl restart slurmd; \"\n            \"fi\\n\"\n        ))\n# END setup_slurmd_cronjob()\n\n\ndef create_compute_images():\n\n    def create_compute_image(instance, partition):\n        try:\n            compute = googleapiclient.discovery.build('compute', 'v1',\n                                                      cache_discovery=False)\n\n            while True:\n                resp = compute.instances().get(\n                    project=cfg.project, zone=cfg.zone, fields=\"status\",\n                    instance=instance).execute()\n                if resp['status'] == 'TERMINATED':\n                    break\n                log.info(f\"waiting for {instance} to be stopped (status: {resp['status']})\"\n                         .format(instance=instance, status=resp['status']))\n                time.sleep(30)\n\n            log.info(\"Creating image of {}...\".format(instance))\n            ver = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n            util.run(f\"gcloud compute images create \"\n                     f\"{instance}-{ver} --source-disk {instance} \"\n                     f\"--source-disk-zone {cfg.zone} --force \"\n                     f\"--family {instance}-family\")\n\n            util.run(\"{}/bin/scontrol update partitionname={} state=up\"\n                     .format(CURR_SLURM_DIR, partition.name))\n        except Exception as e:\n            log.exception(f\"{instance} not found: {e}\")\n\n    threads = []\n    for i, part in enumerate(cfg.partitions):\n        instance = f\"{cfg.compute_node_prefix}-{i}-image\"\n        thread = threading.Thread(target=create_compute_image,\n                                  args=(instance, part))\n        threads.append(thread)\n        thread.start()\n\n    for thread in threads:\n        thread.join()\n\n# END create_compute_image()\n\n\ndef setup_selinux():\n\n    util.run('setenforce 0')\n    with open('/etc/selinux/config', 'w') as f:\n        f.write(\"\"\"\nSELINUX=permissive\nSELINUXTYPE=targeted\n\"\"\")\n# END setup_selinux()\n\n\ndef install_ompi():\n\n    if not cfg.ompi_version:\n        return\n\n    packages = ['autoconf',\n                'flex',\n                'gcc-c++',\n                'libevent-devel',\n                'libtool']\n    util.run(\"yum install -y {}\".format(' '.join(packages)))\n\n    ompi_git = \"https://github.com/open-mpi/ompi.git\"\n    ompi_path = APPS_DIR/'ompi'/cfg.ompi_version/'src'\n    if not ompi_path.exists():\n        ompi_path.mkdir(parents=True)\n    util.run(f\"git clone -b {cfg.ompi_version} {ompi_git} {ompi_path}\")\n    with cd(ompi_path):\n        util.run(\"./autogen.pl\", stdout=DEVNULL)\n\n    build_path = ompi_path/'build'\n    if not build_path.exists():\n        build_path.mkdir(parents=True)\n    with cd(build_path):\n        util.run(\n            f\"../configure --prefix={APPS_DIR}/ompi/{cfg.ompi_version} \"\n            f\"--with-pmi={APPS_DIR}/slurm/current --with-libevent=/usr \"\n            \"--with-hwloc=/usr\", stdout=DEVNULL)\n        util.run(\"make -j install\", stdout=DEVNULL)\n# END install_ompi()\n\n\ndef remove_startup_scripts(hostname):\n\n    cmd = \"gcloud compute instances remove-metadata\"\n    common_keys = \"startup-script,setup_script,util_script,config\"\n    controller_keys = (f\"{common_keys},\"\n                       \"slurm_conf_tpl,slurmdbd_conf_tpl,cgroup_conf_tpl\")\n    compute_keys = f\"{common_keys},slurm_fluentd_log_tpl\"\n\n    # controller\n    util.run(f\"{cmd} {hostname} --zone={cfg.zone} --keys={controller_keys}\")\n\n    # logins\n    for i in range(0, cfg.login_node_count):\n        util.run(\"{} {}-login{} --zone={} --keys={}\"\n                 .format(cmd, cfg.cluster_name, i, cfg.zone, common_keys))\n    # computes\n    for i, part in enumerate(cfg.partitions):\n        # partition compute image\n        util.run(f\"{cmd} {cfg.compute_node_prefix}-{i}-image \"\n                 f\"--zone={cfg.zone} --keys={compute_keys}\")\n        if not part.static_node_count:\n            continue\n        for j in range(part.static_node_count):\n            util.run(\"{} {}-{}-{} --zone={} --keys={}\"\n                     .format(cmd, cfg.compute_node_prefix, i, j,\n                             part.zone, compute_keys))\n# END remove_startup_scripts()\n\n\ndef setup_nss_slurm():\n\n    # setup nss_slurm\n    util.run(\"ln -s {}/lib/libnss_slurm.so.2 /usr/lib64/libnss_slurm.so.2\"\n             .format(CURR_SLURM_DIR))\n    util.run(\n        r\"sed -i 's/\\(^\\(passwd\\|group\\):\\s\\+\\)/\\1slurm /g' /etc/nsswitch.conf\"\n    )\n# END setup_nss_slurm()\n\n\ndef main():\n    hostname = socket.gethostname()\n\n    setup_selinux()\n\n    start_motd()\n\n    add_slurm_user()\n    install_packages()\n    setup_munge()\n    setup_bash_profile()\n    setup_ompi_bash_profile()\n    setup_modules()\n\n    if cfg.controller_secondary_disk and cfg.instance_type == 'controller':\n        setup_secondary_disks()\n    setup_network_storage()\n\n    if not SLURM_LOG.exists():\n        SLURM_LOG.mkdir(parents=True)\n    shutil.chown(SLURM_LOG, user='slurm', group='slurm')\n\n    if cfg.instance_type == 'controller':\n        mount_nfs_vols()\n        time.sleep(5)\n        start_munge()\n        install_slurm()\n        install_ompi()\n\n        try:\n            util.run(str(APPS_DIR/'slurm/scripts/custom-controller-install'))\n        except Exception:\n            # Ignore blank files with no shell magic.\n            pass\n\n        install_controller_service_scripts()\n\n        if not cfg.cloudsql:\n            util.run('systemctl enable mariadb')\n            util.run('systemctl start mariadb')\n\n            mysql = \"mysql -u root -e\"\n            util.run(\n                f\"\"\"{mysql} \"create user 'slurm'@'localhost'\";\"\"\")\n            util.run(\n                f\"\"\"{mysql} \"grant all on slurm_acct_db.* TO 'slurm'@'localhost'\";\"\"\")\n            util.run(\n                f\"\"\"{mysql} \"grant all on slurm_acct_db.* TO 'slurm'@'{CONTROL_MACHINE}'\";\"\"\")\n\n        util.run(\"systemctl enable slurmdbd\")\n        util.run(\"systemctl start slurmdbd\")\n\n        # Wait for slurmdbd to come up\n        time.sleep(5)\n\n        sacctmgr = f\"{CURR_SLURM_DIR}/bin/sacctmgr -i\"\n        util.run(f\"{sacctmgr} add cluster {cfg.cluster_name}\")\n\n        util.run(\"systemctl enable slurmctld\")\n        util.run(\"systemctl start slurmctld\")\n        setup_nfs_threads()\n        # Export at the end to signal that everything is up\n        util.run(\"systemctl enable nfs-server\")\n        util.run(\"systemctl start nfs-server\")\n        setup_nfs_exports()\n\n        setup_sync_cronjob()\n\n        # DOWN partitions until image is created.\n        for part in cfg.partitions:\n            util.run(\"{}/bin/scontrol update partitionname={} state=down\"\n                     .format(CURR_SLURM_DIR, part.name))\n\n        create_compute_images()\n        remove_startup_scripts(hostname)\n        log.info(\"Done installing controller\")\n    elif cfg.instance_type == 'compute':\n        install_compute_service_scripts()\n        mount_nfs_vols()\n        start_munge()\n        setup_nss_slurm()\n        setup_slurmd_cronjob()\n\n        try:\n            util.run(str(APPS_DIR/'slurm/scripts/custom-compute-install'))\n        except Exception:\n            # Ignore blank files with no shell magic.\n            pass\n\n        if hostname.endswith('-image'):\n            end_motd(False)\n            util.run(\"sync\")\n            util.run(f\"gcloud compute instances stop {hostname} \"\n                     f\"--zone {cfg.zone} --quiet\")\n        else:\n            util.run(\"systemctl start slurmd\")\n\n    else:  # login nodes\n        mount_nfs_vols()\n        start_munge()\n\n        try:\n            util.run(str(APPS_DIR/\"slurm/scripts/custom-compute-install\"))\n        except Exception:\n            # Ignore blank files with no shell magic.\n            pass\n\n    setup_logrotate()\n\n    end_motd()\n\n# END main()\n\n\nif __name__ == '__main__':\n    main()\n\n",
              "slurm_conf_tpl": "# slurm.conf file generated by configurator.html.\n# Put this file on all nodes of your cluster.\n# See the slurm.conf man page for more information.\n#\nControlMachine={CONTROL_MACHINE}\n#ControlAddr=\n#BackupController=\n#BackupAddr=\n#\nAuthType=auth/munge\nAuthInfo=cred_expire=120\n#CheckpointType=checkpoint/none\nCryptoType=crypto/munge\n#DisableRootJobs=NO\n#EnforcePartLimits=NO\n#Epilog=\n#EpilogSlurmctld=\n#FirstJobId=1\n#MaxJobId=999999\n#GroupUpdateForce=0\n#GroupUpdateTime=600\n#JobCheckpointDir=/var/slurm/checkpoint\n#JobCredentialPrivateKey=\n#JobCredentialPublicCertificate=\n#JobFileAppend=0\n#JobRequeue=1\n#JobSubmitPlugins=1\n#KillOnBadExit=0\n#LaunchType=launch/slurm\n#Licenses=foo*4,bar\n#MailProg=/bin/mail\n#MaxJobCount=5000\n#MaxStepCount=40000\n#MaxTasksPerNode=128\nMpiDefault={mpi_default}\n#MpiParams=ports=#-#\n#PluginDir=\n#PlugStackConfig=\n#PrivateData=jobs\nLaunchParameters=send_gids,enable_nss_slurm\n\n# Always show cloud nodes. Otherwise cloud nodes are hidden until they are\n# resumed. Having them shown can be useful in detecting downed nodes.\n# NOTE: slurm won't allocate/resume nodes that are down. So in the case of\n# preemptible nodes -- if gcp preempts a node, the node will eventually be put\n# into a down date because the node will stop responding to the controller.\n# (e.g. SlurmdTimeout).\nPrivateData=cloud\n\nProctrackType=proctrack/cgroup\n\n#Prolog=\n#PrologFlags=\n#PrologSlurmctld=\n#PropagatePrioProcess=0\n#PropagateResourceLimits=\n#PropagateResourceLimitsExcept=Sched\n#RebootProgram=\n\nReturnToService=2\n#SallocDefaultCommand=\nSlurmctldPidFile=/var/run/slurm/slurmctld.pid\nSlurmctldPort=6820-6830\nSlurmdPidFile=/var/run/slurm/slurmd.pid\nSlurmdPort=6818\nSlurmdSpoolDir=/var/spool/slurmd\nSlurmUser=slurm\n#SlurmdUser=root\n#SrunEpilog=\n#SrunProlog=\nStateSaveLocation={APPS_DIR}/slurm/state\nSwitchType=switch/none\n#TaskEpilog=\nTaskPlugin=task/affinity,task/cgroup\n#TaskPluginParam=\n#TaskProlog=\n#TopologyPlugin=topology/tree\n#TmpFS=/tmp\n#TrackWCKey=no\n#TreeWidth=\n#UnkillableStepProgram=\n#UsePAM=0\n#\n#\n# TIMERS\n#BatchStartTimeout=10\n#CompleteWait=0\n#EpilogMsgTime=2000\n#GetEnvTimeout=2\n#HealthCheckInterval=0\n#HealthCheckProgram=\nInactiveLimit=0\nKillWait=30\nMessageTimeout=60\n#ResvOverRun=0\nMinJobAge=300\n#OverTimeLimit=0\nSlurmctldTimeout=120\nSlurmdTimeout=300\n#UnkillableStepTimeout=60\n#VSizeFactor=0\nWaittime=0\n#\n#\n# SCHEDULING\nFastSchedule=1\n#MaxMemPerCPU=0\n#SchedulerTimeSlice=30\nSchedulerType=sched/backfill\nSelectType=select/cons_res\nSelectTypeParameters=CR_Core_Memory\n#\n#\n# JOB PRIORITY\n#PriorityFlags=\n#PriorityType=priority/basic\n#PriorityDecayHalfLife=\n#PriorityCalcPeriod=\n#PriorityFavorSmall=\n#PriorityMaxAge=\n#PriorityUsageResetPeriod=\n#PriorityWeightAge=\n#PriorityWeightFairshare=\n#PriorityWeightJobSize=\n#PriorityWeightPartition=\n#PriorityWeightQOS=\n#\n#\n# LOGGING AND ACCOUNTING\n#AccountingStorageEnforce=associations,limits,qos,safe\nAccountingStorageHost={CONTROL_MACHINE}\n#AccountingStorageLoc=\n#AccountingStoragePass=\n#AccountingStoragePort=\nAccountingStorageType=accounting_storage/slurmdbd\n#AccountingStorageUser=\nAccountingStoreJobComment=YES\nClusterName={cfg.cluster_name}\n#DebugFlags=powersave\n#JobCompHost=\n#JobCompLoc=\n#JobCompPass=\n#JobCompPort=\nJobCompType=jobcomp/none\n#JobCompUser=\n#JobContainerType=job_container/none\nJobAcctGatherFrequency=30\nJobAcctGatherType=jobacct_gather/linux\nSlurmctldDebug=info\nSlurmctldLogFile={SLURM_LOG}/slurmctld.log\nSlurmdDebug=info\nSlurmdLogFile={SLURM_LOG}/slurmd-%n.log\n#\n#\n# POWER SAVE SUPPORT FOR IDLE NODES (optional)\nSuspendProgram={APPS_DIR}/slurm/scripts/suspend.py\nResumeProgram={APPS_DIR}/slurm/scripts/resume.py\nResumeFailProgram={APPS_DIR}/slurm/scripts/suspend.py\nSuspendTimeout={SUSPEND_TIMEOUT}\nResumeTimeout={RESUME_TIMEOUT}\nResumeRate=0\n#SuspendExcNodes=\n#SuspendExcParts=\nSuspendRate=0\nSuspendTime={cfg.suspend_time}\n#\nSchedulerParameters=salloc_wait_nodes\nSlurmctldParameters=cloud_dns,idle_on_node_suspend\nCommunicationParameters=NoAddrCache\nGresTypes=gpu\n#\n# COMPUTE NODES\n\n",
              "slurm_resume": "#!/usr/bin/env python3\n\n# Copyright 2017 SchedMD LLC.\n# Modified for use with the Slurm Resource Manager.\n#\n# Copyright 2015 Google Inc. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport argparse\nimport httplib2\nimport logging\nimport os\nimport sys\nimport time\nfrom pathlib import Path\n\nimport googleapiclient.discovery\nfrom google.auth import compute_engine\nimport google_auth_httplib2\nfrom googleapiclient.http import set_user_agent\n\nimport util\n\n\ncfg = util.Config.load_config(Path(__file__).with_name('config.yaml'))\n\nSCONTROL = Path(cfg.slurm_cmd_path or '')/'scontrol'\nLOGFILE = (Path(cfg.log_dir or '')/Path(__file__).name).with_suffix('.log')\n\nTOT_REQ_CNT = 1000\n\ninstances = {}\noperations = {}\nretry_list = []\n\nutil.config_root_logger(level='DEBUG', util_level='ERROR', file=LOGFILE)\nlog = logging.getLogger(Path(__file__).name)\n\n\nif cfg.google_app_cred_path:\n    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = cfg.google_app_cred_path\n\ncredentials = compute_engine.Credentials()\n\nhttp = None\nauthorized_http = None\nif not cfg.google_app_cred_path:\n    http = set_user_agent(httplib2.Http(), \"Slurm_GCP_Scripts/1.1 (GPN:SchedMD)\")\n    authorized_http = google_auth_httplib2.AuthorizedHttp(credentials, http=http)\n\n\ndef wait_for_operation(compute, project, operation):\n    print('Waiting for operation to finish...')\n    while True:\n        result = None\n        if 'zone' in operation:\n            result = compute.zoneOperations().get(\n                project=project,\n                zone=operation['zone'].split('/')[-1],\n                operation=operation['name']).execute()\n        elif 'region' in operation:\n            result = compute.regionOperations().get(\n                project=project,\n                region=operation['region'].split('/')[-1],\n                operation=operation['name']).execute()\n        else:\n            result = compute.globalOperations().get(\n                project=project,\n                operation=operation['name']).execute()\n\n        if result['status'] == 'DONE':\n            print(\"done.\")\n            if 'error' in result:\n                raise Exception(result['error'])\n            return result\n\n        time.sleep(1)\n# [END wait_for_operation]\n\n\ndef update_slurm_node_addrs(compute):\n    for node_name, operation in operations.items():\n        try:\n            # Do this after the instances have been initialized and then wait\n            # for all operations to finish. Then updates their addrs.\n            wait_for_operation(compute, cfg.project, operation)\n\n            pid = util.get_pid(node_name)\n            my_fields = 'networkInterfaces(name,network,networkIP,subnetwork)'\n            instance_networks = compute.instances().get(\n                project=cfg.project, zone=cfg.partitions[pid].zone,\n                instance=node_name, fields=my_fields).execute()\n            instance_ip = instance_networks['networkInterfaces'][0]['networkIP']\n\n            util.run(\n                f\"{SCONTROL} update node={node_name} nodeaddr={instance_ip}\")\n\n            log.info(\"Instance \" + node_name + \" is now up\")\n        except Exception:\n            log.exception(f\"Error in adding {node_name} to slurm\")\n# [END update_slurm_node_addrs]\n\n\ndef create_instance(compute, zone, machine_type, instance_name,\n                    source_disk_image):\n\n    pid = util.get_pid(instance_name)\n    # Configure the machine\n    machine_type_path = f'zones/{zone}/machineTypes/{machine_type}'\n    disk_type = 'projects/{}/zones/{}/diskTypes/{}'.format(\n        cfg.project, zone, cfg.partitions[pid].compute_disk_type)\n\n    config = {\n        'name': instance_name,\n        'machineType': machine_type_path,\n\n        # Specify the boot disk and the image to use as a source.\n        'disks': [{\n            'boot': True,\n            'autoDelete': True,\n            'initializeParams': {\n                'sourceImage': source_disk_image,\n                'diskType': disk_type,\n                'diskSizeGb': cfg.partitions[pid].compute_disk_size_gb\n            }\n        }],\n\n        # Specify a network interface\n        'networkInterfaces': [{\n            'subnetwork': (\n                \"projects/{}/regions/{}/subnetworks/{}\".format(\n                    cfg.shared_vpc_host_project or cfg.project,\n                    cfg.partitions[pid].region,\n                    (cfg.partitions[pid].vpc_subnet\n                     or f'{cfg.cluster_name}-{cfg.partitions[pid].region}'))\n            ),\n        }],\n\n        # Allow the instance to access cloud storage and logging.\n        'serviceAccounts': [{\n            'email': cfg.compute_node_service_account,\n            'scopes': cfg.compute_node_scopes\n        }],\n\n        'tags': {'items': ['compute']},\n\n        'metadata': {\n            'items': [\n                {'key': 'enable-oslogin',\n                 'value': 'TRUE'},\n                {'key': 'VmDnsSetting',\n                 'value': 'GlobalOnly'}\n            ]\n        }\n    }\n\n    shutdown_script_path = Path('/apps/slurm/scripts/compute-shutdown')\n    if shutdown_script_path.exists():\n        config['metadata']['items'].append({\n            'key': 'shutdown-script',\n            'value': shutdown_script_path.read_text()\n        })\n\n    if cfg.partitions[pid].gpu_type:\n        accel_type = ('https://www.googleapis.com/compute/v1/projects/{}/zones/{}/acceleratorTypes/{}'\n                      .format(cfg.project, zone,\n                              cfg.partitions[pid].gpu_type))\n        config['guestAccelerators'] = [{\n            'acceleratorCount': cfg.partitions[pid].gpu_count,\n            'acceleratorType': accel_type\n        }]\n\n        config['scheduling'] = {'onHostMaintenance': 'TERMINATE'}\n\n    if cfg.partitions[pid].preemptible_bursting:\n        config['scheduling'] = {\n            'preemptible': True,\n            'onHostMaintenance': 'TERMINATE',\n            'automaticRestart': False\n        },\n\n    if cfg.partitions[pid].compute_labels:\n        config['labels'] = cfg.partitions[pid].compute_labels,\n\n    if cfg.partitions[pid].cpu_platform:\n        config['minCpuPlatform'] = cfg.partitions[pid].cpu_platform,\n\n    if cfg.external_compute_ips:\n        config['networkInterfaces'][0]['accessConfigs'] = [\n            {'type': 'ONE_TO_ONE_NAT', 'name': 'External NAT'}\n        ]\n\n    return compute.instances().insert(\n        project=cfg.project,\n        zone=zone,\n        body=config)\n# [END create_instance]\n\n\ndef added_instances_cb(request_id, response, exception):\n    if exception is not None:\n        log.error(f\"add exception for node {request_id}: {exception}\")\n        if \"Rate Limit Exceeded\" in str(exception):\n            retry_list.append(request_id)\n    else:\n        operations[request_id] = response\n# [END added_instances_cb]\n\n\n@util.static_vars(images={})\ndef get_source_image(compute, node_name):\n\n    images = get_source_image.images\n    pid = util.get_pid(node_name)\n    if pid not in images:\n        image_name = f\"{cfg.compute_node_prefix}-{pid}-image\"\n        family = (cfg.partitions[pid].compute_image_family\n                  or f\"{image_name}-family\")\n        try:\n            image_response = compute.images().getFromFamily(\n                project=cfg.project, family=family\n            ).execute()\n            if image_response['status'] != 'READY':\n                raise Exception(\"Image not ready\")\n            source_disk_image = image_response['selfLink']\n        except Exception as e:\n            log.error(f\"Image {family} unavailable: {e}\")\n            sys.exit()\n\n        images[pid] = source_disk_image\n    return images[pid]\n# [END get_source_image]\n\n\ndef add_instances(compute, node_list):\n\n    batch_list = []\n    curr_batch = 0\n    req_cnt = 0\n    batch_list.insert(\n        curr_batch, compute.new_batch_http_request(callback=added_instances_cb))\n\n    for node_name in node_list:\n\n        if req_cnt \u003e= TOT_REQ_CNT:\n            req_cnt = 0\n            curr_batch += 1\n            batch_list.insert(\n                curr_batch,\n                compute.new_batch_http_request(callback=added_instances_cb))\n\n        source_disk_image = get_source_image(compute, node_name)\n\n        pid = util.get_pid(node_name)\n        batch_list[curr_batch].add(\n            create_instance(compute, cfg.partitions[pid].zone,\n                            cfg.partitions[pid].machine_type, node_name,\n                            source_disk_image),\n            request_id=node_name)\n        req_cnt += 1\n\n    try:\n        for i, batch in enumerate(batch_list):\n            batch.execute(http=http)\n            if i \u003c (len(batch_list) - 1):\n                time.sleep(30)\n    except Exception:\n        log.exception(\"error in add batch\")\n\n    if cfg.update_node_addrs:\n        update_slurm_node_addrs(compute)\n\n# [END add_instances]\n\n\ndef main(arg_nodes):\n    log.info(f\"Bursting out: {arg_nodes}\")\n    compute = googleapiclient.discovery.build('compute', 'v1',\n                                              http=authorized_http,\n                                              cache_discovery=False)\n\n    # Get node list\n    nodes_str = util.run(f\"{SCONTROL} show hostnames {arg_nodes}\",\n                         check=True, get_stdout=True).stdout\n    node_list = nodes_str.splitlines()\n\n    while True:\n        add_instances(compute, node_list)\n        if not len(retry_list):\n            break\n\n        log.debug(\"got {} nodes to retry ({})\"\n                  .format(len(retry_list), ','.join(retry_list)))\n        node_list = list(retry_list)\n        del retry_list[:]\n\n    log.debug(\"done adding instances\")\n# [END main]\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(\n        description=__doc__,\n        formatter_class=argparse.RawDescriptionHelpFormatter)\n    parser.add_argument('nodes', help='Nodes to burst')\n\n    args = parser.parse_args()\n\n    main(args.nodes)\n\n",
              "slurm_suspend": "#!/usr/bin/env python3\n\n# Copyright 2017 SchedMD LLC.\n# Modified for use with the Slurm Resource Manager.\n#\n# Copyright 2015 Google Inc. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport argparse\nimport logging\nimport os\nimport time\nfrom pathlib import Path\n\nimport googleapiclient.discovery\n\nimport util\n\ncfg = util.Config.load_config(Path(__file__).with_name('config.yaml'))\n\nSCONTROL = Path(cfg.slurm_cmd_path or '')/'scontrol'\nLOGFILE = (Path(cfg.log_dir or '')/Path(__file__).name).with_suffix('.log')\n\nTOT_REQ_CNT = 1000\n\noperations = {}\nretry_list = []\n\nutil.config_root_logger(level='DEBUG', util_level='ERROR', file=LOGFILE)\nlog = logging.getLogger(Path(__file__).name)\n\n\nif cfg.google_app_cred_path:\n    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = cfg.google_app_cred_path\n\n\ndef delete_instances_cb(request_id, response, exception):\n    if exception is not None:\n        log.error(f\"delete exception for node {request_id}: {exception}\")\n        if \"Rate Limit Exceeded\" in str(exception):\n            retry_list.append(request_id)\n    else:\n        operations[request_id] = response\n# [END delete_instances_cb]\n\n\ndef delete_instances(compute, node_list):\n\n    batch_list = []\n    curr_batch = 0\n    req_cnt = 0\n    batch_list.insert(\n        curr_batch,\n        compute.new_batch_http_request(callback=delete_instances_cb))\n\n    for node_name in node_list:\n        if req_cnt \u003e= TOT_REQ_CNT:\n            req_cnt = 0\n            curr_batch += 1\n            batch_list.insert(\n                curr_batch,\n                compute.new_batch_http_request(callback=delete_instances_cb))\n\n        pid = util.get_pid(node_name)\n        batch_list[curr_batch].add(\n            compute.instances().delete(project=cfg.project,\n                                       zone=cfg.partitions[pid].zone,\n                                       instance=node_name),\n            request_id=node_name)\n        req_cnt += 1\n\n    try:\n        for i, batch in enumerate(batch_list):\n            batch.execute()\n            if i \u003c (len(batch_list) - 1):\n                time.sleep(30)\n    except Exception:\n        log.exception(\"error in batch:\")\n\n# [END delete_instances]\n\n\ndef main(arg_nodes):\n    log.info(\"deleting nodes:\" + arg_nodes)\n    compute = googleapiclient.discovery.build('compute', 'v1',\n                                              cache_discovery=False)\n\n    # Get node list\n    nodes_str = util.run(f\"{SCONTROL} show hostnames {arg_nodes}\",\n                         check=True, get_stdout=True).stdout\n    node_list = nodes_str.splitlines()\n\n    while True:\n        delete_instances(compute, node_list)\n        if not len(retry_list):\n            break\n\n        log.debug(\"got {} nodes to retry ({})\"\n                  .format(len(retry_list), ','.join(retry_list)))\n        node_list = list(retry_list)\n        del retry_list[:]\n\n    log.debug(\"done deleting instances\")\n\n# [END main]\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(\n        description=__doc__,\n        formatter_class=argparse.RawDescriptionHelpFormatter)\n    parser.add_argument('nodes', help='Nodes to release')\n\n    args = parser.parse_args()\n\n    main(args.nodes)\n\n",
              "slurmdbd_conf_tpl": "#ArchiveEvents=yes\n#ArchiveJobs=yes\n#ArchiveResvs=yes\n#ArchiveSteps=no\n#ArchiveSuspend=no\n#ArchiveTXN=no\n#ArchiveUsage=no\n\nAuthType=auth/munge\nDbdHost={CONTROL_MACHINE}\nDebugLevel=debug\n\n#PurgeEventAfter=1month\n#PurgeJobAfter=12month\n#PurgeResvAfter=1month\n#PurgeStepAfter=1month\n#PurgeSuspendAfter=1month\n#PurgeTXNAfter=12month\n#PurgeUsageAfter=24month\n\nLogFile={SLURM_LOG}/slurmdbd.log\nPidFile=/var/run/slurm/slurmdbd.pid\n\nSlurmUser=slurm\n\nStorageLoc={db_name}\n\nStorageType=accounting_storage/mysql\nStorageHost={db_host}\nStoragePort={db_port}\nStorageUser={db_user}\nStoragePass={db_pass}\n#StorageUser=database_mgr\n#StoragePass=shazaam\n\n",
              "slurmsync": "#!/usr/bin/env python3\n\n# Copyright 2019 SchedMD LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport collections\nimport fcntl\nimport logging\nimport os\nimport sys\nimport tempfile\nimport time\nfrom pathlib import Path\n\nimport googleapiclient.discovery\n\nimport util\n\n\ncfg = util.Config.load_config(Path(__file__).with_name('config.yaml'))\n\nSCONTROL = Path(cfg.slurm_cmd_path or '')/'scontrol'\nLOGFILE = (Path(cfg.log_dir or '')/Path(__file__).name).with_suffix('.log')\n\nTOT_REQ_CNT = 1000\n\nretry_list = []\n\nutil.config_root_logger(level='DEBUG', util_level='ERROR', file=LOGFILE)\nlog = logging.getLogger(Path(__file__).name)\n\nif cfg.google_app_cred_path:\n    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = cfg.google_app_cred_path\n\n\ndef start_instances_cb(request_id, response, exception):\n    if exception is not None:\n        log.error(\"start exception: \" + str(exception))\n        if \"Rate Limit Exceeded\" in str(exception):\n            retry_list.append(request_id)\n        elif \"was not found\" in str(exception):\n            util.spawn(f\"/apps/slurm/scripts/resume.py {request_id}\")\n# [END start_instances_cb]\n\n\ndef start_instances(compute, node_list):\n\n    req_cnt = 0\n    curr_batch = 0\n    batch_list = []\n    batch_list.insert(\n        curr_batch,\n        compute.new_batch_http_request(callback=start_instances_cb))\n\n    for node in node_list:\n        if req_cnt \u003e= TOT_REQ_CNT:\n            req_cnt = 0\n            curr_batch += 1\n            batch_list.insert(\n                curr_batch,\n                compute.new_batch_http_request(callback=start_instances_cb))\n\n        pid = util.get_pid(node)\n        batch_list[curr_batch].add(\n            compute.instances().start(project=cfg.project,\n                                      zone=cfg.partitions[pid].zone,\n                                      instance=node),\n            request_id=node)\n        req_cnt += 1\n    try:\n        for i, batch in enumerate(batch_list):\n            batch.execute()\n            if i \u003c (len(batch_list) - 1):\n                time.sleep(30)\n    except Exception:\n        log.exception(\"error in start batch: \")\n\n# [END start_instances]\n\n\ndef main():\n    compute = googleapiclient.discovery.build('compute', 'v1',\n                                              cache_discovery=False)\n\n    try:\n        s_nodes = dict()\n        cmd = (f\"{SCONTROL} show nodes | \"\n               r\"grep -oP '^NodeName=\\K(\\S+)|State=\\K(\\S+)' | \"\n               \"paste -sd',\\n'\")\n        nodes = util.run(cmd, shell=True, check=True, get_stdout=True).stdout\n        if nodes:\n            # result is a list of tuples like:\n            # (nodename, (base='base_state', flags=\u003cset of state flags\u003e))\n            # from 'nodename,base_state+flag1+flag2'\n            # state flags include: CLOUD, COMPLETING, DRAIN, FAIL, POWER,\n            #   POWERING_DOWN\n            # Modifiers on base state still include: @ (reboot), $ (maint),\n            #   * (nonresponsive), # (powering up)\n            StateTuple = collections.namedtuple('StateTuple', 'base,flags')\n\n            def make_state_tuple(state):\n                return StateTuple(state[0], set(state[1:]))\n            s_nodes = [(node, make_state_tuple(args.split('+')))\n                       for node, args in\n                       map(lambda x: x.split(','), nodes.rstrip().splitlines())\n                       if 'CLOUD' in args]\n\n        g_nodes = []\n        for i, part in enumerate(cfg.partitions):\n            page_token = \"\"\n            while True:\n                resp = compute.instances().list(\n                    project=cfg.project, zone=part.zone,\n                    pageToken=page_token,\n                    filter=f\"name={cfg.compute_node_prefix}-{i}-*\"\n                ).execute()\n\n                if \"items\" in resp:\n                    g_nodes.extend(resp['items'])\n                if \"nextPageToken\" in resp:\n                    page_token = resp['nextPageToken']\n                    continue\n\n                break\n\n        to_down = []\n        to_idle = []\n        to_start = []\n        for s_node, s_state in s_nodes:\n            g_node = next((item for item in g_nodes\n                           if item[\"name\"] == s_node),\n                          None)\n            pid = util.get_pid(s_node)\n\n            if (('POWER' not in s_state.flags) and\n                    ('POWERING_DOWN' not in s_state.flags)):\n                # slurm nodes that aren't in power_save and are stopped in GCP:\n                #   mark down in slurm\n                #   start them in gcp\n                if g_node and (g_node['status'] == \"TERMINATED\"):\n                    if not s_state.base.startswith('DOWN'):\n                        to_down.append(s_node)\n                    if (cfg.partitions[pid].preemptible_bursting):\n                        to_start.append(s_node)\n\n                # can't check if the node doesn't exist in GCP while the node\n                # is booting because it might not have been created yet by the\n                # resume script.\n                # This should catch the completing states as well.\n                if (g_node is None and \"#\" not in s_state.base and\n                        not s_state.base.startswith('DOWN')):\n                    to_down.append(s_node)\n\n            elif g_node is None:\n                # find nodes that are down~ in slurm and don't exist in gcp:\n                #   mark idle~\n                if s_state.base.startswith('DOWN') and 'POWER' in s_state.flags:\n                    to_idle.append(s_node)\n                elif 'POWERING_DOWN' in s_state.flags:\n                    to_idle.append(s_node)\n                elif s_state.base.startswith('COMPLETING'):\n                    to_down.append(s_node)\n\n        if len(to_down):\n            log.info(\"{} stopped/deleted instances ({})\".format(\n                len(to_down), \",\".join(to_down)))\n            log.info(\"{} instances to start ({})\".format(\n                len(to_start), \",\".join(to_start)))\n\n            # write hosts to a file that can be given to get a slurm\n            # hostlist. Since the number of hosts could be large.\n            tmp_file = tempfile.NamedTemporaryFile(mode='w+t', delete=False)\n            tmp_file.writelines(\"\\n\".join(to_down))\n            tmp_file.close()\n            log.debug(\"tmp_file = {}\".format(tmp_file.name))\n\n            hostlist = util.run(f\"{SCONTROL} show hostlist {tmp_file.name}\",\n                                check=True, get_stdout=True).stdout\n            log.debug(\"hostlist = {}\".format(hostlist))\n            os.remove(tmp_file.name)\n\n            util.run(f\"{SCONTROL} update nodename={hostlist} state=down \"\n                     \"reason='Instance stopped/deleted'\")\n\n            while True:\n                start_instances(compute, to_start)\n                if not len(retry_list):\n                    break\n\n                log.debug(\"got {} nodes to retry ({})\"\n                          .format(len(retry_list), ','.join(retry_list)))\n                to_start = list(retry_list)\n                del retry_list[:]\n\n        if len(to_idle):\n            log.info(\"{} instances to resume ({})\".format(\n                len(to_idle), ','.join(to_idle)))\n\n            # write hosts to a file that can be given to get a slurm\n            # hostlist. Since the number of hosts could be large.\n            tmp_file = tempfile.NamedTemporaryFile(mode='w+t', delete=False)\n            tmp_file.writelines(\"\\n\".join(to_idle))\n            tmp_file.close()\n            log.debug(\"tmp_file = {}\".format(tmp_file.name))\n\n            hostlist = util.run(f\"{SCONTROL} show hostlist {tmp_file.name}\",\n                                check=True, get_stdout=True).stdout\n            log.debug(\"hostlist = {}\".format(hostlist))\n            os.remove(tmp_file.name)\n\n            util.run(f\"{SCONTROL} update nodename={hostlist} state=resume\")\n\n    except Exception:\n        log.exception(\"failed to sync instances\")\n\n# [END main]\n\n\nif __name__ == '__main__':\n\n    # only run one instance at a time\n    pid_file = (Path('/tmp')/Path(__file__).name).with_suffix('.pid')\n    with pid_file.open('w') as fp:\n        try:\n            fcntl.lockf(fp, fcntl.LOCK_EX | fcntl.LOCK_NB)\n        except IOError:\n            sys.exit(0)\n\n    main()\n\n",
              "startup-script": "#!/bin/bash\n# Copyright 2019 SchedMD LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nPACKAGES=(\n        'bind-utils'\n        'environment-modules'\n        'epel-release'\n        'gcc'\n        'git'\n        'hwloc'\n        'hwloc-devel'\n        'libibmad'\n        'libibumad'\n        'lua'\n        'lua-devel'\n        'man2html'\n        'mariadb'\n        'mariadb-devel'\n        'mariadb-server'\n        'munge'\n        'munge-devel'\n        'munge-libs'\n        'ncurses-devel'\n        'nfs-utils'\n        'numactl'\n        'numactl-devel'\n        'openssl-devel'\n        'pam-devel'\n        'perl-ExtUtils-MakeMaker'\n        'python3'\n        'python3-pip'\n        'readline-devel'\n        'rpm-build'\n        'rrdtool-devel'\n        'vim'\n        'wget'\n        'tmux'\n        'pdsh'\n        'openmpi'\n        'yum-utils'\n    )\n\nPY_PACKAGES=(\n        'pyyaml'\n        'requests'\n        'google-api-python-client'\n    )\n\nPING_HOST=8.8.8.8\nuntil ( ping -q -w1 -c1 $PING_HOST \u003e /dev/null ) ; do\n    echo \"Waiting for internet\"\n    sleep .5\ndone\n\necho \"yum install -y ${PACKAGES[*]}\"\nuntil ( yum install -y ${PACKAGES[*]} \u003e /dev/null ) ; do\n    echo \"yum failed to install packages. Trying again in 5 seconds\"\n    sleep 5\ndone\n\necho   \"pip3 install --upgrade ${PY_PACKAGES[*]}\"\nuntil ( pip3 install --upgrade ${PY_PACKAGES[*]} ) ; do\n    echo \"pip3 failed to install python packages. Trying again in 5 seconds\"\n    sleep 5\ndone\n\nSETUP_SCRIPT=\"setup.py\"\nSETUP_META=\"setup_script\"\nDIR=\"/tmp\"\nURL=\"http://metadata.google.internal/computeMetadata/v1/instance/attributes/$SETUP_META\"\nHEADER=\"Metadata-Flavor:Google\"\necho  \"wget -nv --header $HEADER $URL -O $DIR/$SETUP_SCRIPT\"\nif ! ( wget -nv --header $HEADER $URL -O $DIR/$SETUP_SCRIPT ) ; then\n    echo \"Failed to fetch $SETUP_META:$SETUP_SCRIPT from metadata\"\n    exit 1\nfi\n\necho \"running python cluster setup script\"\nchmod +x $DIR/$SETUP_SCRIPT\n$DIR/$SETUP_SCRIPT\n\n",
              "terraform": "TRUE",
              "util_script": "#!/usr/bin/env python3\n# Copyright 2019 SchedMD LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport logging\nimport logging.config\nimport os\nimport shlex\nimport subprocess\nimport sys\nimport time\nfrom pathlib import Path\nfrom contextlib import contextmanager\nfrom collections import OrderedDict\n\nimport requests\nimport yaml\n\n\nlog = logging.getLogger(__name__)\n\n\ndef config_root_logger(level='DEBUG', util_level=None, file=None):\n    if not util_level:\n        util_level = level\n    handler = 'file_handler' if file else 'stdout_handler'\n    config = {\n        'version': 1,\n        'disable_existing_loggers': True,\n        'formatters': {\n            'standard': {\n                'format': '',\n            },\n            'stamp': {\n                'format': '%(asctime)s %(name)s %(levelname)s: %(message)s',\n            },\n        },\n        'handlers': {\n            'stdout_handler': {\n                'level': 'DEBUG',\n                'formatter': 'standard',\n                'class': 'logging.StreamHandler',\n                'stream': sys.stdout,\n            },\n        },\n        'loggers': {\n            '': {\n                'handlers': [handler],\n                'level': level,\n            },\n            __name__: {  # enable util.py logging\n                'level': util_level,\n            }\n        },\n    }\n    if file:\n        config['handlers']['file_handler'] = {\n            'level': 'DEBUG',\n            'formatter': 'stamp',\n            'class': 'logging.handlers.WatchedFileHandler',\n            'filename': file,\n        }\n    logging.config.dictConfig(config)\n\n\ndef get_metadata(path):\n    \"\"\" Get metadata relative to metadata/computeMetadata/v1/instance/ \"\"\"\n    URL = 'http://metadata.google.internal/computeMetadata/v1/instance/'\n    HEADERS = {'Metadata-Flavor': 'Google'}\n    full_path = URL + path\n    try:\n        resp = requests.get(full_path, headers=HEADERS)\n        resp.raise_for_status()\n    except requests.exceptions.RequestException:\n        log.exception(f\"Error while getting metadata from {full_path}\")\n        return None\n    return resp.text\n\n\ndef run(cmd, wait=0, quiet=False, get_stdout=False,\n        shell=False, universal_newlines=True, **kwargs):\n    \"\"\" run in subprocess. Optional wait after return. \"\"\"\n    if not quiet:\n        log.debug(f\"run: {cmd}\")\n    if get_stdout:\n        kwargs['stdout'] = subprocess.PIPE\n\n    args = cmd if shell else shlex.split(cmd)\n    ret = subprocess.run(args, shell=shell,\n                         universal_newlines=universal_newlines,\n                         **kwargs)\n    if wait:\n        time.sleep(wait)\n    return ret\n\n\ndef spawn(cmd, quiet=False, shell=False, **kwargs):\n    \"\"\" nonblocking spawn of subprocess \"\"\"\n    if not quiet:\n        log.debug(f\"spawn: {cmd}\")\n    args = cmd if shell else shlex.split(cmd)\n    return subprocess.Popen(args, shell=shell, **kwargs)\n\n\ndef get_pid(node_name):\n    \"\"\"Convert \u003cprefix\u003e-\u003cpid\u003e-\u003cnid\u003e\"\"\"\n\n    return int(node_name.split('-')[-2])\n\n\n@contextmanager\ndef cd(path):\n    \"\"\" Change working directory for context \"\"\"\n    prev = Path.cwd()\n    os.chdir(path)\n    try:\n        yield\n    finally:\n        os.chdir(prev)\n\n\ndef static_vars(**kwargs):\n    \"\"\"\n    Add variables to the function namespace.\n    @static_vars(var=init): var must be referenced func.var\n    \"\"\"\n    def decorate(func):\n        for k in kwargs:\n            setattr(func, k, kwargs[k])\n        return func\n    return decorate\n\n\nclass cached_property:\n    \"\"\"\n    Descriptor for creating a property that is computed once and cached\n    \"\"\"\n    def __init__(self, factory):\n        self._attr_name = factory.__name__\n        self._factory = factory\n\n    def __get__(self, instance, owner=None):\n        if instance is None:  # only if invoked from class\n            return self\n        attr = self._factory(instance)\n        setattr(instance, self._attr_name, attr)\n        return attr\n\n\nclass Config(OrderedDict):\n    \"\"\" Loads config from yaml and holds values in nested namespaces \"\"\"\n\n    TYPES = set(('compute', 'login', 'controller'))\n    # PROPERTIES defines which properties in slurm.jinja.schema are included\n    #   in the config file. SAVED_PROPS are saved to file via save_config.\n    SAVED_PROPS = ('project',\n                   'zone',\n                   'cluster_name',\n                   'external_compute_ips',\n                   'shared_vpc_host_project',\n                   'compute_node_prefix',\n                   'compute_node_service_account',\n                   'compute_node_scopes',\n                   'slurm_cmd_path',\n                   'log_dir',\n                   'google_app_cred_path',\n                   'update_node_addrs',\n                   'partitions',\n                   )\n    PROPERTIES = (*SAVED_PROPS,\n                  'munge_key',\n                  'external_compute_ips',\n                  'nfs_home_server',\n                  'nfs_home_dir',\n                  'nfs_apps_server',\n                  'nfs_apps_dir',\n                  'ompi_version',\n                  'controller_secondary_disk',\n                  'slurm_version',\n                  'suspend_time',\n                  'network_storage',\n                  'login_network_storage',\n                  'login_node_count',\n                  'cloudsql',\n                  )\n\n    def __init__(self, *args, **kwargs):\n        def from_nested(value):\n            \"\"\" If value is dict, convert to Config. Also recurse lists. \"\"\"\n            if isinstance(value, dict):\n                return Config({k: from_nested(v) for k, v in value.items()})\n            elif isinstance(value, list):\n                return [from_nested(v) for v in value]\n            else:\n                return value\n\n        super(Config, self).__init__(*args, **kwargs)\n        self.__dict__ = self  # all properties are member attributes\n\n        # Convert nested dicts to Configs\n        for k, v in self.items():\n            self[k] = from_nested(v)\n\n    @classmethod\n    def new_config(cls, properties):\n        # If k is ever not found, None will be inserted as the value\n        return cls({k: properties.setdefault(k, None) for k in cls.PROPERTIES})\n\n    @classmethod\n    def load_config(cls, path):\n        config = yaml.safe_load(Path(path).read_text())\n        return cls(config)\n\n    def save_config(self, path):\n        save_dict = Config([(k, self[k]) for k in self.SAVED_PROPS])\n        Path(path).write_text(yaml.dump(save_dict, Dumper=self.Dumper))\n\n    @cached_property\n    def instance_type(self):\n        # get tags, intersect with possible types, get the first or none\n        tags = yaml.safe_load(get_metadata('tags'))\n        # TODO what to default to if no match found.\n        return next(iter(set(tags) \u0026 self.TYPES), None)\n\n    @property\n    def region(self):\n        return self.zone and '-'.join(self.zone.split('-')[:-1])\n\n    def __getattr__(self, item):\n        \"\"\" only called if item is not found in self \"\"\"\n        return None\n\n    class Dumper(yaml.SafeDumper):\n        def __init__(self, *args, **kwargs):\n            super().__init__(*args, **kwargs)\n            self.add_representer(Config, self.represent_config)\n\n        @staticmethod\n        def represent_config(dumper, data):\n            return dumper.represent_mapping('tag:yaml.org,2002:map',\n                                            data.items())\n\n"
            },
            "metadata_fingerprint": "UvesrLRslhs=",
            "metadata_startup_script": null,
            "min_cpu_platform": "",
            "name": "gcluster-controller",
            "network_interface": [
              {
                "access_config": [
                  {
                    "nat_ip": "35.185.207.81",
                    "network_tier": "PREMIUM",
                    "public_ptr_domain_name": ""
                  }
                ],
                "alias_ip_range": [],
                "name": "nic0",
                "network": "https://www.googleapis.com/compute/v1/projects/playground-190420/global/networks/vf-network",
                "network_ip": "192.168.0.28",
                "subnetwork": "https://www.googleapis.com/compute/v1/projects/playground-190420/regions/us-west1/subnetworks/vf-west-subnet",
                "subnetwork_project": "playground-190420"
              }
            ],
            "project": "playground-190420",
            "scheduling": [
              {
                "automatic_restart": true,
                "node_affinities": [],
                "on_host_maintenance": "MIGRATE",
                "preemptible": false
              }
            ],
            "scratch_disk": [],
            "self_link": "https://www.googleapis.com/compute/v1/projects/playground-190420/zones/us-west1-a/instances/gcluster-controller",
            "service_account": [
              {
                "email": "861735104433-compute@developer.gserviceaccount.com",
                "scopes": [
                  "https://www.googleapis.com/auth/cloud-platform"
                ]
              }
            ],
            "shielded_instance_config": [
              {
                "enable_integrity_monitoring": true,
                "enable_secure_boot": false,
                "enable_vtpm": true
              }
            ],
            "tags": [
              "controller"
            ],
            "tags_fingerprint": "e2aTiSrlOyM=",
            "timeouts": null,
            "zone": "us-west1-a"
          },
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjoxMjAwMDAwMDAwMDAwLCJkZWxldGUiOjEyMDAwMDAwMDAwMDAsInVwZGF0ZSI6MTIwMDAwMDAwMDAwMH0sInNjaGVtYV92ZXJzaW9uIjoiNiJ9",
          "dependencies": [
            "module.slurm_cluster_controller.google_compute_disk.secondary",
            "module.slurm_cluster_network.google_compute_subnetwork.cluster_subnet"
          ]
        }
      ]
    },
    {
      "module": "module.slurm_cluster_login",
      "mode": "managed",
      "type": "google_compute_instance",
      "name": "login_node",
      "each": "list",
      "provider": "provider.google",
      "instances": [
        {
          "index_key": 0,
          "schema_version": 6,
          "attributes": {
            "allow_stopping_for_update": null,
            "attached_disk": [],
            "boot_disk": [
              {
                "auto_delete": true,
                "device_name": "persistent-disk-0",
                "disk_encryption_key_raw": "",
                "disk_encryption_key_sha256": "",
                "initialize_params": [
                  {
                    "image": "https://www.googleapis.com/compute/v1/projects/centos-cloud/global/images/centos-7-v20200403",
                    "labels": {},
                    "size": 20,
                    "type": "pd-standard"
                  }
                ],
                "kms_key_self_link": "",
                "mode": "READ_WRITE",
                "source": "https://www.googleapis.com/compute/v1/projects/playground-190420/zones/us-west1-a/disks/gcluster-login0"
              }
            ],
            "can_ip_forward": false,
            "cpu_platform": "Intel Broadwell",
            "current_status": "RUNNING",
            "deletion_protection": false,
            "description": "",
            "desired_status": null,
            "enable_display": false,
            "guest_accelerator": [],
            "hostname": "",
            "id": "projects/playground-190420/zones/us-west1-a/instances/gcluster-login0",
            "instance_id": "2247441854255110359",
            "label_fingerprint": "42WmSpB8rSM=",
            "labels": null,
            "machine_type": "n1-standard-2",
            "metadata": {
              "VmDnsSetting": "GlobalOnly",
              "config": "{\"cluster_name\":\"gcluster\",\"compute_node_prefix\":\"gcluster-compute\",\"controller_secondary_disk\":false,\"login_network_storage\":[],\"munge_key\":null,\"network_storage\":[{\"fs_type\":\"nfs\",\"local_mount\":\"/mnt/virtualflow\",\"mount_options\":\"defaults,hard,intr\",\"remote_mount\":\"/virtualflow\",\"server_ip\":\"10.102.14.42\"}],\"ompi_version\":null}\n",
              "enable-oslogin": "TRUE",
              "setup_script": "#!/usr/bin/env python3\n\n# Copyright 2017 SchedMD LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport datetime\nimport importlib\nimport itertools as it\nimport logging\nimport os\nimport shutil\nimport socket\nimport sys\nimport time\nimport threading\nimport urllib.request\nfrom pathlib import Path\nfrom subprocess import DEVNULL\n\nimport googleapiclient.discovery\nimport requests\nimport yaml\n\n\n# get util.py from metadata\nUTIL_FILE = Path('/tmp/util.py')\ntry:\n    resp = requests.get('http://metadata.google.internal/computeMetadata/v1/instance/attributes/util_script',\n                        headers={'Metadata-Flavor': 'Google'})\n    resp.raise_for_status()\n    UTIL_FILE.write_text(resp.text)\nexcept requests.exceptions.RequestException:\n    print(\"util.py script not found in metadata\")\n    if not UTIL_FILE.exists():\n        print(f\"{UTIL_FILE} also does not exist, aborting\")\n        sys.exit(1)\n\nspec = importlib.util.spec_from_file_location('util', UTIL_FILE)\nutil = importlib.util.module_from_spec(spec)\nsys.modules[spec.name] = util\nspec.loader.exec_module(util)\ncd = util.cd  # import util.cd into local namespace\n\nutil.config_root_logger()\nlog = logging.getLogger(Path(__file__).name)\n\n# get setup config from metadata\nconfig_yaml = yaml.safe_load(util.get_metadata('attributes/config'))\nif not util.get_metadata('attributes/terraform'):\n    config_yaml = yaml.safe_load(config_yaml)\ncfg = util.Config.new_config(config_yaml)\n\nHOME_DIR = Path('/home')\nAPPS_DIR = Path('/apps')\nCURR_SLURM_DIR = APPS_DIR/'slurm/current'\nMUNGE_DIR = Path('/etc/munge')\nSLURM_LOG = Path('/var/log/slurm')\n\nSEC_DISK_DIR = Path('/mnt/disks/sec')\nRESUME_TIMEOUT = 300\nSUSPEND_TIMEOUT = 300\n\nCONTROL_MACHINE = cfg.cluster_name + '-controller'\n\nMOTD_HEADER = \"\"\"\n\n                                 SSSSSSS\n                                SSSSSSSSS\n                                SSSSSSSSS\n                                SSSSSSSSS\n                        SSSS     SSSSSSS     SSSS\n                       SSSSSS               SSSSSS\n                       SSSSSS    SSSSSSS    SSSSSS\n                        SSSS    SSSSSSSSS    SSSS\n                SSS             SSSSSSSSS             SSS\n               SSSSS    SSSS    SSSSSSSSS    SSSS    SSSSS\n                SSS    SSSSSS   SSSSSSSSS   SSSSSS    SSS\n                       SSSSSS    SSSSSSS    SSSSSS\n                SSS    SSSSSS               SSSSSS    SSS\n               SSSSS    SSSS     SSSSSSS     SSSS    SSSSS\n          S     SSS             SSSSSSSSS             SSS     S\n         SSS            SSSS    SSSSSSSSS    SSSS            SSS\n          S     SSS    SSSSSS   SSSSSSSSS   SSSSSS    SSS     S\n               SSSSS   SSSSSS   SSSSSSSSS   SSSSSS   SSSSS\n          S    SSSSS    SSSS     SSSSSSS     SSSS    SSSSS    S\n    S    SSS    SSS                                   SSS    SSS    S\n    S     S                                                   S     S\n                SSS\n                SSS\n                SSS\n                SSS\n SSSSSSSSSSSS   SSS   SSSS       SSSS    SSSSSSSSS   SSSSSSSSSSSSSSSSSSSS\nSSSSSSSSSSSSS   SSS   SSSS       SSSS   SSSSSSSSSS  SSSSSSSSSSSSSSSSSSSSSS\nSSSS            SSS   SSSS       SSSS   SSSS        SSSS     SSSS     SSSS\nSSSS            SSS   SSSS       SSSS   SSSS        SSSS     SSSS     SSSS\nSSSSSSSSSSSS    SSS   SSSS       SSSS   SSSS        SSSS     SSSS     SSSS\n SSSSSSSSSSSS   SSS   SSSS       SSSS   SSSS        SSSS     SSSS     SSSS\n         SSSS   SSS   SSSS       SSSS   SSSS        SSSS     SSSS     SSSS\n         SSSS   SSS   SSSS       SSSS   SSSS        SSSS     SSSS     SSSS\nSSSSSSSSSSSSS   SSS   SSSSSSSSSSSSSSS   SSSS        SSSS     SSSS     SSSS\nSSSSSSSSSSSS    SSS    SSSSSSSSSSSSS    SSSS        SSSS     SSSS     SSSS\n\n\n\"\"\"\n\n\ndef add_slurm_user():\n\n    util.run(\"useradd -m -c SlurmUser -d /var/lib/slurm -U -r slurm\")\n# END add_slurm_user()\n\n\ndef setup_modules():\n\n    appsmfs = Path('/apps/modulefiles')\n\n    with open('/usr/share/Modules/init/.modulespath', 'r+') as dotmp:\n        if str(appsmfs) not in dotmp.read():\n            if cfg.instance_type == 'controller' and not appsmfs.is_dir():\n                appsmfs.mkdir(parents=True)\n            # after read, file cursor is at end of file\n            dotmp.write(f'\\n{appsmfs}\\n')\n# END setup_modules\n\n\ndef start_motd():\n\n    msg = MOTD_HEADER + \"\"\"\n*** Slurm is currently being installed/configured in the background. ***\nA terminal broadcast will announce when installation and configuration is\ncomplete.\n\nPartitions will be marked down until the compute image has been created.\nFor instances with gpus attached, it could take ~10 mins after the controller\nhas finished installing.\n\n\"\"\"\n\n    if cfg.instance_type != \"controller\":\n        msg += \"\"\"/home on the controller will be mounted over the existing /home.\nAny changes in /home will be hidden. Please wait until the installation is\ncomplete before making changes in your home directory.\n\n\"\"\"\n\n    with open('/etc/motd', 'w') as f:\n        f.write(msg)\n# END start_motd()\n\n\ndef end_motd(broadcast=True):\n\n    with open('/etc/motd', 'w') as f:\n        f.write(MOTD_HEADER)\n\n    if not broadcast:\n        return\n\n    util.run(\"wall -n '*** Slurm {} daemon installation complete ***'\"\n             .format(cfg.instance_type))\n\n    if cfg.instance_type != 'controller':\n        util.run(\"\"\"wall -n '\n/home on the controller was mounted over the existing /home.\nEither log out and log back in or cd into ~.\n'\"\"\")\n# END start_motd()\n\n\ndef have_gpus(hostname):\n\n    pid = util.get_pid(hostname)\n    return cfg.partitions[pid].gpu_count \u003e 0\n# END have_gpus()\n\n\ndef install_slurmlog_conf():\n    \"\"\" Install fluentd config for slurm logs \"\"\"\n\n    slurmlog_config = util.get_metadata('attributes/fluentd_conf_tpl')\n    if slurmlog_config:\n        Path('/etc/google-fluentd/config.d/slurmlogs.conf').write_text(\n            slurmlog_config)\n\n\ndef install_packages():\n\n    # install stackdriver monitoring and logging\n    add_mon_script = Path('/tmp/add-monitoring-agent-repo.sh')\n    add_mon_url = f'https://dl.google.com/cloudagents/{add_mon_script.name}'\n    urllib.request.urlretrieve(add_mon_url, add_mon_script)\n    util.run(f\"bash {add_mon_script}\")\n    util.run(\"yum install -y stackdriver-agent\")\n\n    add_log_script = Path('/tmp/install-logging-agent.sh')\n    add_log_url = f'https://dl.google.com/cloudagents/{add_log_script.name}'\n    urllib.request.urlretrieve(add_log_url, add_log_script)\n    util.run(f\"bash {add_log_script}\")\n    install_slurmlog_conf()\n\n    util.run(\"systemctl enable stackdriver-agent google-fluentd\")\n    util.run(\"systemctl start stackdriver-agent google-fluentd\")\n\n    # install cuda if needed\n    if cfg.instance_type == 'compute' and have_gpus(socket.gethostname()):\n        util.run(\"yum -y install kernel-devel-$(uname -r) kernel-headers-$(uname -r)\",\n                 shell=True)\n        repo = 'http://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/cuda-rhel7.repo'\n        util.run(f\"yum-config-manager --add-repo {repo}\")\n        util.run(\"yum clean all\")\n        util.run(\"yum -y install nvidia-driver-latest-dkms cuda\")\n        util.run(\"yum -y install cuda-drivers\")\n        # Creates the device files\n        util.run(\"nvidia-smi\")\n# END install_packages()\n\n\ndef setup_munge():\n\n    munge_service_patch = Path('/usr/lib/systemd/system/munge.service')\n    req_mount = (f\"\\nRequiresMountsFor={MUNGE_DIR}\"\n                 if cfg.instance_type != 'controller' else '')\n    with munge_service_patch.open('w') as f:\n        f.write(f\"\"\"\n[Unit]\nDescription=MUNGE authentication service\nDocumentation=man:munged(8)\nAfter=network.target\nAfter=syslog.target\nAfter=time-sync.target{req_mount}\n\n[Service]\nType=forking\nExecStart=/usr/sbin/munged --num-threads=10\nPIDFile=/var/run/munge/munged.pid\nUser=munge\nGroup=munge\nRestart=on-abort\n\n[Install]\nWantedBy=multi-user.target\n\"\"\")\n\n    util.run(\"systemctl enable munge\")\n\n    if cfg.instance_type != 'controller':\n        return\n\n    if cfg.munge_key:\n        with (MUNGE_DIR/'munge.key').open('w') as f:\n            f.write(cfg.munge_key)\n\n        util.run(f\"chown -R munge: {MUNGE_DIR} /var/log/munge/\")\n\n        (MUNGE_DIR/'munge_key').chmod(0o400)\n        MUNGE_DIR.chmod(0o700)\n        Path('var/log/munge/').chmod(0o700)\n    else:\n        util.run('create-munge-key')\n# END setup_munge ()\n\n\ndef start_munge():\n    util.run(\"systemctl start munge\")\n# END start_munge()\n\n\ndef setup_nfs_exports():\n\n    export_paths = (\n        (HOME_DIR, not EXTERNAL_MOUNT_HOME),\n        (APPS_DIR, not EXTERNAL_MOUNT_APPS),\n        (MUNGE_DIR, not EXTERNAL_MOUNT_MUNGE),\n        (SEC_DISK_DIR, cfg.controller_secondary_disk),\n    )\n\n    # export path if corresponding selector boolean is True\n    for path in it.compress(*zip(*export_paths)):\n        util.run(rf\"sed -i '\\#{path}#d' /etc/exports\")\n        with open('/etc/exports', 'a') as f:\n            f.write(f\"\\n{path}  *(rw,no_subtree_check,no_root_squash)\")\n\n    util.run(\"exportfs -a\")\n# END setup_nfs_exports()\n\n\ndef expand_machine_type():\n\n    machines = []\n    compute = googleapiclient.discovery.build('compute', 'v1',\n                                              cache_discovery=False)\n    for part in cfg.partitions:\n        machine = {'cpus': 1, 'memory': 1}\n        try:\n            type_resp = compute.machineTypes().get(\n                project=cfg.project, zone=part.zone,\n                machineType=part.machine_type).execute()\n            if type_resp:\n                machine['cpus'] = type_resp['guestCpus']\n\n                # Because the actual memory on the host will be different than\n                # what is configured (e.g. kernel will take it). From\n                # experiments, about 16 MB per GB are used (plus about 400 MB\n                # buffer for the first couple of GB's. Using 30 MB to be safe.\n                gb = type_resp['memoryMb'] // 1024\n                machine['memory'] = type_resp['memoryMb'] - (400 + (gb * 30))\n\n        except Exception:\n            log.exception(\"Failed to get MachineType '{}' from google api\"\n                          .format(part.machine_type))\n        finally:\n            machines.append(machine)\n\n    return machines\n# END expand_machine_type()\n\n\ndef install_slurm_conf():\n\n    machines = expand_machine_type()\n\n    if cfg.ompi_version:\n        mpi_default = \"pmi2\"\n    else:\n        mpi_default = \"none\"\n\n    conf_resp = util.get_metadata('attributes/slurm_conf_tpl')\n    conf = conf_resp.format(**globals(), **locals())\n\n    static_nodes = []\n    for i, machine in enumerate(machines):\n        part = cfg.partitions[i]\n        static_range = ''\n        if part.static_node_count:\n            if part.static_node_count \u003e 1:\n                static_range = '{}-{}-[0-{}]'.format(\n                    cfg.compute_node_prefix, i, part.static_node_count - 1)\n            else:\n                static_range = f\"{cfg.compute_node_prefix}-{i}-0\"\n\n        cloud_range = \"\"\n        if (part.max_node_count and\n                (part.max_node_count != part.static_node_count)):\n            cloud_range = \"{}-{}-[{}-{}]\".format(\n                cfg.compute_node_prefix, i, part.static_node_count,\n                part.max_node_count - 1)\n\n        conf += (\"NodeName=DEFAULT \"\n                 f\"CPUs={machine['cpus']} \"\n                 f\"RealMemory={machine['memory']} \"\n                 \"State=UNKNOWN\")\n        conf += '\\n'\n\n        # Nodes\n        gres = \"\"\n        if part.gpu_count:\n            gres = \" Gres=gpu:\" + str(part.gpu_count)\n        if static_range:\n            static_nodes.append(static_range)\n            conf += f\"NodeName={static_range}{gres}\\n\"\n\n        if cloud_range:\n            conf += f\"NodeName={cloud_range} State=CLOUD{gres}\\n\"\n\n        # Partitions\n        part_nodes = f'-{i}-[0-{part.max_node_count - 1}]'\n\n        def_mem_per_cpu = max(100, machine['memory'] // machine['cpus'])\n\n        conf += (\"PartitionName={} Nodes={}-compute{} MaxTime=INFINITE \"\n                 \"State=UP DefMemPerCPU={} LLN=yes\"\n                 .format(part.name, cfg.cluster_name, part_nodes,\n                         def_mem_per_cpu))\n\n        # First partition specified is treated as the default partition\n        if i == 0:\n            conf += \" Default=YES\"\n        conf += \"\\n\\n\"\n\n    if len(static_nodes):\n        conf += \"\\nSuspendExcNodes={}\\n\".format(','.join(static_nodes))\n\n    etc_dir = CURR_SLURM_DIR/'etc'\n    if not etc_dir.exists():\n        etc_dir.mkdir(parents=True)\n    with (etc_dir/'slurm.conf').open('w') as f:\n        f.write(conf)\n# END install_slurm_conf()\n\n\ndef install_slurmdbd_conf():\n    if cfg.cloudsql:\n        db_name = cfg.cloudsql['db_name']\n        db_user = cfg.cloudsql['user']\n        db_pass = cfg.cloudsql['password']\n        db_host_str = cfg.cloudsql['server_ip'].split(':')\n        db_host = db_host_str[0]\n        db_port = db_host_str[1] if len(db_host_str) \u003e= 2 else '3306'\n    else:\n        db_name = \"slurm_acct_db\"\n        db_user = 'slurm'\n        db_pass = '\"\"'\n        db_host = 'localhost'\n        db_port = '3306'\n\n    conf_resp = util.get_metadata('attributes/slurmdbd_conf_tpl')\n    conf = conf_resp.format(**globals(), **locals())\n\n    etc_dir = CURR_SLURM_DIR/'etc'\n    if not etc_dir.exists():\n        etc_dir.mkdir(parents=True)\n    (etc_dir/'slurmdbd.conf').write_text(conf)\n    (etc_dir/'slurmdbd.conf').chmod(0o600)\n\n# END install_slurmdbd_conf()\n\n\ndef install_cgroup_conf():\n\n    conf = util.get_metadata('attributes/cgroup_conf_tpl')\n\n    etc_dir = CURR_SLURM_DIR/'etc'\n    with (etc_dir/'cgroup.conf').open('w') as f:\n        f.write(conf)\n\n    gpu_parts = [(i, x) for i, x in enumerate(cfg.partitions)\n                 if x.gpu_count]\n    gpu_conf = \"\"\n    for i, part in gpu_parts:\n        driver_range = '0'\n        if part.gpu_count \u003e 1:\n            driver_range = '[0-{}]'.format(part.gpu_count-1)\n\n        gpu_conf += (\"NodeName={}-{}-[0-{}] Name=gpu File=/dev/nvidia{}\\n\"\n                     .format(cfg.compute_node_prefix, i,\n                             part.max_node_count - 1, driver_range))\n    if gpu_conf:\n        with (etc_dir/'gres.conf').open('w') as f:\n            f.write(gpu_conf)\n\n# END install_cgroup_conf()\n\n\ndef install_meta_files():\n\n    scripts_path = APPS_DIR/'slurm/scripts'\n    if not scripts_path.exists():\n        scripts_path.mkdir(parents=True)\n\n    cfg.slurm_cmd_path = str(CURR_SLURM_DIR/'bin')\n    cfg.log_dir = str(SLURM_LOG)\n\n    cfg.save_config(scripts_path/'config.yaml')\n\n    meta_files = [\n        ('suspend.py', 'slurm_suspend'),\n        ('resume.py', 'slurm_resume'),\n        ('slurmsync.py', 'slurmsync'),\n        ('util.py', 'util_script'),\n        ('compute-shutdown', 'compute-shutdown'),\n        ('custom-compute-install', 'custom-compute-install'),\n        ('custom-controller-install', 'custom-controller-install'),\n    ]\n\n    for file_name, meta_name in meta_files:\n        text = util.get_metadata('attributes/' + meta_name)\n        if not text:\n            continue\n\n        with (scripts_path/file_name).open('w') as f:\n            f.write(text)\n        (scripts_path/file_name).chmod(0o755)\n\n    util.run(\n        \"gcloud compute instances remove-metadata {} --zone={} --keys={}\"\n        .format(CONTROL_MACHINE, cfg.zone,\n                ','.join([x[1] for x in meta_files])))\n\n# END install_meta_files()\n\n\ndef install_slurm():\n\n    src_path = APPS_DIR/'slurm/src'\n    if not src_path.exists():\n        src_path.mkdir(parents=True)\n\n    with cd(src_path):\n        use_version = ''\n        if (cfg.slurm_version[0:2] == 'b:'):\n            GIT_URL = 'https://github.com/SchedMD/slurm.git'\n            use_version = cfg.slurm_version[2:]\n            util.run(\n                \"git clone -b {0} {1} {0}\".format(use_version, GIT_URL))\n        else:\n            file = 'slurm-{}.tar.bz2'.format(cfg.slurm_version)\n            slurm_url = 'https://download.schedmd.com/slurm/' + file\n            urllib.request.urlretrieve(slurm_url, src_path/file)\n\n            use_version = util.run(f\"tar -xvjf {file}\", check=True,\n                                   get_stdout=True).stdout.splitlines()[0][:-1]\n\n    SLURM_PREFIX = APPS_DIR/'slurm'/use_version\n\n    build_dir = src_path/use_version/'build'\n    if not build_dir.exists():\n        build_dir.mkdir(parents=True)\n\n    with cd(build_dir):\n        util.run(\"../configure --prefix={} --sysconfdir={}/etc\"\n                 .format(SLURM_PREFIX, CURR_SLURM_DIR), stdout=DEVNULL)\n        util.run(\"make -j install\", stdout=DEVNULL)\n    with cd(build_dir/'contribs'):\n        util.run(\"make -j install\", stdout=DEVNULL)\n\n    os.symlink(SLURM_PREFIX, CURR_SLURM_DIR)\n\n    state_dir = APPS_DIR/'slurm/state'\n    if not state_dir.exists():\n        state_dir.mkdir(parents=True)\n        util.run(f\"chown -R slurm: {state_dir}\")\n\n    install_slurm_conf()\n    install_slurmdbd_conf()\n    install_cgroup_conf()\n    install_meta_files()\n\n# END install_slurm()\n\n\ndef install_slurm_tmpfile():\n\n    run_dir = Path('/var/run/slurm')\n\n    with open('/etc/tmpfiles.d/slurm.conf', 'w') as f:\n        f.write(f\"\\nd {run_dir} 0755 slurm slurm -\")\n\n    if not run_dir.exists():\n        run_dir.mkdir(parents=True)\n    run_dir.chmod(0o755)\n\n    util.run(f\"chown slurm: {run_dir}\")\n\n# END install_slurm_tmpfile()\n\n\ndef install_controller_service_scripts():\n\n    install_slurm_tmpfile()\n\n    # slurmctld.service\n    ctld_service = Path('/usr/lib/systemd/system/slurmctld.service')\n    with ctld_service.open('w') as f:\n        f.write(\"\"\"\n[Unit]\nDescription=Slurm controller daemon\nAfter=network.target munge.service\nConditionPathExists={prefix}/etc/slurm.conf\n\n[Service]\nType=forking\nEnvironmentFile=-/etc/sysconfig/slurmctld\nExecStart={prefix}/sbin/slurmctld $SLURMCTLD_OPTIONS\nExecReload=/bin/kill -HUP $MAINPID\nPIDFile=/var/run/slurm/slurmctld.pid\n\n[Install]\nWantedBy=multi-user.target\n\"\"\".format(prefix=CURR_SLURM_DIR))\n\n    ctld_service.chmod(0o644)\n\n    # slurmdbd.service\n    dbd_service = Path('/usr/lib/systemd/system/slurmdbd.service')\n    with dbd_service.open('w') as f:\n        f.write(\"\"\"\n[Unit]\nDescription=Slurm DBD accounting daemon\nAfter=network.target munge.service\nConditionPathExists={prefix}/etc/slurmdbd.conf\n\n[Service]\nType=forking\nEnvironmentFile=-/etc/sysconfig/slurmdbd\nExecStart={prefix}/sbin/slurmdbd $SLURMDBD_OPTIONS\nExecReload=/bin/kill -HUP $MAINPID\nPIDFile=/var/run/slurm/slurmdbd.pid\n\n[Install]\nWantedBy=multi-user.target\n\"\"\".format(prefix=CURR_SLURM_DIR))\n\n    dbd_service.chmod(0o644)\n\n# END install_controller_service_scripts()\n\n\ndef install_compute_service_scripts():\n\n    install_slurm_tmpfile()\n\n    # slurmd.service\n    slurmd_service = Path('/usr/lib/systemd/system/slurmd.service')\n    with slurmd_service.open('w') as f:\n        f.write(\"\"\"\n[Unit]\nDescription=Slurm node daemon\nAfter=network.target munge.service home.mount apps.mount etc-munge.mount\nConditionPathExists={prefix}/etc/slurm.conf\n\n[Service]\nType=forking\nEnvironmentFile=-/etc/sysconfig/slurmd\nExecStart={prefix}/sbin/slurmd $SLURMD_OPTIONS\nExecReload=/bin/kill -HUP $MAINPID\nPIDFile=/var/run/slurm/slurmd.pid\nKillMode=process\nLimitNOFILE=51200\nLimitMEMLOCK=infinity\nLimitSTACK=infinity\n\n[Install]\nWantedBy=multi-user.target\n\"\"\".format(prefix=CURR_SLURM_DIR))\n\n    slurmd_service.chmod(0o644)\n    util.run('systemctl enable slurmd')\n\n# END install_compute_service_scripts()\n\n\ndef setup_bash_profile():\n\n    with open('/etc/profile.d/slurm.sh', 'w') as f:\n        f.write(\"\"\"\nS_PATH={}\nPATH=$PATH:$S_PATH/bin:$S_PATH/sbin\n\"\"\".format(CURR_SLURM_DIR))\n\n    if cfg.instance_type == 'compute' and have_gpus(socket.gethostname()):\n        with open('/etc/profile.d/cuda.sh', 'w') as f:\n            f.write(\"\"\"\nCUDA_PATH=/usr/local/cuda\nPATH=$CUDA_PATH/bin${PATH:+:${PATH}}\nLD_LIBRARY_PATH=$CUDA_PATH/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\n\"\"\")\n\n# END setup_bash_profile()\n\n\ndef setup_ompi_bash_profile():\n    if not cfg.ompi_version:\n        return\n    with open(f'/etc/profile.d/ompi-{cfg.ompi_version}.sh', 'w') as f:\n        f.write(f\"PATH={APPS_DIR}/ompi/{cfg.ompi_version}/bin:$PATH\")\n# END setup_ompi_bash_profile()\n\n\ndef setup_logrotate():\n    with open('/etc/logrotate.d/slurm', 'w') as f:\n        f.write(\"\"\"\n##\n# Slurm Logrotate Configuration\n##\n/var/log/slurm/*.log {\n        compress\n        missingok\n        nocopytruncate\n        nodelaycompress\n        nomail\n        notifempty\n        noolddir\n        rotate 5\n        sharedscripts\n        size=5M\n        create 640 slurm root\n        postrotate\n                pkill -x --signal SIGUSR2 slurmctld\n                pkill -x --signal SIGUSR2 slurmd\n                pkill -x --signal SIGUSR2 slurmdbd\n                exit 0\n        endscript\n}\n\"\"\")\n# END setup_logrotate()\n\n\ndef setup_network_storage():\n    log.info(\"Set up network storage\")\n\n    global EXTERNAL_MOUNT_APPS\n    global EXTERNAL_MOUNT_HOME\n    global EXTERNAL_MOUNT_MUNGE\n\n    EXTERNAL_MOUNT_APPS = False\n    EXTERNAL_MOUNT_HOME = False\n    EXTERNAL_MOUNT_MUNGE = False\n    cifs_installed = False\n\n    # create dict of mounts, local_mount: mount_info\n    if cfg.instance_type == 'controller':\n        ext_mounts = {}\n    else:  # on non-controller instances, low priority mount these\n        CONTROL_NFS = {\n            'server_ip': CONTROL_MACHINE,\n            'remote_mount': 'none',\n            'local_mount': 'none',\n            'fs_type': 'nfs',\n            'mount_options': 'defaults,hard,intr',\n        }\n        ext_mounts = {\n            HOME_DIR: dict(CONTROL_NFS, remote_mount=HOME_DIR,\n                           local_mount=HOME_DIR),\n            APPS_DIR: dict(CONTROL_NFS, remote_mount=APPS_DIR,\n                           local_mount=APPS_DIR),\n            MUNGE_DIR: dict(CONTROL_NFS, remote_mount=MUNGE_DIR,\n                            local_mount=MUNGE_DIR),\n        }\n\n    # convert network_storage list of mounts to dict of mounts,\n    #   local_mount as key\n    def listtodict(mountlist):\n        return {Path(d['local_mount']).resolve(): d for d in mountlist}\n\n    ext_mounts.update(listtodict(cfg.network_storage))\n    if cfg.instance_type == 'compute':\n        pid = util.get_pid(socket.gethostname())\n        ext_mounts.update(listtodict(cfg.partitions[pid].network_storage))\n    else:\n        ext_mounts.update(listtodict(cfg.login_network_storage))\n\n    # Install lustre, cifs, and/or gcsfuse as needed and write mount to fstab\n    fstab_entries = []\n    for local_mount, mount in ext_mounts.items():\n        remote_mount = mount['remote_mount']\n        fs_type = mount['fs_type']\n        server_ip = mount['server_ip']\n        log.info(\"Setting up mount ({}) {}{} to {}\".format(\n            fs_type, server_ip+':' if fs_type != 'gcsfuse' else \"\",\n            remote_mount, local_mount))\n        if not local_mount.exists():\n            local_mount.mkdir(parents=True)\n        # Check if we're going to overlap with what's normally hosted on the\n        # controller (/apps, /home, /etc/munge).\n        # If so delete the entries pointing to the controller, and tell the\n        # nodes.\n        if local_mount == APPS_DIR:\n            EXTERNAL_MOUNT_APPS = True\n        elif local_mount == HOME_DIR:\n            EXTERNAL_MOUNT_HOME = True\n        elif local_mount == MUNGE_DIR:\n            EXTERNAL_MOUNT_MUNGE = True\n\n        lustre_path = Path('/sys/module/lustre')\n        gcsf_path = Path('/etc/yum.repos.d/gcsfuse.repo')\n\n        if fs_type == 'cifs' and not cifs_installed:\n            util.run(\"sudo yum install -y cifs-utils\")\n            cifs_installed = True\n        elif fs_type == 'lustre' and not lustre_path.exists():\n            lustre_url = 'https://downloads.whamcloud.com/public/lustre/latest-release/el7.7.1908/client/RPMS/x86_64/'\n            lustre_tmp = Path('/tmp/lustre')\n            lustre_tmp.mkdir(parents=True)\n            util.run('sudo yum update -y')\n            util.run('sudo yum install -y wget libyaml')\n            for rpm in ('kmod-lustre-client-2*.rpm', 'lustre-client-2*.rpm'):\n                util.run(\n                    f\"wget -r -l1 --no-parent -A '{rpm}' '{lustre_url}' -P {lustre_tmp}\")\n            util.run(\n                f\"find {lustre_tmp} -name '*.rpm' -execdir rpm -ivh {{}} ';'\")\n            util.run(f\"rm -rf {lustre_tmp}\")\n            util.run(\"modprobe lustre\")\n        elif fs_type == 'gcsfuse' and not gcsf_path.exists():\n            with gcsf_path.open('a') as f:\n                f.write(\"\"\"\n[gcsfuse]\nname=gcsfuse (packages.cloud.google.com)\nbaseurl=https://packages.cloud.google.com/yum/repos/gcsfuse-el7-x86_64\nenabled=1\ngpgcheck=1\nrepo_gpgcheck=1\ngpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg\nhttps://packages.cloud.google.com/yum/doc/rpm-package-key.gpg\"\"\")\n            util.run(\"sudo yum update -y\")\n            util.run(\"sudo yum install -y gcsfuse\")\n\n        mount_options = mount['mount_options']\n        if fs_type == 'gcsfuse':\n            if 'nonempty' not in mount['mount_options']:\n                mount_options += \",nonempty\"\n            fstab_entries.append(\n                \"\\n{0}   {1}     {2}     {3}     0 0\"\n                .format(remote_mount, local_mount, fs_type, mount_options))\n        else:\n            remote_mount = Path(remote_mount).resolve()\n            fstab_entries.append(\n                \"\\n{0}:{1}    {2}     {3}      {4}  0 0\"\n                .format(server_ip, remote_mount, local_mount,\n                        fs_type, mount_options))\n\n    with open('/etc/fstab', 'a') as f:\n        for entry in fstab_entries:\n            f.write(entry)\n# END setup_network_storage()\n\n\ndef setup_secondary_disks():\n\n    if not SEC_DISK_DIR.exists():\n        SEC_DISK_DIR.mkdir(parents=True)\n    util.run(\n        \"sudo mkfs.ext4 -m 0 -F -E lazy_itable_init=0,lazy_journal_init=0,discard /dev/sdb\")\n    with open('/etc/fstab', 'a') as f:\n        f.write(\n            \"\\n/dev/sdb     {0}     ext4    discard,defaults,nofail     0 2\"\n            .format(SEC_DISK_DIR))\n\n# END setup_secondary_disks()\n\n\ndef mount_nfs_vols():\n\n    mount_paths = (\n        (HOME_DIR, EXTERNAL_MOUNT_HOME),\n        (APPS_DIR, EXTERNAL_MOUNT_APPS),\n        (MUNGE_DIR, EXTERNAL_MOUNT_MUNGE),\n    )\n    # compress yields values from the first arg that are matched with True\n    # in the second arg. The result is the paths filtered by the booleans.\n    # For non-controller instances, all three are always external nfs\n    for path in it.compress(*zip(*mount_paths)):\n        while not os.path.ismount(path):\n            log.info(f\"Waiting for {path} to be mounted\")\n            util.run(f\"mount {path}\", wait=5)\n    util.run(\"mount -a\", wait=1)\n\n# END mount_nfs_vols()\n\n\n# Tune the NFS server to support many mounts\ndef setup_nfs_threads():\n\n    with open('/etc/sysconfig/nfs', 'a') as f:\n        f.write(\"\"\"\n# Added by Google\nRPCNFSDCOUNT=256\n\"\"\")\n\n# END setup_nfs_threads()\n\n\ndef setup_sync_cronjob():\n\n    util.run(\"crontab -u slurm -\", input=(\n        f\"*/1 * * * * {APPS_DIR}/slurm/scripts/slurmsync.py\\n\"))\n\n# END setup_sync_cronjob()\n\n\ndef setup_slurmd_cronjob():\n    util.run(\n        \"crontab -u root -\", input=(\n            \"*/2 * * * * \"\n            \"if [ `systemctl status slurmd | grep -c inactive` -gt 0 ]; then \"\n            \"mount -a; \"\n            \"systemctl restart munge; \"\n            \"systemctl restart slurmd; \"\n            \"fi\\n\"\n        ))\n# END setup_slurmd_cronjob()\n\n\ndef create_compute_images():\n\n    def create_compute_image(instance, partition):\n        try:\n            compute = googleapiclient.discovery.build('compute', 'v1',\n                                                      cache_discovery=False)\n\n            while True:\n                resp = compute.instances().get(\n                    project=cfg.project, zone=cfg.zone, fields=\"status\",\n                    instance=instance).execute()\n                if resp['status'] == 'TERMINATED':\n                    break\n                log.info(f\"waiting for {instance} to be stopped (status: {resp['status']})\"\n                         .format(instance=instance, status=resp['status']))\n                time.sleep(30)\n\n            log.info(\"Creating image of {}...\".format(instance))\n            ver = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n            util.run(f\"gcloud compute images create \"\n                     f\"{instance}-{ver} --source-disk {instance} \"\n                     f\"--source-disk-zone {cfg.zone} --force \"\n                     f\"--family {instance}-family\")\n\n            util.run(\"{}/bin/scontrol update partitionname={} state=up\"\n                     .format(CURR_SLURM_DIR, partition.name))\n        except Exception as e:\n            log.exception(f\"{instance} not found: {e}\")\n\n    threads = []\n    for i, part in enumerate(cfg.partitions):\n        instance = f\"{cfg.compute_node_prefix}-{i}-image\"\n        thread = threading.Thread(target=create_compute_image,\n                                  args=(instance, part))\n        threads.append(thread)\n        thread.start()\n\n    for thread in threads:\n        thread.join()\n\n# END create_compute_image()\n\n\ndef setup_selinux():\n\n    util.run('setenforce 0')\n    with open('/etc/selinux/config', 'w') as f:\n        f.write(\"\"\"\nSELINUX=permissive\nSELINUXTYPE=targeted\n\"\"\")\n# END setup_selinux()\n\n\ndef install_ompi():\n\n    if not cfg.ompi_version:\n        return\n\n    packages = ['autoconf',\n                'flex',\n                'gcc-c++',\n                'libevent-devel',\n                'libtool']\n    util.run(\"yum install -y {}\".format(' '.join(packages)))\n\n    ompi_git = \"https://github.com/open-mpi/ompi.git\"\n    ompi_path = APPS_DIR/'ompi'/cfg.ompi_version/'src'\n    if not ompi_path.exists():\n        ompi_path.mkdir(parents=True)\n    util.run(f\"git clone -b {cfg.ompi_version} {ompi_git} {ompi_path}\")\n    with cd(ompi_path):\n        util.run(\"./autogen.pl\", stdout=DEVNULL)\n\n    build_path = ompi_path/'build'\n    if not build_path.exists():\n        build_path.mkdir(parents=True)\n    with cd(build_path):\n        util.run(\n            f\"../configure --prefix={APPS_DIR}/ompi/{cfg.ompi_version} \"\n            f\"--with-pmi={APPS_DIR}/slurm/current --with-libevent=/usr \"\n            \"--with-hwloc=/usr\", stdout=DEVNULL)\n        util.run(\"make -j install\", stdout=DEVNULL)\n# END install_ompi()\n\n\ndef remove_startup_scripts(hostname):\n\n    cmd = \"gcloud compute instances remove-metadata\"\n    common_keys = \"startup-script,setup_script,util_script,config\"\n    controller_keys = (f\"{common_keys},\"\n                       \"slurm_conf_tpl,slurmdbd_conf_tpl,cgroup_conf_tpl\")\n    compute_keys = f\"{common_keys},slurm_fluentd_log_tpl\"\n\n    # controller\n    util.run(f\"{cmd} {hostname} --zone={cfg.zone} --keys={controller_keys}\")\n\n    # logins\n    for i in range(0, cfg.login_node_count):\n        util.run(\"{} {}-login{} --zone={} --keys={}\"\n                 .format(cmd, cfg.cluster_name, i, cfg.zone, common_keys))\n    # computes\n    for i, part in enumerate(cfg.partitions):\n        # partition compute image\n        util.run(f\"{cmd} {cfg.compute_node_prefix}-{i}-image \"\n                 f\"--zone={cfg.zone} --keys={compute_keys}\")\n        if not part.static_node_count:\n            continue\n        for j in range(part.static_node_count):\n            util.run(\"{} {}-{}-{} --zone={} --keys={}\"\n                     .format(cmd, cfg.compute_node_prefix, i, j,\n                             part.zone, compute_keys))\n# END remove_startup_scripts()\n\n\ndef setup_nss_slurm():\n\n    # setup nss_slurm\n    util.run(\"ln -s {}/lib/libnss_slurm.so.2 /usr/lib64/libnss_slurm.so.2\"\n             .format(CURR_SLURM_DIR))\n    util.run(\n        r\"sed -i 's/\\(^\\(passwd\\|group\\):\\s\\+\\)/\\1slurm /g' /etc/nsswitch.conf\"\n    )\n# END setup_nss_slurm()\n\n\ndef main():\n    hostname = socket.gethostname()\n\n    setup_selinux()\n\n    start_motd()\n\n    add_slurm_user()\n    install_packages()\n    setup_munge()\n    setup_bash_profile()\n    setup_ompi_bash_profile()\n    setup_modules()\n\n    if cfg.controller_secondary_disk and cfg.instance_type == 'controller':\n        setup_secondary_disks()\n    setup_network_storage()\n\n    if not SLURM_LOG.exists():\n        SLURM_LOG.mkdir(parents=True)\n    shutil.chown(SLURM_LOG, user='slurm', group='slurm')\n\n    if cfg.instance_type == 'controller':\n        mount_nfs_vols()\n        time.sleep(5)\n        start_munge()\n        install_slurm()\n        install_ompi()\n\n        try:\n            util.run(str(APPS_DIR/'slurm/scripts/custom-controller-install'))\n        except Exception:\n            # Ignore blank files with no shell magic.\n            pass\n\n        install_controller_service_scripts()\n\n        if not cfg.cloudsql:\n            util.run('systemctl enable mariadb')\n            util.run('systemctl start mariadb')\n\n            mysql = \"mysql -u root -e\"\n            util.run(\n                f\"\"\"{mysql} \"create user 'slurm'@'localhost'\";\"\"\")\n            util.run(\n                f\"\"\"{mysql} \"grant all on slurm_acct_db.* TO 'slurm'@'localhost'\";\"\"\")\n            util.run(\n                f\"\"\"{mysql} \"grant all on slurm_acct_db.* TO 'slurm'@'{CONTROL_MACHINE}'\";\"\"\")\n\n        util.run(\"systemctl enable slurmdbd\")\n        util.run(\"systemctl start slurmdbd\")\n\n        # Wait for slurmdbd to come up\n        time.sleep(5)\n\n        sacctmgr = f\"{CURR_SLURM_DIR}/bin/sacctmgr -i\"\n        util.run(f\"{sacctmgr} add cluster {cfg.cluster_name}\")\n\n        util.run(\"systemctl enable slurmctld\")\n        util.run(\"systemctl start slurmctld\")\n        setup_nfs_threads()\n        # Export at the end to signal that everything is up\n        util.run(\"systemctl enable nfs-server\")\n        util.run(\"systemctl start nfs-server\")\n        setup_nfs_exports()\n\n        setup_sync_cronjob()\n\n        # DOWN partitions until image is created.\n        for part in cfg.partitions:\n            util.run(\"{}/bin/scontrol update partitionname={} state=down\"\n                     .format(CURR_SLURM_DIR, part.name))\n\n        create_compute_images()\n        remove_startup_scripts(hostname)\n        log.info(\"Done installing controller\")\n    elif cfg.instance_type == 'compute':\n        install_compute_service_scripts()\n        mount_nfs_vols()\n        start_munge()\n        setup_nss_slurm()\n        setup_slurmd_cronjob()\n\n        try:\n            util.run(str(APPS_DIR/'slurm/scripts/custom-compute-install'))\n        except Exception:\n            # Ignore blank files with no shell magic.\n            pass\n\n        if hostname.endswith('-image'):\n            end_motd(False)\n            util.run(\"sync\")\n            util.run(f\"gcloud compute instances stop {hostname} \"\n                     f\"--zone {cfg.zone} --quiet\")\n        else:\n            util.run(\"systemctl start slurmd\")\n\n    else:  # login nodes\n        mount_nfs_vols()\n        start_munge()\n\n        try:\n            util.run(str(APPS_DIR/\"slurm/scripts/custom-compute-install\"))\n        except Exception:\n            # Ignore blank files with no shell magic.\n            pass\n\n    setup_logrotate()\n\n    end_motd()\n\n# END main()\n\n\nif __name__ == '__main__':\n    main()\n\n",
              "startup-script": "#!/bin/bash\n# Copyright 2019 SchedMD LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nPACKAGES=(\n        'bind-utils'\n        'environment-modules'\n        'epel-release'\n        'gcc'\n        'git'\n        'hwloc'\n        'hwloc-devel'\n        'libibmad'\n        'libibumad'\n        'lua'\n        'lua-devel'\n        'man2html'\n        'mariadb'\n        'mariadb-devel'\n        'mariadb-server'\n        'munge'\n        'munge-devel'\n        'munge-libs'\n        'ncurses-devel'\n        'nfs-utils'\n        'numactl'\n        'numactl-devel'\n        'openssl-devel'\n        'pam-devel'\n        'perl-ExtUtils-MakeMaker'\n        'python3'\n        'python3-pip'\n        'readline-devel'\n        'rpm-build'\n        'rrdtool-devel'\n        'vim'\n        'wget'\n        'tmux'\n        'pdsh'\n        'openmpi'\n        'yum-utils'\n    )\n\nPY_PACKAGES=(\n        'pyyaml'\n        'requests'\n        'google-api-python-client'\n    )\n\nPING_HOST=8.8.8.8\nuntil ( ping -q -w1 -c1 $PING_HOST \u003e /dev/null ) ; do\n    echo \"Waiting for internet\"\n    sleep .5\ndone\n\necho \"yum install -y ${PACKAGES[*]}\"\nuntil ( yum install -y ${PACKAGES[*]} \u003e /dev/null ) ; do\n    echo \"yum failed to install packages. Trying again in 5 seconds\"\n    sleep 5\ndone\n\necho   \"pip3 install --upgrade ${PY_PACKAGES[*]}\"\nuntil ( pip3 install --upgrade ${PY_PACKAGES[*]} ) ; do\n    echo \"pip3 failed to install python packages. Trying again in 5 seconds\"\n    sleep 5\ndone\n\nSETUP_SCRIPT=\"setup.py\"\nSETUP_META=\"setup_script\"\nDIR=\"/tmp\"\nURL=\"http://metadata.google.internal/computeMetadata/v1/instance/attributes/$SETUP_META\"\nHEADER=\"Metadata-Flavor:Google\"\necho  \"wget -nv --header $HEADER $URL -O $DIR/$SETUP_SCRIPT\"\nif ! ( wget -nv --header $HEADER $URL -O $DIR/$SETUP_SCRIPT ) ; then\n    echo \"Failed to fetch $SETUP_META:$SETUP_SCRIPT from metadata\"\n    exit 1\nfi\n\necho \"running python cluster setup script\"\nchmod +x $DIR/$SETUP_SCRIPT\n$DIR/$SETUP_SCRIPT\n\n",
              "terraform": "TRUE",
              "util_script": "#!/usr/bin/env python3\n# Copyright 2019 SchedMD LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport logging\nimport logging.config\nimport os\nimport shlex\nimport subprocess\nimport sys\nimport time\nfrom pathlib import Path\nfrom contextlib import contextmanager\nfrom collections import OrderedDict\n\nimport requests\nimport yaml\n\n\nlog = logging.getLogger(__name__)\n\n\ndef config_root_logger(level='DEBUG', util_level=None, file=None):\n    if not util_level:\n        util_level = level\n    handler = 'file_handler' if file else 'stdout_handler'\n    config = {\n        'version': 1,\n        'disable_existing_loggers': True,\n        'formatters': {\n            'standard': {\n                'format': '',\n            },\n            'stamp': {\n                'format': '%(asctime)s %(name)s %(levelname)s: %(message)s',\n            },\n        },\n        'handlers': {\n            'stdout_handler': {\n                'level': 'DEBUG',\n                'formatter': 'standard',\n                'class': 'logging.StreamHandler',\n                'stream': sys.stdout,\n            },\n        },\n        'loggers': {\n            '': {\n                'handlers': [handler],\n                'level': level,\n            },\n            __name__: {  # enable util.py logging\n                'level': util_level,\n            }\n        },\n    }\n    if file:\n        config['handlers']['file_handler'] = {\n            'level': 'DEBUG',\n            'formatter': 'stamp',\n            'class': 'logging.handlers.WatchedFileHandler',\n            'filename': file,\n        }\n    logging.config.dictConfig(config)\n\n\ndef get_metadata(path):\n    \"\"\" Get metadata relative to metadata/computeMetadata/v1/instance/ \"\"\"\n    URL = 'http://metadata.google.internal/computeMetadata/v1/instance/'\n    HEADERS = {'Metadata-Flavor': 'Google'}\n    full_path = URL + path\n    try:\n        resp = requests.get(full_path, headers=HEADERS)\n        resp.raise_for_status()\n    except requests.exceptions.RequestException:\n        log.exception(f\"Error while getting metadata from {full_path}\")\n        return None\n    return resp.text\n\n\ndef run(cmd, wait=0, quiet=False, get_stdout=False,\n        shell=False, universal_newlines=True, **kwargs):\n    \"\"\" run in subprocess. Optional wait after return. \"\"\"\n    if not quiet:\n        log.debug(f\"run: {cmd}\")\n    if get_stdout:\n        kwargs['stdout'] = subprocess.PIPE\n\n    args = cmd if shell else shlex.split(cmd)\n    ret = subprocess.run(args, shell=shell,\n                         universal_newlines=universal_newlines,\n                         **kwargs)\n    if wait:\n        time.sleep(wait)\n    return ret\n\n\ndef spawn(cmd, quiet=False, shell=False, **kwargs):\n    \"\"\" nonblocking spawn of subprocess \"\"\"\n    if not quiet:\n        log.debug(f\"spawn: {cmd}\")\n    args = cmd if shell else shlex.split(cmd)\n    return subprocess.Popen(args, shell=shell, **kwargs)\n\n\ndef get_pid(node_name):\n    \"\"\"Convert \u003cprefix\u003e-\u003cpid\u003e-\u003cnid\u003e\"\"\"\n\n    return int(node_name.split('-')[-2])\n\n\n@contextmanager\ndef cd(path):\n    \"\"\" Change working directory for context \"\"\"\n    prev = Path.cwd()\n    os.chdir(path)\n    try:\n        yield\n    finally:\n        os.chdir(prev)\n\n\ndef static_vars(**kwargs):\n    \"\"\"\n    Add variables to the function namespace.\n    @static_vars(var=init): var must be referenced func.var\n    \"\"\"\n    def decorate(func):\n        for k in kwargs:\n            setattr(func, k, kwargs[k])\n        return func\n    return decorate\n\n\nclass cached_property:\n    \"\"\"\n    Descriptor for creating a property that is computed once and cached\n    \"\"\"\n    def __init__(self, factory):\n        self._attr_name = factory.__name__\n        self._factory = factory\n\n    def __get__(self, instance, owner=None):\n        if instance is None:  # only if invoked from class\n            return self\n        attr = self._factory(instance)\n        setattr(instance, self._attr_name, attr)\n        return attr\n\n\nclass Config(OrderedDict):\n    \"\"\" Loads config from yaml and holds values in nested namespaces \"\"\"\n\n    TYPES = set(('compute', 'login', 'controller'))\n    # PROPERTIES defines which properties in slurm.jinja.schema are included\n    #   in the config file. SAVED_PROPS are saved to file via save_config.\n    SAVED_PROPS = ('project',\n                   'zone',\n                   'cluster_name',\n                   'external_compute_ips',\n                   'shared_vpc_host_project',\n                   'compute_node_prefix',\n                   'compute_node_service_account',\n                   'compute_node_scopes',\n                   'slurm_cmd_path',\n                   'log_dir',\n                   'google_app_cred_path',\n                   'update_node_addrs',\n                   'partitions',\n                   )\n    PROPERTIES = (*SAVED_PROPS,\n                  'munge_key',\n                  'external_compute_ips',\n                  'nfs_home_server',\n                  'nfs_home_dir',\n                  'nfs_apps_server',\n                  'nfs_apps_dir',\n                  'ompi_version',\n                  'controller_secondary_disk',\n                  'slurm_version',\n                  'suspend_time',\n                  'network_storage',\n                  'login_network_storage',\n                  'login_node_count',\n                  'cloudsql',\n                  )\n\n    def __init__(self, *args, **kwargs):\n        def from_nested(value):\n            \"\"\" If value is dict, convert to Config. Also recurse lists. \"\"\"\n            if isinstance(value, dict):\n                return Config({k: from_nested(v) for k, v in value.items()})\n            elif isinstance(value, list):\n                return [from_nested(v) for v in value]\n            else:\n                return value\n\n        super(Config, self).__init__(*args, **kwargs)\n        self.__dict__ = self  # all properties are member attributes\n\n        # Convert nested dicts to Configs\n        for k, v in self.items():\n            self[k] = from_nested(v)\n\n    @classmethod\n    def new_config(cls, properties):\n        # If k is ever not found, None will be inserted as the value\n        return cls({k: properties.setdefault(k, None) for k in cls.PROPERTIES})\n\n    @classmethod\n    def load_config(cls, path):\n        config = yaml.safe_load(Path(path).read_text())\n        return cls(config)\n\n    def save_config(self, path):\n        save_dict = Config([(k, self[k]) for k in self.SAVED_PROPS])\n        Path(path).write_text(yaml.dump(save_dict, Dumper=self.Dumper))\n\n    @cached_property\n    def instance_type(self):\n        # get tags, intersect with possible types, get the first or none\n        tags = yaml.safe_load(get_metadata('tags'))\n        # TODO what to default to if no match found.\n        return next(iter(set(tags) \u0026 self.TYPES), None)\n\n    @property\n    def region(self):\n        return self.zone and '-'.join(self.zone.split('-')[:-1])\n\n    def __getattr__(self, item):\n        \"\"\" only called if item is not found in self \"\"\"\n        return None\n\n    class Dumper(yaml.SafeDumper):\n        def __init__(self, *args, **kwargs):\n            super().__init__(*args, **kwargs)\n            self.add_representer(Config, self.represent_config)\n\n        @staticmethod\n        def represent_config(dumper, data):\n            return dumper.represent_mapping('tag:yaml.org,2002:map',\n                                            data.items())\n\n"
            },
            "metadata_fingerprint": "Pu9dRLXQg1o=",
            "metadata_startup_script": null,
            "min_cpu_platform": "",
            "name": "gcluster-login0",
            "network_interface": [
              {
                "access_config": [
                  {
                    "nat_ip": "34.82.41.78",
                    "network_tier": "PREMIUM",
                    "public_ptr_domain_name": ""
                  }
                ],
                "alias_ip_range": [],
                "name": "nic0",
                "network": "https://www.googleapis.com/compute/v1/projects/playground-190420/global/networks/vf-network",
                "network_ip": "192.168.0.27",
                "subnetwork": "https://www.googleapis.com/compute/v1/projects/playground-190420/regions/us-west1/subnetworks/vf-west-subnet",
                "subnetwork_project": "playground-190420"
              }
            ],
            "project": "playground-190420",
            "scheduling": [
              {
                "automatic_restart": true,
                "node_affinities": [],
                "on_host_maintenance": "MIGRATE",
                "preemptible": false
              }
            ],
            "scratch_disk": [],
            "self_link": "https://www.googleapis.com/compute/v1/projects/playground-190420/zones/us-west1-a/instances/gcluster-login0",
            "service_account": [
              {
                "email": "861735104433-compute@developer.gserviceaccount.com",
                "scopes": [
                  "https://www.googleapis.com/auth/logging.write",
                  "https://www.googleapis.com/auth/monitoring.write"
                ]
              }
            ],
            "shielded_instance_config": [
              {
                "enable_integrity_monitoring": true,
                "enable_secure_boot": false,
                "enable_vtpm": true
              }
            ],
            "tags": [
              "login"
            ],
            "tags_fingerprint": "2KMAI-0fjjw=",
            "timeouts": null,
            "zone": "us-west1-a"
          },
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjoxMjAwMDAwMDAwMDAwLCJkZWxldGUiOjEyMDAwMDAwMDAwMDAsInVwZGF0ZSI6MTIwMDAwMDAwMDAwMH0sInNjaGVtYV92ZXJzaW9uIjoiNiJ9",
          "dependencies": [
            "module.slurm_cluster_network.google_compute_subnetwork.cluster_subnet"
          ]
        }
      ]
    },
    {
      "module": "module.slurm_cluster_network",
      "mode": "managed",
      "type": "google_compute_network",
      "name": "cluster_network",
      "each": "list",
      "provider": "provider.google",
      "instances": []
    },
    {
      "module": "module.slurm_cluster_network",
      "mode": "managed",
      "type": "google_compute_router",
      "name": "cluster_router",
      "each": "list",
      "provider": "provider.google",
      "instances": []
    },
    {
      "module": "module.slurm_cluster_network",
      "mode": "managed",
      "type": "google_compute_router_nat",
      "name": "cluster_nat",
      "each": "list",
      "provider": "provider.google",
      "instances": []
    },
    {
      "module": "module.slurm_cluster_network",
      "mode": "managed",
      "type": "google_compute_subnetwork",
      "name": "cluster_subnet",
      "each": "list",
      "provider": "provider.google",
      "instances": []
    }
  ]
}
